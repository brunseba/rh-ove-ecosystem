{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RH OVE Ecosystem Design and Management","text":"<p>Welcome to the comprehensive guide for designing, deploying, and managing the multi-cluster RH OVE ecosystem.</p>"},{"location":"#solution-overview","title":"Solution Overview","text":"<p>This documentation covers a complete multi-cluster RH OVE implementation consisting of:</p> <ul> <li>1 Management Cluster: Centralized control plane for governance, policy, monitoring, and GitOps</li> <li>N Application Clusters: Dedicated workload execution environments for virtual machines and containers</li> </ul> <pre><code>graph TB\n    subgraph \"Management Cluster\"\n        A[Management Control Plane]\n        B[Argo CD Hub]\n        C[RHACM Central]\n        D[RHACS Security]\n        E[Observability Stack]\n    end\n\n    subgraph \"Application Clusters\"\n        F[Production Environments]\n        G[Staging Environments]\n        H[Development Environments]\n    end\n\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    F --&gt; E\n    G --&gt; E\n    H --&gt; E</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#1-design-phase","title":"1. Design Phase","text":"<ul> <li>Multi-cluster topology for separation of management and workloads</li> <li>Centralized governance through the management cluster</li> <li>Consistent security using RHACS and Kyverno policies</li> </ul>"},{"location":"#2-deployment-phase","title":"2. Deployment Phase","text":"<ul> <li>Rubrik integration for enterprise backup and recovery</li> <li>Dynatrace monitoring for comprehensive observability</li> <li>GitOps methodology using Argo CD for declarative management</li> </ul>"},{"location":"#3-management-phase","title":"3. Management Phase","text":"<ul> <li>Enhanced admission control with OpenShift defaults plus Kyverno policies</li> <li>CRD-based management leveraging KubeVirt resources</li> <li>Event-driven integrations with CMDB systems</li> </ul>"},{"location":"#4-best-practices","title":"4. Best Practices","text":"<ul> <li>Resource management and multi-tenancy</li> <li>Security and isolation enforcement</li> <li>Continuous improvement through monitoring</li> </ul>"},{"location":"#5-references","title":"5. References","text":"<p>Comprehensive product documentation and URIs for all integrated components.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Review the Architecture Overview</li> <li>Follow the Installation Guide</li> <li>Configure Admission Control</li> <li>Set up Monitoring</li> </ol>"},{"location":"#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"External Systems\"\n        EXT1[Rubrik Backup Platform]\n        EXT2[Dynatrace Monitoring]\n        EXT3[ServiceNow CMDB]\n        EXT4[Git Repository]\n    end\n\n    subgraph \"RH OVE Cluster\"\n        subgraph \"Control Plane\"\n            CP1[OpenShift API Server]\n            CP2[etcd]\n            CP3[Controller Manager]\n        end\n\n        subgraph \"Admission Control Layer\"\n            AC1[OpenShift Built-in Controllers]\n            AC2[KubeVirt Webhooks]\n            AC3[Kyverno Policy Engine]\n        end\n\n        subgraph \"Network Layer\"\n            NET1[Cilium CNI]\n            NET2[eBPF Programs]\n            NET3[Network Policies]\n        end\n\n        subgraph \"GitOps Layer\"\n            GO1[Argo CD]\n            GO2[Application Controller]\n        end\n\n        subgraph \"Workload Layer\"\n            WL1[Virtual Machines]\n            WL2[Container Pods]\n            WL3[Persistent Volumes]\n        end\n    end\n\n    EXT4 --&gt; GO1\n    GO1 --&gt; CP1\n    CP1 --&gt; AC1\n    CP1 --&gt; AC2\n    CP1 --&gt; AC3\n    AC1 --&gt; WL1\n    AC2 --&gt; WL1\n    AC3 --&gt; WL1\n    AC3 --&gt; WL2\n    NET1 --&gt; WL1\n    NET1 --&gt; WL2\n    WL1 --&gt; EXT1\n    WL2 --&gt; EXT1\n    NET1 --&gt; EXT2\n    WL1 --&gt; EXT2\n    WL2 --&gt; EXT2\n    CP1 --&gt; EXT3</code></pre> <p>This solution provides a modern, secure, and scalable approach to managing virtualized workloads alongside containerized applications in a unified OpenShift platform.</p>"},{"location":"TASKS/","title":"RH OVE Ecosystem - Task Management","text":"<p>This document describes the task management system for the RH OVE Ecosystem project.</p>"},{"location":"TASKS/#task-runner","title":"Task Runner","text":"<p>This project uses Go's Task (gotask) as a task runner to simplify common development and maintenance tasks.</p>"},{"location":"TASKS/#installing-task","title":"Installing Task","text":"<p>If you don't have <code>task</code> installed:</p> <pre><code># macOS\nbrew install go-task/tap/go-task\n\n# Linux\nsh -c \"$(curl -ssL https://taskfile.dev/install.sh)\"\n\n# Or via Go\ngo install github.com/go-task/task/v3/cmd/task@latest\n\n# Or download from releases\n# https://github.com/go-task/task/releases\n</code></pre>"},{"location":"TASKS/#global-tasks-project-root","title":"Global Tasks (Project Root)","text":"<p>From the project root directory, run:</p> <pre><code>task --list\n</code></pre>"},{"location":"TASKS/#available-global-tasks","title":"Available Global Tasks","text":"<ul> <li><code>task setup</code> - Setup the entire project environment</li> <li><code>task clean</code> - Clean all temporary files and caches</li> <li><code>task export-workload</code> - Export workload data to XLSX</li> <li><code>task docs:build</code> - Generate project documentation</li> <li><code>task docs:serve</code> - Serve documentation locally</li> <li><code>task health-check</code> - Run project health check</li> <li><code>task update</code> - Update all dependencies</li> <li><code>task status</code> - Show project status</li> <li><code>task init</code> - Initialize a new development environment</li> <li><code>task ci</code> - Run continuous integration checks</li> </ul>"},{"location":"TASKS/#scripts-specific-tasks","title":"Scripts-Specific Tasks","text":"<p>From the <code>scripts/</code> directory, run:</p> <pre><code>cd scripts\ntask --list\n</code></pre>"},{"location":"TASKS/#available-scripts-tasks","title":"Available Scripts Tasks","text":"<ul> <li><code>task setup</code> - Setup the scripts environment</li> <li><code>task install</code> - Install dependencies</li> <li><code>task add &lt;dep&gt;</code> - Add a new dependency (with prompt)</li> <li><code>task export-workload</code> - Run the workload export script</li> <li><code>task export-workload-py</code> - Run export script directly with Python</li> <li><code>task check</code> - Check Python syntax</li> <li><code>task format</code> - Format code</li> <li><code>task lint</code> - Lint code</li> <li><code>task test</code> - Run tests</li> <li><code>task clean</code> - Clean Python cache files</li> <li><code>task update</code> - Update dependencies</li> <li><code>task info</code> - Show project information</li> <li><code>task dev-setup</code> - Install development tools</li> <li><code>task dev</code> - Run development workflow</li> <li><code>task build</code> - Build/validate the project</li> <li><code>task watch</code> - Watch for changes and run export</li> <li><code>task validate</code> - Validate all aspects of the project</li> </ul>"},{"location":"TASKS/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Initial Setup: <pre><code>task init\n</code></pre></p> </li> <li> <p>Export Workload Data: <pre><code>task export-workload\n</code></pre></p> </li> <li> <p>Check Project Health: <pre><code>task health-check\n</code></pre></p> </li> <li> <p>Development Workflow (from root): <pre><code>task scripts:dev-setup\ntask scripts:dev\n</code></pre></p> </li> <li> <p>Watch for Changes (from root): <pre><code>task scripts:watch\n</code></pre></p> </li> <li> <p>Or work directly in scripts directory: <pre><code>cd scripts\ntask dev-setup\ntask watch\n</code></pre></p> </li> </ol>"},{"location":"TASKS/#file-structure","title":"File Structure","text":"<pre><code>.\n\u251c\u2500\u2500 Taskfile.yml         # Global task definitions\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 Taskfile.yml     # Scripts-specific task definitions\n\u2502   \u251c\u2500\u2500 pyproject.toml   # Python project configuration\n\u2502   \u2514\u2500\u2500 *.py             # Python scripts\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 export/          # Generated export files\n</code></pre>"},{"location":"TASKS/#advanced-features","title":"Advanced Features","text":""},{"location":"TASKS/#task-dependencies","title":"Task Dependencies","text":"<p>Tasks can depend on other tasks:</p> <pre><code>tasks:\n  build:\n    deps:\n      - check\n      - lint\n    cmds:\n      - echo \"Building...\"\n</code></pre>"},{"location":"TASKS/#file-watching","title":"File Watching","text":"<p>Task supports file watching with sources and generates:</p> <pre><code>tasks:\n  export-workload:\n    sources:\n      - \"{{.SOURCE_FILE}}\"\n      - export_workload_to_xlsx.py\n    generates:\n      - \"{{.EXPORT_FILE}}\"\n    cmds:\n      - uv run export-workload\n</code></pre>"},{"location":"TASKS/#variables-and-templating","title":"Variables and Templating","text":"<p>Use variables for configuration:</p> <pre><code>vars:\n  PROJECT_NAME: RH OVE Ecosystem\n  SCRIPTS_DIR: scripts\n\ntasks:\n  info:\n    cmds:\n      - echo \"Project: {{.PROJECT_NAME}}\"\n</code></pre>"},{"location":"TASKS/#adding-new-tasks","title":"Adding New Tasks","text":"<p>To add new tasks, edit the appropriate <code>Taskfile.yml</code>:</p> <ul> <li>Global tasks: Edit <code>./Taskfile.yml</code></li> <li>Scripts tasks: Edit <code>./scripts/Taskfile.yml</code></li> </ul>"},{"location":"TASKS/#example-task","title":"Example Task","text":"<pre><code>tasks:\n  my-task:\n    desc: Description of what the task does\n    deps:\n      - other-task\n    sources:\n      - src/**/*.py\n    generates:\n      - dist/output.txt\n    cmds:\n      - echo \"Running task...\"\n      - command-to-run\n      - echo \"Task complete!\"\n</code></pre>"},{"location":"TASKS/#dependencies","title":"Dependencies","text":"<ul> <li><code>task</code> - Go-based task runner</li> <li><code>uv</code> - Python package manager (for scripts)</li> <li><code>mkdocs</code> - Documentation generator (optional)</li> <li><code>watchexec</code> - File watcher (optional, for <code>task watch</code>)</li> </ul>"},{"location":"architecture/context-diagram/","title":"RH OVE Ecosystem Context Diagram","text":""},{"location":"architecture/context-diagram/#overview","title":"Overview","text":"<p>This context diagram provides a high-level view of the RH OVE (Red Hat OpenShift Virtualization Engine) ecosystem within the broader enterprise environment. It illustrates the system boundaries, external entities, data flows, and key integrations that define how the RH OVE ecosystem interacts with users, external systems, and enterprise services.</p>"},{"location":"architecture/context-diagram/#system-context","title":"System Context","text":"<p>The RH OVE ecosystem operates as a comprehensive multi-cluster virtualization platform that bridges traditional virtualization workloads with modern cloud-native operations, providing seamless integration with enterprise infrastructure and services.</p>"},{"location":"architecture/context-diagram/#context-diagram","title":"Context Diagram","text":"<pre><code>graph LR\n    %% External Users and Roles\n    subgraph \"Enterprise Users\"\n        DEV[Developers]\n        OPS[Operations Teams]\n        SEC[Security Teams]\n        BIZ[Business Users]\n        ADM[Platform Administrators]\n    end\n\n    %% External Enterprise Systems\n    subgraph \"Enterprise Identity &amp; Access\"\n        AD[Active Directory/LDAP]\n        SSO[Enterprise SSO/OIDC]\n        PAM[Privileged Access Management]\n    end\n\n    subgraph \"Enterprise Management\"\n        CMDB[Configuration Management Database]\n        ITSM[IT Service Management]\n        SIEM[Security Information Event Management]\n        ASSET[Asset Management Systems]\n    end\n\n    subgraph \"Enterprise Infrastructure\"\n        DNS[Enterprise DNS]\n        NTP[Network Time Protocol]\n        PKI[Public Key Infrastructure]\n        PROXY[Enterprise Proxy/Firewall]\n        LB[Load Balancers]\n    end\n\n    subgraph \"Data Center Infrastructure\"\n        COMPUTE[Physical/Virtual Compute]\n        STORAGE[Enterprise Storage Systems]\n        NETWORK[Enterprise Network Infrastructure]\n        BACKUP[Enterprise Backup Systems]\n    end\n\n    subgraph \"External Services\"\n        REGISTRY[Container Registries]\n        REPOS[Git Repositories]\n        MONITOR[External Monitoring]\n        CLOUD[Public Cloud Services]\n    end\n\n    %% RH OVE Ecosystem - Main System\n    subgraph \"RH OVE Ecosystem\" \n        direction TB\n\n        subgraph \"Management Plane\"\n            MGMT[Management Cluster]\n            GITOPS[GitOps Platform - ArgoCD]\n            POLICY[Policy Management - Kyverno]\n            SECURITY[Security - RHACS]\n            OBSERV[Observability Stack]\n            CLUSTER_MGMT[Multi-Cluster Management - RHACM]\n        end\n\n        subgraph \"Application Planes\"\n            PROD[Production Clusters]\n            STAGING[Staging Clusters]\n            DEV_CLUSTER[Development Clusters]\n            EDGE[Edge Clusters]\n        end\n\n        subgraph \"Virtualization Layer\"\n            KUBEVIRT[KubeVirt Engine]\n            VM_WORKLOADS[Virtual Machine Workloads]\n            CONTAINER_WORKLOADS[Container Workloads]\n        end\n\n        subgraph \"Infrastructure Services\"\n            CNI[Cilium CNI]\n            CSI[Storage CSI Drivers]\n            MULTUS[Multi-Network - Multus]\n            BACKUP_AGENT[Backup Agents]\n        end\n    end\n\n    %% Legacy Systems Integration\n    subgraph \"Legacy Infrastructure\"\n        VMWARE[VMware Infrastructure]\n        LEGACY_APP[Legacy Applications]\n        MAINFRAME[Mainframe Systems]\n        PHYSICAL[Physical Servers]\n    end\n\n    %% External Connections - Users\n    DEV --&gt; MGMT\n    OPS --&gt; MGMT\n    SEC --&gt; SECURITY\n    BIZ --&gt; VM_WORKLOADS\n    ADM --&gt; CLUSTER_MGMT\n\n    %% External Connections - Identity\n    AD --&gt; MGMT\n    SSO --&gt; MGMT\n    PAM --&gt; MGMT\n\n    %% External Connections - Management\n    MGMT --&gt; CMDB\n    MGMT --&gt; ITSM\n    SECURITY --&gt; SIEM\n    OBSERV --&gt; MONITOR\n    MGMT --&gt; ASSET\n\n    %% External Connections - Infrastructure\n    MGMT --&gt; DNS\n    MGMT --&gt; NTP\n    MGMT --&gt; PKI\n    MGMT --&gt; PROXY\n    LB --&gt; MGMT\n\n    %% External Connections - Data Center\n    KUBEVIRT --&gt; COMPUTE\n    CSI --&gt; STORAGE\n    CNI --&gt; NETWORK\n    BACKUP_AGENT --&gt; BACKUP\n\n    %% External Connections - Services\n    GITOPS --&gt; REPOS\n    MGMT --&gt; REGISTRY\n    OBSERV --&gt; MONITOR\n    MGMT --&gt; CLOUD\n\n    %% Legacy Integration\n    KUBEVIRT --&gt; VMWARE\n    VM_WORKLOADS --&gt; LEGACY_APP\n    KUBEVIRT --&gt; PHYSICAL\n\n    %% Internal Connections\n    MGMT --&gt; PROD\n    MGMT --&gt; STAGING\n    MGMT --&gt; DEV_CLUSTER\n    MGMT --&gt; EDGE\n\n    GITOPS --&gt; PROD\n    GITOPS --&gt; STAGING\n    GITOPS --&gt; DEV_CLUSTER\n\n    POLICY --&gt; PROD\n    POLICY --&gt; STAGING\n    POLICY --&gt; DEV_CLUSTER\n\n    SECURITY --&gt; PROD\n    SECURITY --&gt; STAGING\n    SECURITY --&gt; DEV_CLUSTER\n\n    OBSERV --&gt; PROD\n    OBSERV --&gt; STAGING\n    OBSERV --&gt; DEV_CLUSTER\n\n    %% Styling\n    classDef userClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef enterpriseClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef coreClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef legacyClass fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef infraClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px\n\n    class DEV,OPS,SEC,BIZ,ADM userClass\n    class AD,SSO,PAM,CMDB,ITSM,SIEM,ASSET,DNS,NTP,PKI,PROXY,LB enterpriseClass\n    class MGMT,GITOPS,POLICY,SECURITY,OBSERV,CLUSTER_MGMT,PROD,STAGING,DEV_CLUSTER,EDGE,KUBEVIRT coreClass\n    class VMWARE,LEGACY_APP,MAINFRAME,PHYSICAL legacyClass\n    class COMPUTE,STORAGE,NETWORK,BACKUP,REGISTRY,REPOS,MONITOR,CLOUD infraClass</code></pre>"},{"location":"architecture/context-diagram/#system-boundaries-and-responsibilities","title":"System Boundaries and Responsibilities","text":""},{"location":"architecture/context-diagram/#rh-ove-ecosystem-core","title":"RH OVE Ecosystem Core","text":"<p>The central system encompasses: - Management Plane: Centralized governance, policy, and operations control - Application Planes: Multiple clusters for different environments and purposes - Virtualization Layer: KubeVirt-based VM and container workload execution - Infrastructure Services: Networking, storage, and backup integration</p>"},{"location":"architecture/context-diagram/#key-external-integrations","title":"Key External Integrations","text":""},{"location":"architecture/context-diagram/#identity-and-access-management","title":"Identity and Access Management","text":"<ul> <li>Active Directory/LDAP: Enterprise user directory integration</li> <li>Enterprise SSO/OIDC: Single sign-on and OAuth/OIDC authentication</li> <li>Privileged Access Management: Elevated access control and auditing</li> </ul>"},{"location":"architecture/context-diagram/#enterprise-management-systems","title":"Enterprise Management Systems","text":"<ul> <li>CMDB: Configuration item tracking and relationship mapping</li> <li>ITSM: Service request and incident management integration</li> <li>SIEM: Security event correlation and threat detection</li> <li>Asset Management: Hardware and software asset tracking</li> </ul>"},{"location":"architecture/context-diagram/#infrastructure-dependencies","title":"Infrastructure Dependencies","text":"<ul> <li>Enterprise DNS: Name resolution services</li> <li>NTP: Time synchronization across all components</li> <li>PKI: Certificate management and trust establishment</li> <li>Network Infrastructure: Physical and virtual networking</li> <li>Storage Systems: Persistent storage for VMs and containers</li> <li>Backup Systems: Data protection and recovery services</li> </ul>"},{"location":"architecture/context-diagram/#development-and-operations","title":"Development and Operations","text":"<ul> <li>Git Repositories: Source code and configuration management</li> <li>Container Registries: Image storage and distribution</li> <li>External Monitoring: Enterprise monitoring system integration</li> <li>Public Cloud Services: Hybrid and multi-cloud connectivity</li> </ul>"},{"location":"architecture/context-diagram/#legacy-system-integration","title":"Legacy System Integration","text":"<p>The RH OVE ecosystem provides migration paths and integration capabilities for: - VMware Infrastructure: VM migration and workload transformation - Legacy Applications: Containerization and modernization support - Physical Servers: Bare metal integration and management - Mainframe Systems: API integration and data exchange</p>"},{"location":"architecture/context-diagram/#data-flow-patterns","title":"Data Flow Patterns","text":""},{"location":"architecture/context-diagram/#inbound-data-flows","title":"Inbound Data Flows","text":"<ul> <li>User Authentication: From enterprise identity systems</li> <li>Configuration Data: From Git repositories and CMDB</li> <li>Monitoring Metrics: To enterprise monitoring systems</li> <li>Security Events: To SIEM platforms</li> <li>Backup Data: From enterprise backup systems</li> </ul>"},{"location":"architecture/context-diagram/#outbound-data-flows","title":"Outbound Data Flows","text":"<ul> <li>Audit Logs: To compliance and logging systems</li> <li>Performance Metrics: To enterprise dashboards</li> <li>Security Alerts: To security operations centers</li> <li>Configuration Changes: To change management systems</li> <li>Service Status: To IT service management platforms</li> </ul>"},{"location":"architecture/context-diagram/#bidirectional-integration","title":"Bidirectional Integration","text":"<ul> <li>Identity Federation: Continuous authentication and authorization</li> <li>Policy Synchronization: Enterprise policy distribution and compliance</li> <li>Asset Discovery: Dynamic configuration item updates</li> <li>Network Connectivity: Secure communication channels</li> </ul>"},{"location":"architecture/context-diagram/#security-boundaries","title":"Security Boundaries","text":""},{"location":"architecture/context-diagram/#trust-zones","title":"Trust Zones","text":"<ol> <li>Management Zone: High-security administrative functions</li> <li>Production Zone: Business-critical workload execution</li> <li>Development Zone: Lower-trust development activities</li> <li>DMZ: External-facing services and integrations</li> </ol>"},{"location":"architecture/context-diagram/#security-controls","title":"Security Controls","text":"<ul> <li>Network Segmentation: Micro-segmentation with Cilium</li> <li>Zero Trust Architecture: Identity-based access controls</li> <li>Encryption: End-to-end data protection</li> <li>Audit Logging: Comprehensive activity tracking</li> </ul>"},{"location":"architecture/context-diagram/#scalability-and-growth","title":"Scalability and Growth","text":"<p>The context diagram illustrates the ecosystem's ability to: - Horizontal Scaling: Add application clusters as needed - Geographic Distribution: Deploy across multiple data centers - Hybrid Integration: Seamlessly connect on-premises and cloud resources - Legacy Modernization: Gradual transformation of existing systems</p>"},{"location":"architecture/context-diagram/#operational-model","title":"Operational Model","text":""},{"location":"architecture/context-diagram/#day-1-operations-deployment","title":"Day-1 Operations (Deployment)","text":"<ul> <li>Initial cluster provisioning and configuration</li> <li>Integration with enterprise systems</li> <li>Security policy establishment</li> <li>Baseline monitoring setup</li> </ul>"},{"location":"architecture/context-diagram/#day-2-operations-management","title":"Day-2 Operations (Management)","text":"<ul> <li>Ongoing cluster lifecycle management</li> <li>Policy updates and compliance monitoring</li> <li>Performance optimization and scaling</li> <li>Security incident response</li> </ul> <p>This context diagram serves as the foundation for understanding how the RH OVE ecosystem integrates with and enhances existing enterprise infrastructure while providing a modern, scalable platform for virtualization and containerization workloads.</p>"},{"location":"architecture/design-principles/","title":"Design Principles","text":""},{"location":"architecture/design-principles/#overview","title":"Overview","text":"<p>The RH OVE solution is built on fundamental design principles that ensure scalability, security, and operational efficiency for hybrid container and VM workloads.</p>"},{"location":"architecture/design-principles/#core-principles","title":"Core Principles","text":""},{"location":"architecture/design-principles/#1-application-namespace-based-topology","title":"1. Application Namespace-Based Topology","text":"<p>Based on the analysis from our research, using an application namespace-based topology is considered a best practice for RH OVE clusters.</p> <pre><code>graph TB\n    subgraph \"Cluster Level\"\n        A[RH OVE Cluster]\n    end\n\n    subgraph \"Application Namespaces\"\n        B[app-web]\n        C[app-database]\n        D[app-analytics]\n        E[app-monitoring]\n    end\n\n    subgraph \"Resources per Namespace\"\n        B --&gt; F[VMs + Containers + Storage + Network]\n        C --&gt; G[VMs + Containers + Storage + Network]\n        D --&gt; H[VMs + Containers + Storage + Network]\n        E --&gt; I[VMs + Containers + Storage + Network]\n    end</code></pre> <p>Benefits: - Isolation and Security: Strong RBAC and network policy enforcement - Operational Efficiency: Simplified management and troubleshooting - Network Segregation: Namespace-scoped NetworkAttachmentDefinitions - Scalability: Prevents resource clutter and performance bottlenecks - Policy Management: Granular security policies and quotas</p> <p>Implementation: - Group related VMs and Kubernetes resources by application or business domain - Apply consistent labeling for automation and cost management - Combine with network policies and RBAC rules - Designate separate namespaces for dev, test, and prod environments</p>"},{"location":"architecture/design-principles/#2-mixed-workload-strategy","title":"2. Mixed Workload Strategy","text":"<p>Multiplexing Kubernetes container workloads and VM workloads on the same RH OVE cluster is highly advantageous:</p> <pre><code>graph LR\n    subgraph \"RH OVE Cluster\"\n        A[Unified Management Platform]\n\n        subgraph \"Container Workloads\"\n            B[Microservices]\n            C[Cloud-Native Apps]\n            D[API Gateways]\n        end\n\n        subgraph \"VM Workloads\"\n            E[Legacy Applications]\n            F[Windows Workloads]\n            G[Databases]\n            H[Monolithic Applications]\n        end\n    end\n\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    A --&gt; F\n    A --&gt; G\n    A --&gt; H</code></pre> <p>Advantages: - Unified Management: Same Kubernetes-native interface for all workloads - Resource Optimization: Better hardware consolidation - Flexibility: Gradual modernization path for legacy applications - Streamlined DevOps: Integrated CI/CD pipelines for all workload types - Advanced Platform Features: HA, storage provisioning, monitoring for all</p>"},{"location":"architecture/design-principles/#3-security-first-design","title":"3. Security-First Design","text":"<p>Implement defense-in-depth security across all layers:</p> <pre><code>graph TD\n    A[Security Layers] --&gt; B[Network Security]\n    A --&gt; C[Admission Control]\n    A --&gt; D[RBAC &amp; SCC]\n    A --&gt; E[Pod Security Standards]\n    A --&gt; F[Image Security]\n\n    B --&gt; G[Cilium CNI with eBPF]\n    B --&gt; H[Network Policies]\n    B --&gt; I[Microsegmentation]\n\n    C --&gt; J[OpenShift Built-in]\n    C --&gt; K[KubeVirt Webhooks]\n    C --&gt; L[Kyverno Policies]\n\n    D --&gt; M[Role-Based Access Control]\n    D --&gt; N[Security Context Constraints]\n\n    E --&gt; O[Pod Security Standards]\n    E --&gt; P[Workload Isolation]\n\n    F --&gt; Q[Image Scanning]\n    F --&gt; R[Registry Security]</code></pre>"},{"location":"architecture/design-principles/#4-gitops-driven-operations","title":"4. GitOps-Driven Operations","text":"<p>Implement infrastructure and application management through GitOps principles:</p> <p>Benefits: - Single Source of Truth: All configurations version-controlled in Git - Declarative Management: Infrastructure as Code for VMs and containers - Automation: Reduced human error through automated deployments - Auditability: Complete change tracking and rollback capabilities - Collaboration: Peer review through pull requests</p>"},{"location":"architecture/design-principles/#5-observability-and-monitoring","title":"5. Observability and Monitoring","text":"<p>Comprehensive monitoring strategy across all workload types:</p> <pre><code>graph TB\n    subgraph \"Monitoring Stack\"\n        A[Dynatrace] --&gt; B[Full-Stack Monitoring]\n        A --&gt; C[AI-Powered Analytics]\n        A --&gt; D[Application Performance]\n\n        E[Prometheus] --&gt; F[Metrics Collection]\n        E --&gt; G[Custom Metrics]\n\n        H[OpenShift Monitoring] --&gt; I[Cluster Health]\n        H --&gt; J[Platform Metrics]\n    end\n\n    subgraph \"Workloads\"\n        K[VMs]\n        L[Containers]\n        M[Infrastructure]\n    end\n\n    B --&gt; K\n    B --&gt; L\n    B --&gt; M\n    F --&gt; K\n    F --&gt; L\n    I --&gt; M</code></pre>"},{"location":"architecture/design-principles/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"architecture/design-principles/#namespace-design","title":"Namespace Design","text":"<pre><code># Example namespace structure\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: app-web-prod\n  labels:\n    app: web\n    environment: production\n    tier: frontend\n  annotations:\n    network-policy: strict\n    backup-policy: daily\n</code></pre>"},{"location":"architecture/design-principles/#resource-quotas","title":"Resource Quotas","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: app-web-prod\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: 20Gi\n    limits.cpu: \"20\"\n    limits.memory: 40Gi\n    persistentvolumeclaims: \"10\"\n</code></pre>"},{"location":"architecture/design-principles/#network-policies","title":"Network Policies","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: app-web-netpol\n  namespace: app-web-prod\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: app-gateway-prod\n</code></pre>"},{"location":"architecture/design-principles/#best-practices","title":"Best Practices","text":""},{"location":"architecture/design-principles/#design-decisions","title":"Design Decisions","text":"<ol> <li>Plan namespace strategy early: Define naming conventions and access hierarchies</li> <li>Implement least privilege: Use RBAC and network policies consistently</li> <li>Design for scale: Consider resource limits and node capacity planning</li> <li>Plan for disaster recovery: Include backup and restoration strategies</li> </ol>"},{"location":"architecture/design-principles/#operational-considerations","title":"Operational Considerations","text":"<ol> <li>Monitor resource utilization: Prevent resource contention between workload types</li> <li>Implement proper logging: Centralized logging for both VMs and containers</li> <li>Regular security assessments: Continuous compliance and vulnerability management</li> <li>Performance testing: Regular load testing for mixed workload scenarios</li> </ol> <p>These design principles ensure that the RH OVE solution provides a robust, secure, and scalable platform for modern hybrid workloads while supporting organizational transformation initiatives.</p>"},{"location":"architecture/global-overview/","title":"Global Architecture Overview","text":""},{"location":"architecture/global-overview/#overview","title":"Overview","text":"<p>The RH OVE ecosystem is designed as a multi-cluster architecture that separates concerns between management operations and application workloads. This design provides scalability, security, and operational efficiency by dedicating specialized clusters for different purposes while maintaining centralized governance and oversight.</p>"},{"location":"architecture/global-overview/#architecture-principles","title":"Architecture Principles","text":""},{"location":"architecture/global-overview/#separation-of-concerns","title":"Separation of Concerns","text":"<ul> <li> <p>Management Cluster: Centralized control plane for governance, policy, monitoring, and operations</p> </li> <li> <p>Application Clusters: Dedicated workload execution environments for virtual machines and containers</p> </li> <li> <p>Clear Boundaries: Well-defined interfaces and responsibilities between cluster types</p> </li> </ul>"},{"location":"architecture/global-overview/#scalability-and-growth","title":"Scalability and Growth","text":"<ul> <li> <p>Horizontal Scaling: Add application clusters as demand grows</p> </li> <li> <p>Regional Distribution: Deploy clusters across different geographic locations</p> </li> <li> <p>Resource Optimization: Right-size clusters based on workload requirements</p> </li> </ul>"},{"location":"architecture/global-overview/#security-and-compliance","title":"Security and Compliance","text":"<ul> <li> <p>Zero Trust Architecture: Network-level security between clusters</p> </li> <li> <p>Centralized Policy Management: Consistent security policies across all clusters</p> </li> <li> <p>Compliance Monitoring: Unified compliance reporting and auditing</p> </li> </ul>"},{"location":"architecture/global-overview/#multi-cluster-topology","title":"Multi-Cluster Topology","text":"<pre><code>graph TB\n    subgraph \"Management Cluster\"\n        subgraph \"GitOps Platform\"\n            ARGO[ArgoCD Hub]\n            GIT[Git Repositories]\n        end\n\n        subgraph \"Policy &amp; Security\"\n            RHACS[Red Hat Advanced Cluster Security]\n            POL[Policy Engine - Kyverno]\n        end\n\n        subgraph \"Multi-Cluster Management\"\n            RHACM[Red Hat Advanced Cluster Management]\n            FLEET[Fleet Management]\n        end\n\n        subgraph \"Observability Stack\"\n            PROM[Prometheus Federation]\n            GRAF[Grafana Central]\n            ALERT[AlertManager]\n            LOG[Logging Aggregation]\n        end\n\n        subgraph \"Backup &amp; DR\"\n            RUBRIK[Rubrik Management]\n            BACKUP[Backup Policies]\n        end\n    end\n\n    subgraph \"Application Cluster 1 - Production\"\n        subgraph \"Virtualization Stack 1\"\n            OVE1[OpenShift Virtualization]\n            VM1[Virtual Machines]\n            CDI1[Containerized Data Importer]\n        end\n\n        subgraph \"Networking 1\"\n            CIL1[Cilium CNI]\n            MULT1[Multus Multi-Network]\n            SRIOV1[SR-IOV Networks]\n        end\n\n        subgraph \"Storage 1\"\n            CSI1[CSI Drivers]\n            PV1[Persistent Volumes]\n        end\n\n        subgraph \"Local Agents\"\n            ARGO1[ArgoCD Agent]\n            RHACS1[RHACS Agent]\n            MON1[Monitoring Agents]\n        end\n    end\n\n    subgraph \"Application Cluster 2 - Staging\"\n        subgraph \"Virtualization Stack 2\"\n            OVE2[OpenShift Virtualization]\n            VM2[Virtual Machines]\n            CDI2[Containerized Data Importer]\n        end\n\n        subgraph \"Networking 2\"\n            CIL2[Cilium CNI]\n            MULT2[Multus Multi-Network]\n            SRIOV2[SR-IOV Networks]\n        end\n\n        subgraph \"Storage 2\"\n            CSI2[CSI Drivers]\n            PV2[Persistent Volumes]\n        end\n\n        subgraph \"Local Agents\"\n            ARGO2[ArgoCD Agent]\n            RHACS2[RHACS Agent]\n            MON2[Monitoring Agents]\n        end\n    end\n\n    subgraph \"Application Cluster N - Development\"\n        subgraph \"Virtualization Stack N\"\n            OVEN[OpenShift Virtualization]\n            VMN[Virtual Machines]\n            CDIN[Containerized Data Importer]\n        end\n\n        subgraph \"Networking N\"\n            CILN[Cilium CNI]\n            MULTN[Multus Multi-Network]\n            SRIOVN[SR-IOV Networks]\n        end\n\n        subgraph \"Storage N\"\n            CSIN[CSI Drivers]\n            PVN[Persistent Volumes]\n        end\n\n        subgraph \"Local Agents\"\n            ARGON[ArgoCD Agent]\n            RHACSN[RHACS Agent]\n            MONN[Monitoring Agents]\n        end\n    end\n\n    %% Management to Application Connections\n    ARGO --&gt; ARGO1\n    ARGO --&gt; ARGO2\n    ARGO --&gt; ARGON\n\n    RHACM --&gt; ARGO1\n    RHACM --&gt; ARGO2\n    RHACM --&gt; ARGON\n\n    RHACS --&gt; RHACS1\n    RHACS --&gt; RHACS2\n    RHACS --&gt; RHACSN\n\n    PROM --&gt; MON1\n    PROM --&gt; MON2\n    PROM --&gt; MONN\n\n    RUBRIK --&gt; PV1\n    RUBRIK --&gt; PV2\n    RUBRIK --&gt; PVN\n\n    %% Git to ArgoCD\n    GIT --&gt; ARGO\n\n    %% Policy Distribution\n    POL --&gt; RHACS1\n    POL --&gt; RHACS2\n    POL --&gt; RHACSN</code></pre>"},{"location":"architecture/global-overview/#management-cluster-components","title":"Management Cluster Components","text":""},{"location":"architecture/global-overview/#core-management-services","title":"Core Management Services","text":""},{"location":"architecture/global-overview/#red-hat-advanced-cluster-management-rhacm","title":"Red Hat Advanced Cluster Management (RHACM)","text":"<pre><code>apiVersion: operator.open-cluster-management.io/v1\nkind: MultiClusterHub\nmetadata:\n  name: multiclusterhub\n  namespace: open-cluster-management\nspec:\n  availabilityConfig: High\n  enableClusterBackup: true\n  overrides:\n    components:\n    - name: multicluster-observability-operator\n      enabled: true\n    - name: cluster-lifecycle\n      enabled: true\n    - name: cluster-permission\n      enabled: true\n</code></pre> <p>Responsibilities: - Cluster lifecycle management - Policy distribution and compliance - Application deployment coordination - Resource optimization across clusters</p>"},{"location":"architecture/global-overview/#argocd-hub-configuration","title":"ArgoCD Hub Configuration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ArgoCD\nmetadata:\n  name: argocd-hub\n  namespace: argocd\nspec:\n  server:\n    route:\n      enabled: true\n      tls:\n        termination: reencrypt\n    replicas: 3\n  controller:\n    resources:\n      requests:\n        cpu: 500m\n        memory: 1Gi\n      limits:\n        cpu: 2\n        memory: 4Gi\n  dex:\n    openShiftOAuth: true\n  ha:\n    enabled: true\n  rbac:\n    defaultPolicy: 'role:readonly'\n    policy: |\n      p, role:admin, applications, *, */*, allow\n      p, role:admin, clusters, *, *, allow\n      p, role:admin, repositories, *, *, allow\n      g, argocd-admins, role:admin\n</code></pre> <p>Responsibilities: - GitOps workflow orchestration - Application deployment to target clusters - Configuration drift detection and remediation - Multi-cluster application synchronization</p>"},{"location":"architecture/global-overview/#security-and-compliance_1","title":"Security and Compliance","text":""},{"location":"architecture/global-overview/#red-hat-advanced-cluster-security-rhacs","title":"Red Hat Advanced Cluster Security (RHACS)","text":"<pre><code>apiVersion: platform.stackrox.io/v1alpha1\nkind: Central\nmetadata:\n  name: stackrox-central-services\n  namespace: stackrox\nspec:\n  central:\n    exposure:\n      loadBalancer:\n        enabled: true\n    persistence:\n      persistentVolumeClaim:\n        claimName: central-db\n    resources:\n      requests:\n        cpu: 1500m\n        memory: 4Gi\n      limits:\n        cpu: 4000m\n        memory: 8Gi\n  scanner:\n    resources:\n      requests:\n        cpu: 200m\n        memory: 200Mi\n      limits:\n        cpu: 2000m\n        memory: 4Gi\n</code></pre> <p>Responsibilities: - Centralized security policy management - Vulnerability scanning across clusters - Runtime threat detection - Compliance reporting and audit trails</p>"},{"location":"architecture/global-overview/#policy-engine-kyverno","title":"Policy Engine (Kyverno)","text":"<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: multi-cluster-vm-policy\nspec:\n  validationFailureAction: enforce\n  background: true\n  rules:\n  - name: require-vm-labels\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n    validate:\n      message: \"VMs must have required labels: environment, owner, backup-policy\"\n      pattern:\n        metadata:\n          labels:\n            environment: \"?*\"\n            owner: \"?*\"\n            backup-policy: \"?*\"\n</code></pre>"},{"location":"architecture/global-overview/#observability-and-monitoring","title":"Observability and Monitoring","text":""},{"location":"architecture/global-overview/#federated-prometheus-configuration","title":"Federated Prometheus Configuration","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus-federation\n  namespace: monitoring\nspec:\n  replicas: 3\n  retention: 30d\n  storage:\n    volumeClaimTemplate:\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 500Gi\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchLabels:\n      prometheus: federation\n  additionalScrapeConfigs:\n    name: additional-scrape-configs\n    key: prometheus-additional.yaml\n</code></pre> <p>Federation Configuration: <pre><code>- job_name: 'federate-app-clusters'\n  scrape_interval: 15s\n  honor_labels: true\n  metrics_path: '/federate'\n  params:\n    'match[]':\n      - '{job=~\"kubernetes-.*\"}'\n      - '{job=~\"node-.*\"}'\n      - '{job=~\"kubevirt-.*\"}'\n  static_configs:\n    - targets:\n      - 'prometheus-app-cluster-1.monitoring.svc.cluster.local:9090'\n      - 'prometheus-app-cluster-2.monitoring.svc.cluster.local:9090'\n      - 'prometheus-app-cluster-n.monitoring.svc.cluster.local:9090'\n</code></pre></p>"},{"location":"architecture/global-overview/#centralized-logging","title":"Centralized Logging","text":"<pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: central-log-forwarder\n  namespace: openshift-logging\nspec:\n  outputs:\n  - name: central-elasticsearch\n    type: elasticsearch\n    url: https://elasticsearch-central.logging.svc.cluster.local:9200\n    secret:\n      name: elasticsearch-central-secret\n  pipelines:\n  - name: forward-app-logs\n    inputRefs:\n    - application\n    - infrastructure\n    - audit\n    outputRefs:\n    - central-elasticsearch\n</code></pre>"},{"location":"architecture/global-overview/#application-cluster-architecture","title":"Application Cluster Architecture","text":""},{"location":"architecture/global-overview/#cluster-sizing-and-resource-allocation","title":"Cluster Sizing and Resource Allocation","text":""},{"location":"architecture/global-overview/#production-cluster-profile","title":"Production Cluster Profile","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-profile-production\ndata:\n  profile: |\n    cluster_type: production\n    node_count: 12\n    master_nodes: 3\n    worker_nodes: 9\n    storage_nodes: 3\n\n    node_specifications:\n      master:\n        cpu: 16\n        memory: 64Gi\n        storage: 500Gi SSD\n      worker:\n        cpu: 32\n        memory: 128Gi\n        storage: 1Ti NVMe\n      storage:\n        cpu: 8\n        memory: 32Gi\n        storage: 4Ti SSD\n\n    network_configuration:\n      cni: cilium\n      multi_network: multus\n      sr_iov: enabled\n      encryption: wireguard\n\n    virtualization:\n      kubevirt_version: \"v1.1.0\"\n      nested_virtualization: true\n      hugepages: 1Gi\n      cpu_pinning: enabled\n</code></pre>"},{"location":"architecture/global-overview/#stagingdevelopment-cluster-profile","title":"Staging/Development Cluster Profile","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-profile-staging\ndata:\n  profile: |\n    cluster_type: staging\n    node_count: 6\n    master_nodes: 3\n    worker_nodes: 3\n\n    node_specifications:\n      master:\n        cpu: 8\n        memory: 32Gi\n        storage: 200Gi SSD\n      worker:\n        cpu: 16\n        memory: 64Gi\n        storage: 500Gi SSD\n\n    network_configuration:\n      cni: cilium\n      multi_network: multus\n      sr_iov: optional\n      encryption: ipsec\n\n    virtualization:\n      kubevirt_version: \"v1.1.0\"\n      nested_virtualization: false\n      hugepages: optional\n      cpu_pinning: disabled\n</code></pre>"},{"location":"architecture/global-overview/#virtualization-stack-configuration","title":"Virtualization Stack Configuration","text":""},{"location":"architecture/global-overview/#openshift-virtualization-deployment","title":"OpenShift Virtualization Deployment","text":"<pre><code>apiVersion: hco.kubevirt.io/v1beta1\nkind: HyperConverged\nmetadata:\n  name: kubevirt-hyperconverged\n  namespace: openshift-cnv\nspec:\n  infra:\n    nodePlacement:\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n  workloads:\n    nodePlacement:\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n  featureGates:\n    enableCommonBootImageImport: true\n    deployTektonTaskResources: true\n    enableApplicationAwareQuota: true\n  configuration:\n    network:\n      networkBinding:\n        plugins:\n          macvtap: {}\n          passt: {}\n    virtualMachineOptions:\n      disableFreePageReporting: false\n      disableSerialConsoleLog: false\n</code></pre>"},{"location":"architecture/global-overview/#multi-network-configuration","title":"Multi-Network Configuration","text":""},{"location":"architecture/global-overview/#network-attachment-definitions-for-different-environments","title":"Network Attachment Definitions for Different Environments","text":"<pre><code># Production Network Configuration\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: prod-management-network\n  namespace: vm-production\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"prod-management-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n---\n# Staging Network Configuration\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: staging-management-network\n  namespace: vm-staging\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"staging-management-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192\",\n      \"mode\": \"bridge\",\n      \"vlan\": 100,\n      \"ipam\": {\n        \"type\": \"dhcp\"\n      }\n    }\n</code></pre>"},{"location":"architecture/global-overview/#cluster-lifecycle-management","title":"Cluster Lifecycle Management","text":""},{"location":"architecture/global-overview/#cluster-provisioning-workflow","title":"Cluster Provisioning Workflow","text":"<pre><code>sequenceDiagram\n    participant Admin as Platform Admin\n    participant RHACM as RHACM Hub\n    participant Git as Git Repository\n    participant ArgoCD as ArgoCD Hub\n    participant Cluster as New Cluster\n\n    Admin-&gt;&gt;Git: Commit cluster definition\n    Git-&gt;&gt;ArgoCD: Webhook trigger\n    ArgoCD-&gt;&gt;RHACM: Apply cluster manifest\n    RHACM-&gt;&gt;Cluster: Provision cluster\n    Cluster-&gt;&gt;RHACM: Registration\n    RHACM-&gt;&gt;ArgoCD: Cluster ready notification\n    ArgoCD-&gt;&gt;Cluster: Deploy applications\n    Cluster-&gt;&gt;Admin: Cluster operational</code></pre>"},{"location":"architecture/global-overview/#cluster-template","title":"Cluster Template","text":"<pre><code>apiVersion: cluster.open-cluster-management.io/v1\nkind: ManagedCluster\nmetadata:\n  name: app-cluster-{{ .Values.environment }}-{{ .Values.region }}\n  labels:\n    environment: {{ .Values.environment }}\n    region: {{ .Values.region }}\n    cluster.open-cluster-management.io/clusterset: {{ .Values.clusterset }}\nspec:\n  hubAcceptsClient: true\n  leaseDurationSeconds: 60\n---\napiVersion: agent.open-cluster-management.io/v1\nkind: KlusterletAddonConfig\nmetadata:\n  name: app-cluster-{{ .Values.environment }}-{{ .Values.region }}\n  namespace: app-cluster-{{ .Values.environment }}-{{ .Values.region }}\nspec:\n  clusterName: app-cluster-{{ .Values.environment }}-{{ .Values.region }}\n  clusterNamespace: app-cluster-{{ .Values.environment }}-{{ .Values.region }}\n  clusterLabels:\n    environment: {{ .Values.environment }}\n    region: {{ .Values.region }}\n  applicationManager:\n    enabled: true\n  policyController:\n    enabled: true\n  searchCollector:\n    enabled: true\n  certPolicyController:\n    enabled: true\n</code></pre>"},{"location":"architecture/global-overview/#multi-cluster-networking","title":"Multi-Cluster Networking","text":""},{"location":"architecture/global-overview/#cluster-network-isolation","title":"Cluster Network Isolation","text":"<pre><code>graph TB\n    subgraph \"Management Network - 10.0.0.0/16\"\n        MGT[Management Cluster]\n        MGT_API[API Endpoints]\n        MGT_MON[Monitoring Services]\n    end\n\n    subgraph \"Production Network - 10.1.0.0/16\"\n        PROD[Production Cluster]\n        PROD_VM[Production VMs]\n        PROD_SVC[Production Services]\n    end\n\n    subgraph \"Staging Network - 10.2.0.0/16\"\n        STAGE[Staging Cluster]\n        STAGE_VM[Staging VMs]\n        STAGE_SVC[Staging Services]\n    end\n\n    subgraph \"Development Network - 10.3.0.0/16\"\n        DEV[Development Cluster]\n        DEV_VM[Development VMs]\n        DEV_SVC[Development Services]\n    end\n\n    subgraph \"Shared Services Network - 10.254.0.0/16\"\n        DNS[DNS Services]\n        NTP[NTP Services]\n        LDAP[LDAP/AD Services]\n        BACKUP[Backup Services]\n    end\n\n    %% Management connections\n    MGT_API -.-&gt; PROD\n    MGT_API -.-&gt; STAGE\n    MGT_API -.-&gt; DEV\n    MGT_MON -.-&gt; PROD\n    MGT_MON -.-&gt; STAGE\n    MGT_MON -.-&gt; DEV\n\n    %% Shared services connections\n    PROD -.-&gt; DNS\n    STAGE -.-&gt; DNS\n    DEV -.-&gt; DNS\n    PROD -.-&gt; BACKUP\n    STAGE -.-&gt; BACKUP\n    DEV -.-&gt; BACKUP</code></pre>"},{"location":"architecture/global-overview/#service-mesh-integration","title":"Service Mesh Integration","text":"<pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: cross-cluster-vm-service\nspec:\n  hosts:\n  - vm-service.production.svc.cluster.local\n  gateways:\n  - mesh\n  - cross-cluster-gateway\n  http:\n  - match:\n    - headers:\n        cluster:\n          exact: staging\n    route:\n    - destination:\n        host: vm-service.staging.svc.cluster.local\n  - route:\n    - destination:\n        host: vm-service.production.svc.cluster.local\n</code></pre>"},{"location":"architecture/global-overview/#disaster-recovery-and-business-continuity","title":"Disaster Recovery and Business Continuity","text":""},{"location":"architecture/global-overview/#multi-cluster-backup-strategy","title":"Multi-Cluster Backup Strategy","text":"<pre><code>apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: multi-cluster-backup\n  namespace: velero\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  template:\n    includedNamespaces:\n    - vm-production\n    - vm-staging\n    - openshift-cnv\n    excludedResources:\n    - pods\n    - replicasets\n    snapshotVolumes: true\n    ttl: 720h  # 30 days\n    hooks:\n      resources:\n      - name: vm-backup-hook\n        includedNamespaces:\n        - vm-production\n        - vm-staging\n        labelSelector:\n          matchLabels:\n            backup.kubevirt.io/enable: \"true\"\n        pre:\n        - exec:\n            container: virt-launcher\n            command:\n            - /bin/bash\n            - -c\n            - \"virtctl freeze --namespace $NAMESPACE $VM_NAME\"\n        post:\n        - exec:\n            container: virt-launcher\n            command:\n            - /bin/bash\n            - -c\n            - \"virtctl unfreeze --namespace $NAMESPACE $VM_NAME\"\n</code></pre>"},{"location":"architecture/global-overview/#cross-cluster-failover","title":"Cross-Cluster Failover","text":"<pre><code>apiVersion: cluster.open-cluster-management.io/v1beta1\nkind: Placement\nmetadata:\n  name: vm-workload-placement\n  namespace: vm-production\nspec:\n  predicates:\n  - requiredClusterSelector:\n      labelSelector:\n        matchLabels:\n          environment: production\n          region: primary\n  - requiredClusterSelector:\n      labelSelector:\n        matchLabels:\n          environment: production\n          region: secondary\n  numberOfClusters: 2\n  prioritizerPolicy:\n    mode: Additive\n    configurations:\n    - scoreCoordinate:\n        type: BuiltIn\n        builtIn: Steady\n      weight: 1\n    - scoreCoordinate:\n        type: BuiltIn\n        builtIn: ResourceAllocatableCPU\n      weight: 1\n</code></pre>"},{"location":"architecture/global-overview/#scalability-and-performance","title":"Scalability and Performance","text":""},{"location":"architecture/global-overview/#cluster-auto-scaling","title":"Cluster Auto-Scaling","text":"<pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineAutoscaler\nmetadata:\n  name: worker-autoscaler\n  namespace: openshift-machine-api\nspec:\n  minReplicas: 3\n  maxReplicas: 20\n  scaleTargetRef:\n    apiVersion: machine.openshift.io/v1beta1\n    kind: MachineSet\n    name: worker-machineset\n---\napiVersion: autoscaling.openshift.io/v1\nkind: ClusterAutoscaler\nmetadata:\n  name: default\nspec:\n  podPriorityThreshold: -10\n  resourceLimits:\n    maxNodesTotal: 50\n    cores:\n      min: 16\n      max: 1000\n    memory:\n      min: 64Gi\n      max: 4000Gi\n  scaleDown:\n    enabled: true\n    delayAfterAdd: 10m\n    delayAfterDelete: 10s\n    delayAfterFailure: 30s\n    unneededTime: 60s\n</code></pre>"},{"location":"architecture/global-overview/#vm-resource-management","title":"VM Resource Management","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: scalable-vm-template\n  namespace: vm-production\nspec:\n  template:\n    spec:\n      domain:\n        cpu:\n          cores: 4\n          sockets: 1\n          threads: 1\n        memory:\n          guest: 8Gi\n        resources:\n          requests:\n            cpu: 2\n            memory: 4Gi\n          limits:\n            cpu: 4\n            memory: 8Gi\n        devices:\n          autoattachPodInterface: false\n          autoattachSerialConsole: true\n          autoattachGraphicsDevice: true\n      evictionStrategy: LiveMigrate\n      terminationGracePeriodSeconds: 180\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n        vm-workload: \"true\"\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: vm.kubevirt.io/name\n                  operator: Exists\n              topologyKey: kubernetes.io/hostname\n</code></pre>"},{"location":"architecture/global-overview/#operational-procedures","title":"Operational Procedures","text":""},{"location":"architecture/global-overview/#day-2-operations-workflow","title":"Day-2 Operations Workflow","text":"<pre><code>graph TB\n    subgraph \"Management Operations\"\n        PATCH[Security Patches]\n        UPDATE[Component Updates]\n        SCALE[Capacity Scaling]\n        BACKUP[Backup Verification]\n    end\n\n    subgraph \"Application Operations\"\n        DEPLOY[VM Deployment]\n        MIGRATE[VM Migration]\n        MONITOR[Performance Monitoring]\n        TROUBLESHOOT[Issue Resolution]\n    end\n\n    subgraph \"Governance\"\n        POLICY[Policy Compliance]\n        AUDIT[Security Audit]\n        REPORT[Reporting]\n        REVIEW[Architecture Review]\n    end\n\n    PATCH --&gt; UPDATE\n    UPDATE --&gt; SCALE\n    SCALE --&gt; BACKUP\n\n    DEPLOY --&gt; MIGRATE\n    MIGRATE --&gt; MONITOR\n    MONITOR --&gt; TROUBLESHOOT\n\n    POLICY --&gt; AUDIT\n    AUDIT --&gt; REPORT\n    REPORT --&gt; REVIEW\n\n    BACKUP -.-&gt; DEPLOY\n    TROUBLESHOOT -.-&gt; POLICY\n    REVIEW -.-&gt; PATCH</code></pre>"},{"location":"architecture/global-overview/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: multi-cluster-alerts\n  namespace: monitoring\nspec:\n  groups:\n  - name: cluster.health\n    rules:\n    - alert: ClusterDown\n      expr: up{job=\"kubernetes-apiservers\"} == 0\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Cluster {{ $labels.cluster }} is down\"\n        description: \"Cluster {{ $labels.cluster }} has been down for more than 5 minutes\"\n\n    - alert: VMHighMemory\n      expr: kubevirt_vm_memory_usage_bytes / kubevirt_vm_memory_available_bytes &gt; 0.9\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} high memory usage\"\n        description: \"VM {{ $labels.name }} in cluster {{ $labels.cluster }} has high memory usage\"\n\n    - alert: VMMigrationFailed\n      expr: increase(kubevirt_vm_migration_failed_total[5m]) &gt; 0\n      labels:\n        severity: critical\n      annotations:\n        summary: \"VM migration failed\"\n        description: \"VM migration failed in cluster {{ $labels.cluster }}\"\n</code></pre>"},{"location":"architecture/global-overview/#best-practices-and-recommendations","title":"Best Practices and Recommendations","text":""},{"location":"architecture/global-overview/#cluster-design-guidelines","title":"Cluster Design Guidelines","text":"<ol> <li>Resource Planning</li> <li>Size clusters based on workload requirements</li> <li>Plan for 20-30% overhead for system components</li> <li> <p>Consider NUMA topology for high-performance VMs</p> </li> <li> <p>Network Segmentation</p> </li> <li>Isolate management and data plane traffic</li> <li>Use VLANs for multi-tenant environments</li> <li> <p>Implement east-west encryption</p> </li> <li> <p>Storage Strategy</p> </li> <li>Use local storage for high-performance workloads</li> <li>Implement storage classes for different performance tiers</li> <li> <p>Plan for backup and disaster recovery</p> </li> <li> <p>Security Architecture</p> </li> <li>Implement pod security standards</li> <li>Use network policies for microsegmentation</li> <li>Regular security scanning and compliance checks</li> </ol>"},{"location":"architecture/global-overview/#operational-excellence","title":"Operational Excellence","text":"<ol> <li>GitOps Workflow</li> <li>All changes through version control</li> <li>Automated testing and validation</li> <li> <p>Rollback capabilities</p> </li> <li> <p>Monitoring Strategy</p> </li> <li>Proactive alerting and monitoring</li> <li>Centralized logging and metrics</li> <li> <p>Regular performance reviews</p> </li> <li> <p>Disaster Recovery</p> </li> <li>Regular backup testing</li> <li>Cross-region replication</li> <li>Documented recovery procedures</li> </ol> <p>This global architecture overview provides a comprehensive foundation for understanding how the RH OVE ecosystem scales across multiple clusters while maintaining centralized governance, security, and operational efficiency. The architecture supports growth from small deployments to large-scale multi-region installations while preserving consistent management and security practices.</p>"},{"location":"architecture/iam/","title":"Identity and Access Management (IAM) Strategy","text":""},{"location":"architecture/iam/#overview","title":"Overview","text":"<p>This document outlines the comprehensive Identity and Access Management (IAM) strategy for the RH OVE multi-cluster ecosystem, implementing authentication and authorization using OpenID Connect (OIDC) providers with enterprise-grade security controls.</p>"},{"location":"architecture/iam/#executive-summary","title":"Executive Summary","text":"<p>The IAM strategy for the RH OVE multi-cluster ecosystem provides enterprise-grade identity and access management through a comprehensive OIDC-based approach that ensures security, compliance, and operational efficiency across all clusters and services.</p>"},{"location":"architecture/iam/#key-components","title":"Key Components","text":""},{"location":"architecture/iam/#1-architecture-components","title":"1. Architecture Components","text":"<ul> <li>Mermaid diagram showing OIDC provider integration across management and application clusters</li> <li>Identity Provider selection with Keycloak as the recommended solution (Red Hat SSO)</li> <li>Service integration with ArgoCD, Grafana, Prometheus, and Kubernetes Dashboard</li> <li>Dex OIDC Proxy deployment for centralized authentication</li> </ul>"},{"location":"architecture/iam/#2-authentication-implementation","title":"2. Authentication Implementation","text":"<ul> <li>OpenShift OAuth configuration with native OIDC integration</li> <li>Dex OIDC Proxy for service-to-service authentication</li> <li>Multi-Factor Authentication using Keycloak authentication flows with mandatory MFA for admin accounts</li> <li>JWT token management with proper security controls and time-limited tokens</li> <li>Single Sign-On (SSO) seamless access across all clusters and services</li> </ul>"},{"location":"architecture/iam/#3-authorization-implementation","title":"3. Authorization Implementation","text":"<ul> <li>Kubernetes RBAC integration with OIDC groups mapping</li> <li>ArgoCD RBAC configuration for GitOps access control with application-specific permissions</li> <li>Service Account token management with time-limited tokens and projected volumes</li> <li>Namespace-scoped permissions aligned with application teams and business units</li> <li>Role-Based Access Control with predefined organizational roles</li> </ul>"},{"location":"architecture/iam/#4-user-lifecycle-management","title":"4. User Lifecycle Management","text":"<ul> <li>SCIM integration for automated user provisioning and deprovisioning</li> <li>Group-based access control with predefined roles (platform-admins, web-developers, database-admins, security-auditors)</li> <li>Python automation examples for user management workflows</li> <li>Self-service capabilities for password reset and account recovery</li> </ul>"},{"location":"architecture/iam/#5-security-controls","title":"5. Security Controls","text":"<ul> <li>Token security with JWT validation rules and proper expiration policies</li> <li>Network security policies for authentication services with ingress/egress controls</li> <li>Encryption and key management with AES-256 and regular key rotation</li> <li>Zero Trust Principles implementation with least-privilege access patterns</li> </ul>"},{"location":"architecture/iam/#6-monitoring-and-audit","title":"6. Monitoring and Audit","text":"<ul> <li>Prometheus metrics for authentication monitoring and alerting</li> <li>Grafana dashboards for IAM visibility and operational insights</li> <li>Kubernetes audit policies for compliance logging and security tracking</li> <li>Automated access reviews and compliance reporting for SOC 2, GDPR, HIPAA</li> <li>Failed authentication tracking and security incident response</li> </ul>"},{"location":"architecture/iam/#7-disaster-recovery","title":"7. Disaster Recovery","text":"<ul> <li>Identity provider backup strategies with automated daily backups</li> <li>Multi-region failover configuration for high availability</li> <li>High availability for authentication services with automated health checks</li> </ul>"},{"location":"architecture/iam/#technical-implementation-highlights","title":"Technical Implementation Highlights","text":"<ul> <li>OIDC Provider Integration: Keycloak (Red Hat SSO) as primary, with Auth0/Okta alternatives</li> <li>Multi-Factor Authentication: Mandatory for administrative accounts with TOTP/SMS support</li> <li>Service Account Automation: Time-limited tokens (15min-2hours) with proper lifecycle management</li> <li>Audit and Compliance: Complete alignment with SOC 2, GDPR, and HIPAA requirements</li> <li>Enterprise Integration: LDAP/Active Directory federation for existing identity infrastructure</li> </ul>"},{"location":"architecture/iam/#business-benefits","title":"Business Benefits","text":"<ul> <li>Enhanced Security Posture: Zero trust principles with identity-aware access controls</li> <li>Operational Efficiency: Centralized identity management across all clusters and services</li> <li>Compliance Readiness: Automated audit trails and regulatory framework alignment</li> <li>Developer Experience: Single sign-on with self-service capabilities</li> <li>Cost Optimization: Reduced operational overhead through automation</li> </ul>"},{"location":"architecture/iam/#implementation-phases","title":"Implementation Phases","text":"<ol> <li>Phase 1: Design and Planning (Identity provider selection, architecture design)</li> <li>Phase 2: Deployment and Configuration (OIDC integration, RBAC implementation)</li> <li>Phase 3: Testing and Validation (Security testing, SSO validation, user acceptance)</li> <li>Phase 4: Monitoring and Maintenance (Metrics setup, audit automation, user training)</li> </ol> <p>This IAM strategy ensures robust and flexible identity management, leveraging existing enterprise IAM solutions for seamless integration and compliance within the RH OVE multi-cluster ecosystem while providing a foundation for future growth and regulatory requirements.</p>"},{"location":"architecture/iam/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/iam/#oidc-provider-integration","title":"OIDC Provider Integration","text":"<pre><code>graph TB\n    subgraph \"External Identity Provider\"\n        OIDC[\"OIDC Provider&lt;br/&gt;(Keycloak/Auth0/Okta)\"]\n        AD[\"Active Directory/LDAP\"]\n        MFA[\"MFA Service\"]\n    end\n\n    subgraph \"Management Cluster\"\n        DEXM[\"Dex OIDC Proxy\"]\n        OAUTHM[\"OAuth2 Proxy\"]\n        APIM[\"Kubernetes API Server\"]\n    end\n\n    subgraph \"Application Clusters\"\n        DEXA[\"Dex OIDC Proxy\"]\n        OAUTHA[\"OAuth2 Proxy\"]\n        APIA[\"Kubernetes API Server\"]\n    end\n\n    subgraph \"Services\"\n        ARGO[\"ArgoCD\"]\n        GRAF[\"Grafana\"]\n        PROM[\"Prometheus\"]\n        KUBE[\"Kubernetes Dashboard\"]\n    end\n\n    OIDC --&gt; DEXM\n    OIDC --&gt; DEXA\n    AD --&gt; OIDC\n    MFA --&gt; OIDC\n\n    DEXM --&gt; APIM\n    DEXM --&gt; OAUTHM\n    DEXA --&gt; APIA\n    DEXA --&gt; OAUTHA\n\n    OAUTHM --&gt; ARGO\n    OAUTHM --&gt; GRAF\n    OAUTHM --&gt; PROM\n    OAUTHA --&gt; KUBE</code></pre>"},{"location":"architecture/iam/#identity-provider-selection","title":"Identity Provider Selection","text":""},{"location":"architecture/iam/#recommended-keycloak-red-hat-sso","title":"Recommended: Keycloak (Red Hat SSO)","text":"<ul> <li>Advantages: Open source, Red Hat supported, full OIDC compliance</li> <li>Features: User federation, social login, fine-grained authorization</li> <li>Integration: Native OpenShift integration, Kubernetes RBAC mapping</li> </ul>"},{"location":"architecture/iam/#alternative-enterprise-solutions","title":"Alternative: Enterprise Solutions","text":"<ul> <li>Auth0: SaaS solution with extensive integrations</li> <li>Okta: Enterprise-grade with advanced security features</li> <li>Azure AD: Microsoft ecosystem integration</li> </ul>"},{"location":"architecture/iam/#authentication-implementation","title":"Authentication Implementation","text":""},{"location":"architecture/iam/#openshift-oauth-configuration","title":"OpenShift OAuth Configuration","text":"<pre><code>apiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n  - name: keycloak-oidc\n    mappingMethod: claim\n    type: OpenID\n    openID:\n      clientID: openshift-cluster\n      clientSecret:\n        name: oidc-client-secret\n      issuer: https://keycloak.company.com/auth/realms/openshift\n      claims:\n        preferredUsername:\n        - preferred_username\n        name:\n        - name\n        email:\n        - email\n        groups:\n        - groups\n</code></pre>"},{"location":"architecture/iam/#dex-oidc-proxy-configuration","title":"Dex OIDC Proxy Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dex-config\n  namespace: auth-system\ndata:\n  config.yaml: |\n    issuer: https://dex.company.com\n    storage:\n      type: kubernetes\n      config:\n        inCluster: true\n    web:\n      https: 0.0.0.0:5556\n      tlsCert: /etc/dex/tls/tls.crt\n      tlsKey: /etc/dex/tls/tls.key\n    connectors:\n    - type: oidc\n      id: keycloak\n      name: Keycloak\n      config:\n        issuer: https://keycloak.company.com/auth/realms/company\n        clientID: dex-client\n        clientSecret: $DEX_CLIENT_SECRET\n        redirectURI: https://dex.company.com/callback\n        scopes:\n        - openid\n        - profile\n        - email\n        - groups\n    staticClients:\n    - id: kubernetes\n      redirectURIs:\n      - https://kubectl.company.com/callback\n      name: 'Kubernetes CLI'\n      secret: $KUBERNETES_CLIENT_SECRET\n    - id: argocd\n      redirectURIs:\n      - https://argocd.company.com/auth/callback\n      name: 'ArgoCD'\n      secret: $ARGOCD_CLIENT_SECRET\n</code></pre>"},{"location":"architecture/iam/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<pre><code># Keycloak Authentication Flow\nauthenticationFlows:\n  - alias: \"browser-with-mfa\"\n    description: \"Browser flow with mandatory MFA\"\n    providerId: \"basic-flow\"\n    topLevel: true\n    builtIn: false\n    authenticationExecutions:\n      - authenticator: \"auth-cookie\"\n        requirement: \"ALTERNATIVE\"\n      - authenticator: \"auth-spnego\"\n        requirement: \"DISABLED\"\n      - authenticator: \"identity-provider-redirector\"\n        requirement: \"ALTERNATIVE\"\n      - flowAlias: \"forms\"\n        requirement: \"ALTERNATIVE\"\n  - alias: \"forms\"\n    description: \"Username, password, otp and other auth forms.\"\n    providerId: \"basic-flow\"\n    topLevel: false\n    builtIn: false\n    authenticationExecutions:\n      - authenticator: \"auth-username-password-form\"\n        requirement: \"REQUIRED\"\n      - authenticator: \"auth-otp-form\"\n        requirement: \"REQUIRED\"\n</code></pre>"},{"location":"architecture/iam/#authorization-implementation","title":"Authorization Implementation","text":""},{"location":"architecture/iam/#kubernetes-rbac-integration","title":"Kubernetes RBAC Integration","text":"<pre><code># ClusterRole for Platform Administrators\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: platform-admin\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\n# ClusterRoleBinding with OIDC Groups\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: platform-admin-binding\nsubjects:\n- kind: Group\n  name: \"platform-admins\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: platform-admin\n  apiGroup: rbac.authorization.k8s.io\n---\n# Namespace-scoped Role for Application Teams\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: app-web-prod\n  name: app-developer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\", \"configmaps\", \"secrets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n---\n# RoleBinding with OIDC Groups\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-developer-binding\n  namespace: app-web-prod\nsubjects:\n- kind: Group\n  name: \"web-developers\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: app-developer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"architecture/iam/#argocd-rbac-integration","title":"ArgoCD RBAC Integration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.default: role:readonly\n  policy.csv: |\n    # Platform Administrators\n    g, platform-admins, role:admin\n\n    # Application Teams\n    g, web-developers, role:web-app-admin\n    g, database-admins, role:database-admin\n\n    # Custom Roles\n    role:web-app-admin, applications, *, app-web-*/*, allow\n    role:web-app-admin, repositories, *, *, allow\n    role:web-app-admin, certificates, *, *, deny\n\n    role:database-admin, applications, *, app-database-*/*, allow\n    role:database-admin, repositories, *, *, allow\n    role:database-admin, certificates, *, *, deny\n</code></pre>"},{"location":"architecture/iam/#service-account-token-management","title":"Service Account Token Management","text":"<pre><code># Time-limited Service Account Tokens\napiVersion: v1\nkind: Secret\nmetadata:\n  name: build-robot-secret\n  annotations:\n    kubernetes.io/service-account.name: build-robot\n    kubernetes.io/service-account.token-ttl: \"3600\"  # 1 hour\ntype: kubernetes.io/service-account-token\n---\n# Projected Service Account Token (Preferred)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  serviceAccountName: build-robot\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /var/run/secrets/tokens\n      name: vault-token\n  volumes:\n  - name: vault-token\n    projected:\n      sources:\n      - serviceAccountToken:\n          path: vault-token\n          expirationSeconds: 7200  # 2 hours\n          audience: vault\n</code></pre>"},{"location":"architecture/iam/#user-lifecycle-management","title":"User Lifecycle Management","text":""},{"location":"architecture/iam/#automated-user-provisioning","title":"Automated User Provisioning","text":"<pre><code># SCIM Integration Example\nclass SCIMUserProvisioning:\n    def __init__(self, keycloak_client, kubernetes_client):\n        self.keycloak = keycloak_client\n        self.k8s = kubernetes_client\n\n    def provision_user(self, user_data):\n        # Create user in Keycloak\n        user = self.keycloak.create_user({\n            \"username\": user_data[\"username\"],\n            \"email\": user_data[\"email\"],\n            \"firstName\": user_data[\"firstName\"],\n            \"lastName\": user_data[\"lastName\"],\n            \"enabled\": True,\n            \"groups\": user_data[\"groups\"]\n        })\n\n        # Assign groups based on role\n        for group in user_data[\"groups\"]:\n            self.keycloak.assign_group_to_user(user[\"id\"], group)\n\n        # Create ServiceAccount if needed\n        if user_data.get(\"service_account\"):\n            self.create_service_account(user_data[\"username\"])\n\n    def deprovision_user(self, username):\n        # Remove from Keycloak\n        user = self.keycloak.get_user_by_username(username)\n        self.keycloak.delete_user(user[\"id\"])\n\n        # Clean up Kubernetes resources\n        self.cleanup_user_resources(username)\n</code></pre>"},{"location":"architecture/iam/#group-based-access-control","title":"Group-Based Access Control","text":"<pre><code># Keycloak Group Configuration\ngroups:\n  - name: \"platform-admins\"\n    description: \"Platform administrators with full cluster access\"\n    attributes:\n      kubernetes-role: [\"cluster-admin\"]\n      argocd-role: [\"admin\"]\n\n  - name: \"web-developers\"\n    description: \"Web application developers\"\n    attributes:\n      kubernetes-role: [\"app-developer\"]\n      kubernetes-namespaces: [\"app-web-prod\", \"app-web-staging\", \"app-web-dev\"]\n      argocd-role: [\"web-app-admin\"]\n\n  - name: \"database-admins\"\n    description: \"Database administrators\"\n    attributes:\n      kubernetes-role: [\"app-developer\"]\n      kubernetes-namespaces: [\"app-database-prod\", \"app-database-staging\"]\n      argocd-role: [\"database-admin\"]\n\n  - name: \"security-auditors\"\n    description: \"Security team with read-only access\"\n    attributes:\n      kubernetes-role: [\"view\"]\n      argocd-role: [\"readonly\"]\n</code></pre>"},{"location":"architecture/iam/#security-controls","title":"Security Controls","text":""},{"location":"architecture/iam/#token-security","title":"Token Security","text":"<pre><code># JWT Token Configuration\njwtPolicy:\n  issuer: \"https://keycloak.company.com/auth/realms/company\"\n  audiences:\n    - \"kubernetes\"\n    - \"argocd\"\n    - \"grafana\"\n  jwksUri: \"https://keycloak.company.com/auth/realms/company/protocol/openid_connect/certs\"\n\n# Token Validation Rules\ntokenValidation:\n  expiration:\n    accessToken: 900   # 15 minutes\n    refreshToken: 3600 # 1 hour\n    idToken: 300       # 5 minutes\n\n  claims:\n    required:\n      - \"iss\"\n      - \"aud\"\n      - \"exp\"\n      - \"iat\"\n      - \"sub\"\n    groups: \"groups\"\n    email: \"email\"\n    name: \"name\"\n</code></pre>"},{"location":"architecture/iam/#network-security","title":"Network Security","text":"<pre><code># Network Policy for Auth Services\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: auth-system-netpol\n  namespace: auth-system\nspec:\n  podSelector:\n    matchLabels:\n      app: dex\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    ports:\n    - protocol: TCP\n      port: 5556\n  egress:\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 443  # HTTPS to external OIDC provider\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: TCP\n      port: 443  # Kubernetes API\n</code></pre>"},{"location":"architecture/iam/#monitoring-and-audit","title":"Monitoring and Audit","text":""},{"location":"architecture/iam/#authentication-metrics","title":"Authentication Metrics","text":"<pre><code># Prometheus ServiceMonitor for Dex\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: dex-metrics\n  namespace: auth-system\nspec:\n  selector:\n    matchLabels:\n      app: dex\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n---\n# Grafana Dashboard Configuration\ndashboard:\n  title: \"IAM Authentication Metrics\"\n  panels:\n    - title: \"Authentication Requests\"\n      type: \"graph\"\n      targets:\n        - expr: \"rate(dex_requests_total[5m])\"\n          legendFormat: \"{{method}} {{code}}\"\n\n    - title: \"Active Sessions\"\n      type: \"stat\"\n      targets:\n        - expr: \"dex_sessions_active\"\n\n    - title: \"Failed Logins\"\n      type: \"graph\"\n      targets:\n        - expr: \"rate(dex_requests_total{code!~\"2..\"}[5m])\"\n          legendFormat: \"Failed Authentications\"\n</code></pre>"},{"location":"architecture/iam/#audit-logging","title":"Audit Logging","text":"<pre><code># Kubernetes Audit Policy\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n# Log authentication events\n- level: Request\n  namespaces: [\"kube-system\", \"auth-system\"]\n  verbs: [\"create\", \"update\", \"delete\"]\n  resources:\n  - group: \"\"\n    resources: [\"secrets\", \"serviceaccounts\"]\n  - group: \"rbac.authorization.k8s.io\"\n    resources: [\"roles\", \"rolebindings\", \"clusterroles\", \"clusterrolebindings\"]\n\n# Log user actions in application namespaces\n- level: Metadata\n  namespaces: [\"app-web-prod\", \"app-database-prod\"]\n  verbs: [\"create\", \"update\", \"delete\"]\n  users: [\"system:serviceaccount:*\"]\n  omitStages:\n  - RequestReceived\n</code></pre>"},{"location":"architecture/iam/#compliance-reporting","title":"Compliance Reporting","text":"<pre><code># Automated Access Review\nclass AccessReviewAutomation:\n    def __init__(self, k8s_client, keycloak_client):\n        self.k8s = k8s_client\n        self.keycloak = keycloak_client\n\n    def generate_access_report(self):\n        report = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"users\": [],\n            \"service_accounts\": [],\n            \"orphaned_resources\": []\n        }\n\n        # Get all users from Keycloak\n        keycloak_users = self.keycloak.get_all_users()\n\n        for user in keycloak_users:\n            user_report = {\n                \"username\": user[\"username\"],\n                \"email\": user[\"email\"],\n                \"groups\": self.keycloak.get_user_groups(user[\"id\"]),\n                \"last_login\": user.get(\"lastAccess\"),\n                \"kubernetes_access\": self.get_k8s_user_access(user[\"username\"])\n            }\n            report[\"users\"].append(user_report)\n\n        return report\n\n    def get_k8s_user_access(self, username):\n        # Get user's effective permissions\n        access = []\n\n        # Check ClusterRoleBindings\n        cluster_bindings = self.k8s.list_cluster_role_binding()\n        for binding in cluster_bindings.items:\n            if self.user_in_binding(username, binding):\n                access.append({\n                    \"type\": \"cluster\",\n                    \"role\": binding.role_ref.name,\n                    \"binding\": binding.metadata.name\n                })\n\n        # Check RoleBindings per namespace\n        for namespace in self.get_user_namespaces(username):\n            role_bindings = self.k8s.list_namespaced_role_binding(namespace)\n            for binding in role_bindings.items:\n                if self.user_in_binding(username, binding):\n                    access.append({\n                        \"type\": \"namespace\",\n                        \"namespace\": namespace,\n                        \"role\": binding.role_ref.name,\n                        \"binding\": binding.metadata.name\n                    })\n\n        return access\n</code></pre>"},{"location":"architecture/iam/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"architecture/iam/#identity-provider-backup","title":"Identity Provider Backup","text":"<pre><code># Keycloak Backup Configuration\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: keycloak-backup\n  namespace: auth-system\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:13\n            command:\n            - /bin/bash\n            - -c\n            - |\n              pg_dump -h keycloak-db -U keycloak keycloak &gt; /backup/keycloak-$(date +%Y%m%d).sql\n              aws s3 cp /backup/keycloak-$(date +%Y%m%d).sql s3://iam-backups/\n            env:\n            - name: PGPASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: keycloak-db-secret\n                  key: password\n            volumeMounts:\n            - name: backup-volume\n              mountPath: /backup\n          volumes:\n          - name: backup-volume\n            emptyDir: {}\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"architecture/iam/#failover-configuration","title":"Failover Configuration","text":"<pre><code># Multi-Region OIDC Provider Setup\nregions:\n  primary:\n    region: \"us-east-1\"\n    keycloak_url: \"https://keycloak-primary.company.com\"\n    dex_url: \"https://dex-primary.company.com\"\n\n  secondary:\n    region: \"us-west-2\"\n    keycloak_url: \"https://keycloak-secondary.company.com\"\n    dex_url: \"https://dex-secondary.company.com\"\n\n# Automated Failover Logic\nfailover:\n  health_check_interval: 30s\n  failure_threshold: 3\n  recovery_threshold: 2\n  dns_ttl: 60  # Low TTL for quick failover\n</code></pre> <p>This comprehensive IAM strategy provides enterprise-grade identity and access management for the RH OVE multi-cluster ecosystem, ensuring security, compliance, and operational efficiency through OIDC-based authentication and Kubernetes-native authorization.</p>"},{"location":"architecture/network/","title":"Network Architecture","text":""},{"location":"architecture/network/#overview","title":"Overview","text":"<p>The RH OVE network architecture leverages Cilium CNI for enhanced security and observability, providing advanced network capabilities through eBPF technology for both container and VM workloads.</p>"},{"location":"architecture/network/#cilium-cni-integration","title":"Cilium CNI Integration","text":"<p>Based on our research, using Cilium for RH OVE is widely regarded as a strong, future-proof approach with several key advantages:</p>"},{"location":"architecture/network/#benefits","title":"Benefits","text":"<ul> <li>Red Hat Certification: Cilium is a certified CNI plugin for OpenShift</li> <li>eBPF-Powered Enforcement: Advanced security, visibility, and traffic control</li> <li>Multi-platform Support: Works for both containers and VMs in hybrid environments</li> <li>High Performance: Superior performance compared to traditional iptables-based CNIs</li> <li>Service Mesh Capabilities: L7 security and observability without sidecar proxies</li> </ul>"},{"location":"architecture/network/#network-architecture-diagram","title":"Network Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"External Networks\"\n        EXT[External Networks]\n        LB[Load Balancers]\n    end\n\n    subgraph \"OpenShift Cluster\"\n        subgraph \"Control Plane\"\n            API[API Server]\n            ETCD[etcd]\n        end\n\n        subgraph \"Cilium Control Plane\"\n            CA[Cilium Agent]\n            CO[Cilium Operator]\n            HUB[Hubble Relay]\n        end\n\n        subgraph \"Multus Control Plane\"\n            MDS[Multus DaemonSet]\n            NADAPI[NAD API Server]\n        end\n\n        subgraph \"Node 1\"\n            subgraph \"Cilium Agent 1\"\n                EBPF1[eBPF Programs]\n                POL1[Policy Engine]\n                ENC1[Encryption]\n            end\n\n            subgraph \"Multus CNI 1\"\n                MCNI1[Multus CNI]\n                NAD1[Network Attachments]\n                SRIOV1[SR-IOV CNI]\n                MACVLAN1[MacVLAN CNI]\n            end\n\n            subgraph \"Workloads 1\"\n                POD1[Container Pods]\n                VM1[Virtual Machines]\n                MVMI1[Multi-NIC VMs]\n            end\n        end\n\n        subgraph \"Node 2\"\n            subgraph \"Cilium Agent 2\"\n                EBPF2[eBPF Programs]\n                POL2[Policy Engine]\n                ENC2[Encryption]\n            end\n\n            subgraph \"Multus CNI 2\"\n                MCNI2[Multus CNI]\n                NAD2[Network Attachments]\n                SRIOV2[SR-IOV CNI]\n                MACVLAN2[MacVLAN CNI]\n            end\n\n            subgraph \"Workloads 2\"\n                POD2[Container Pods]\n                VM2[Virtual Machines]\n                MVMI2[Multi-NIC VMs]\n            end\n        end\n    end\n\n    EXT --&gt; LB\n    LB --&gt; API\n    API --&gt; CA\n    API --&gt; MDS\n    CA --&gt; CO\n    CO --&gt; HUB\n\n    NADAPI --&gt; NAD1\n    NADAPI --&gt; NAD2\n\n    EBPF1 --&gt; POD1\n    EBPF1 --&gt; VM1\n    EBPF2 --&gt; POD2\n    EBPF2 --&gt; VM2\n\n    MCNI1 --&gt; MVMI1\n    MCNI2 --&gt; MVMI2\n    NAD1 --&gt; SRIOV1\n    NAD1 --&gt; MACVLAN1\n    NAD2 --&gt; SRIOV2\n    NAD2 --&gt; MACVLAN2\n\n    SRIOV1 --&gt; MVMI1\n    MACVLAN1 --&gt; MVMI1\n    SRIOV2 --&gt; MVMI2\n    MACVLAN2 --&gt; MVMI2\n\n    POL1 --&gt; EBPF1\n    POL2 --&gt; EBPF2\n    ENC1 --&gt; EBPF1\n    ENC2 --&gt; EBPF2</code></pre>"},{"location":"architecture/network/#network-security-features","title":"Network Security Features","text":""},{"location":"architecture/network/#identity-aware-security","title":"Identity-Aware Security","text":"<pre><code>graph LR\n    subgraph \"Traditional Security\"\n        A[IP-based Rules]\n        B[Port-based Rules]\n    end\n\n    subgraph \"Cilium Identity-Aware\"\n        C[Label-based Identity]\n        D[Service Identity]\n        E[Namespace Identity]\n        F[Application Identity]\n    end\n\n    A --&gt; G[Limited Flexibility]\n    B --&gt; G\n\n    C --&gt; H[Zero Trust Architecture]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H</code></pre>"},{"location":"architecture/network/#network-policies","title":"Network Policies","text":""},{"location":"architecture/network/#basic-network-policy-for-vms","title":"Basic Network Policy for VMs","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: vm-web-policy\n  namespace: app-web-prod\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-vm\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: api-gateway\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      - port: \"443\"\n        protocol: TCP\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        app: database-vm\n    toPorts:\n    - ports:\n      - port: \"5432\"\n        protocol: TCP\n</code></pre>"},{"location":"architecture/network/#l7-http-policy","title":"L7 HTTP Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l7-http-policy\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-api\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"\n        - method: \"POST\"\n          path: \"/api/v1/users\"\n</code></pre>"},{"location":"architecture/network/#multi-tenant-networking","title":"Multi-Tenant Networking","text":""},{"location":"architecture/network/#namespace-isolation","title":"Namespace Isolation","text":"<pre><code>graph TB\n    subgraph \"Tenant A Namespace\"\n        A1[VM Workloads A]\n        A2[Container Workloads A]\n        A3[Network Policies A]\n    end\n\n    subgraph \"Tenant B Namespace\"\n        B1[VM Workloads B]\n        B2[Container Workloads B]\n        B3[Network Policies B]\n    end\n\n    subgraph \"Shared Services\"\n        S1[DNS]\n        S2[Monitoring]\n        S3[Logging]\n    end\n\n    A3 --&gt; A1\n    A3 --&gt; A2\n    B3 --&gt; B1\n    B3 --&gt; B2\n\n    A1 -.-&gt; S1\n    A2 -.-&gt; S2\n    B1 -.-&gt; S1\n    B2 -.-&gt; S3</code></pre>"},{"location":"architecture/network/#networkattachmentdefinition-for-vms","title":"NetworkAttachmentDefinition for VMs","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: vm-network\n  namespace: app-database-prod\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"vm-network\",\n      \"type\": \"cilium-cni\",\n      \"ipam\": {\n        \"type\": \"cilium\"\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#multus-multi-network-configuration","title":"Multus Multi-Network Configuration","text":""},{"location":"architecture/network/#overview_1","title":"Overview","text":"<p>Multus CNI enables attaching multiple network interfaces to pods and VMs, allowing for complex networking scenarios such as: - Separation of management and data traffic - VLAN-based network segmentation - High-performance networking with SR-IOV - Legacy application networking requirements</p> <pre><code>graph TB\n    subgraph \"VM with Multiple Network Interfaces\"\n        A[Virtual Machine]\n        B[eth0 - Default Network]\n        C[eth1 - Management Network]\n        D[eth2 - Storage Network]\n        E[eth3 - SR-IOV Network]\n    end\n\n    subgraph \"Network Attachments\"\n        F[Default CNI - Cilium]\n        G[Management NAD]\n        H[Storage NAD]\n        I[SR-IOV NAD]\n    end\n\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n\n    B --&gt; F\n    C --&gt; G\n    D --&gt; H\n    E --&gt; I</code></pre>"},{"location":"architecture/network/#multus-installation","title":"Multus Installation","text":"<p>Multus is typically installed as part of OpenShift Container Platform:</p> <pre><code># Verify Multus is installed\noc get network.operator.openshift.io cluster -o yaml\n\n# Check Multus DaemonSet\noc get ds multus -n openshift-multus\n</code></pre>"},{"location":"architecture/network/#network-attachment-definitions-nads","title":"Network Attachment Definitions (NADs)","text":""},{"location":"architecture/network/#management-network-nad","title":"Management Network NAD","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: management-network\n  namespace: vm-infrastructure\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"management-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"192.168.100.0/24\",\n            \"gateway\": \"192.168.100.1\"\n          }\n        ],\n        \"dns\": {\n          \"nameservers\": [\"192.168.100.10\", \"8.8.8.8\"]\n        }\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#storage-network-nad","title":"Storage Network NAD","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: storage-network\n  namespace: vm-infrastructure\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"storage-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens224\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"10.0.200.0/24\"\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#vlan-based-network-nad","title":"VLAN-based Network NAD","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: vlan-100-network\n  namespace: vm-production\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"vlan-100-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192.100\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"dhcp\"\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#sr-iov-network-nad","title":"SR-IOV Network NAD","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: sriov-high-performance\n  namespace: vm-production\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"sriov-high-performance\",\n      \"type\": \"sriov\",\n      \"deviceID\": \"1017\",\n      \"vf\": 0,\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"10.0.50.0/24\"\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#vm-configuration-with-multiple-network-cards","title":"VM Configuration with Multiple Network Cards","text":""},{"location":"architecture/network/#vm-with-multiple-interfaces","title":"VM with Multiple Interfaces","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: multi-network-vm\n  namespace: vm-infrastructure\n  annotations:\n    k8s.v1.cni.cncf.io/networks: |\n      [\n        {\n          \"name\": \"management-network\",\n          \"ips\": [\"192.168.100.50/24\"]\n        },\n        {\n          \"name\": \"storage-network\",\n          \"ips\": [\"10.0.200.50/24\"]\n        },\n        {\n          \"name\": \"vlan-100-network\"\n        }\n      ]\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        app: multi-network-app\n    spec:\n      domain:\n        cpu:\n          cores: 4\n        memory:\n          guest: 8Gi\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: management\n            bridge: {}\n          - name: storage\n            bridge: {}\n          - name: vlan-network\n            bridge: {}\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n        resources:\n          requests:\n            cpu: 4\n            memory: 8Gi\n      networks:\n      - name: default\n        pod: {}\n      - name: management\n        multus:\n          networkName: management-network\n      - name: storage\n        multus:\n          networkName: storage-network\n      - name: vlan-network\n        multus:\n          networkName: vlan-100-network\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: multi-network-vm-root\n</code></pre>"},{"location":"architecture/network/#high-performance-vm-with-sr-iov","title":"High-Performance VM with SR-IOV","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: high-perf-vm\n  namespace: vm-production\n  annotations:\n    k8s.v1.cni.cncf.io/networks: |\n      [\n        {\n          \"name\": \"management-network\",\n          \"ips\": [\"192.168.100.100/24\"]\n        },\n        {\n          \"name\": \"sriov-high-performance\",\n          \"ips\": [\"10.0.50.100/24\"]\n        }\n      ]\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        app: high-performance-app\n    spec:\n      domain:\n        cpu:\n          cores: 8\n          dedicatedCpuPlacement: true\n        memory:\n          guest: 16Gi\n          hugepages:\n            pageSize: 1Gi\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: management\n            bridge: {}\n          - name: sriov-net\n            sriov: {}\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n        resources:\n          requests:\n            cpu: 8\n            memory: 16Gi\n            hugepages-1Gi: 16Gi\n      networks:\n      - name: default\n        pod: {}\n      - name: management\n        multus:\n          networkName: management-network\n      - name: sriov-net\n        multus:\n          networkName: sriov-high-performance\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: high-perf-vm-root\n</code></pre>"},{"location":"architecture/network/#advanced-multus-configurations","title":"Advanced Multus Configurations","text":""},{"location":"architecture/network/#bond-network-interface","title":"Bond Network Interface","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: bond-network\n  namespace: vm-infrastructure\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"bond-network\",\n      \"type\": \"bond\",\n      \"mode\": \"active-backup\",\n      \"miimon\": \"100\",\n      \"links\": [\n        {\"name\": \"ens192\"},\n        {\"name\": \"ens224\"}\n      ],\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"10.0.100.0/24\",\n            \"gateway\": \"10.0.100.1\"\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#ovs-bridge-network","title":"OVS Bridge Network","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ovs-bridge-network\n  namespace: vm-infrastructure\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"ovs-bridge-network\",\n      \"type\": \"ovs\",\n      \"bridge\": \"br-data\",\n      \"vlan\": 200,\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"10.0.200.0/24\"\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"architecture/network/#encryption-and-security","title":"Encryption and Security","text":""},{"location":"architecture/network/#transparent-encryption","title":"Transparent Encryption","text":"<p>Cilium supports both IPsec and WireGuard for transparent encryption:</p> <pre><code># Cilium ConfigMap for WireGuard encryption\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-wireguard: \"true\"\n  wireguard-userspace-fallback: \"true\"\n</code></pre>"},{"location":"architecture/network/#service-mesh-without-sidecars","title":"Service Mesh without Sidecars","text":"<pre><code>graph LR\n    subgraph \"Traditional Service Mesh\"\n        A[Application Pod]\n        B[Sidecar Proxy]\n        C[Network]\n    end\n\n    subgraph \"Cilium Service Mesh\"\n        D[Application Pod]\n        E[eBPF in Kernel]\n        F[Network]\n    end\n\n    A --&gt; B\n    B --&gt; C\n\n    D --&gt; E\n    E --&gt; F\n\n    G[Higher Resource Usage] --&gt; A\n    H[Lower Latency] --&gt; D\n    I[Better Performance] --&gt; D</code></pre>"},{"location":"architecture/network/#observability-with-hubble","title":"Observability with Hubble","text":""},{"location":"architecture/network/#network-visibility","title":"Network Visibility","text":"<pre><code>graph TB\n    subgraph \"Hubble Observability\"\n        A[Hubble Agent]\n        B[Hubble Relay]\n        C[Hubble UI]\n        D[Hubble CLI]\n    end\n\n    subgraph \"Data Sources\"\n        E[Network Flows]\n        F[Service Dependencies]\n        G[Security Events]\n        H[Performance Metrics]\n    end\n\n    E --&gt; A\n    F --&gt; A\n    G --&gt; A\n    H --&gt; A\n\n    A --&gt; B\n    B --&gt; C\n    B --&gt; D</code></pre>"},{"location":"architecture/network/#hubble-configuration","title":"Hubble Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-hubble: \"true\"\n  hubble-listen-address: \":4244\"\n  hubble-socket-path: \"/var/run/cilium/hubble.sock\"\n  hubble-metrics-server: \":9091\"\n  hubble-metrics: &gt;-\n    dns:query;ignoreAAAA\n    drop\n    tcp\n    flow\n    icmp\n    http\n</code></pre>"},{"location":"architecture/network/#vm-specific-networking","title":"VM-Specific Networking","text":""},{"location":"architecture/network/#vm-network-integration","title":"VM Network Integration","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: database-vm\n  namespace: app-database-prod\nspec:\n  template:\n    spec:\n      networks:\n      - name: default\n        pod: {}\n      - name: vm-network\n        multus:\n          networkName: vm-network\n      domain:\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: vm-network\n            bridge: {}\n</code></pre>"},{"location":"architecture/network/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/network/#ebpf-performance-benefits","title":"eBPF Performance Benefits","text":"<ol> <li>Bypass iptables overhead: Direct kernel-space processing</li> <li>Reduced context switches: Fewer user-space to kernel-space transitions</li> <li>Optimized packet processing: Custom eBPF programs for specific workloads</li> <li>Hardware acceleration: Support for XDP (eXpress Data Path)</li> </ol>"},{"location":"architecture/network/#network-performance-tuning","title":"Network Performance Tuning","text":"<pre><code># Cilium DaemonSet configuration for performance\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\nspec:\n  template:\n    spec:\n      containers:\n      - name: cilium-agent\n        args:\n        - --enable-bandwidth-manager=true\n        - --enable-local-redirect-policy=true\n        - --kube-proxy-replacement=strict\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n</code></pre>"},{"location":"architecture/network/#integration-with-external-systems","title":"Integration with External Systems","text":""},{"location":"architecture/network/#load-balancer-integration","title":"Load Balancer Integration","text":"<pre><code>apiVersion: cilium.io/v2alpha1\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: vm-pool\nspec:\n  cidrs:\n  - cidr: \"10.100.0.0/24\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vm-web-service\n  annotations:\n    io.cilium/lb-ipam-ips: \"10.100.0.10\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: web-vm\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"architecture/network/#troubleshooting-and-monitoring","title":"Troubleshooting and Monitoring","text":""},{"location":"architecture/network/#network-flow-monitoring","title":"Network Flow Monitoring","text":"<pre><code># Monitor network flows\nhubble observe --namespace app-web-prod\n\n# Check policy violations\nhubble observe --verdict DENIED\n\n# Monitor specific VM traffic\nhubble observe --pod vm-database-vm-xxx\n</code></pre>"},{"location":"architecture/network/#performance-metrics","title":"Performance Metrics","text":"<p>Key metrics to monitor: - Network throughput per VM/pod - Policy enforcement latency - eBPF program execution time - Hubble flow processing rate</p> <p>This network architecture provides enterprise-grade security, performance, and observability for mixed VM and container workloads in the RH OVE environment.</p>"},{"location":"architecture/overview/","title":"RH OVE Solution Design and Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>This document provides an overview of the RH OVE solution, detailing the architecture, deployment, and management strategies.</p>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<ul> <li>Utilize a namespace-based topology for isolation and security.</li> <li>Implement Cilium for network security using eBPF.</li> <li>Integrate multiplex workloads to optimize resource utilization.</li> </ul>"},{"location":"architecture/overview/#network-architecture","title":"Network Architecture","text":"<p>Mermaid diagram for network architecture:</p> <pre><code>graph TD;\n  A[User] --&gt;|Access| B{OpenShift\n  Virtualization}\n  B --&gt; C(Cilium)\n  C --&gt; D[VM Workloads]\n  C --&gt; E[Container Workloads]</code></pre>"},{"location":"architecture/overview/#storage-architecture","title":"Storage Architecture","text":"<p>Include storage considerations and architecture here.</p>"},{"location":"architecture/storage/","title":"Storage Architecture","text":""},{"location":"architecture/storage/#overview","title":"Overview","text":"<p>The RH OVE storage architecture provides unified storage management for both container and VM workloads, leveraging Kubernetes-native storage concepts while supporting traditional VM storage requirements.</p>"},{"location":"architecture/storage/#storage-components","title":"Storage Components","text":"<pre><code>graph TB\n    subgraph \"Storage Infrastructure\"\n        A[Storage Classes]\n        B[Persistent Volumes]\n        C[Container Storage Interface]\n        D[Storage Backends]\n    end\n\n    subgraph \"VM Storage\"\n        E[DataVolumes]\n        F[VM Disks]\n        G[CDI - Containerized Data Importer]\n        H[VM Templates]\n    end\n\n    subgraph \"Container Storage\"\n        I[Persistent Volume Claims]\n        J[ConfigMaps]\n        K[Secrets]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n\n    E --&gt; G\n    F --&gt; E\n    H --&gt; E\n\n    I --&gt; B\n    J --&gt; K\n\n    B --&gt; E\n    B --&gt; I</code></pre>"},{"location":"architecture/storage/#datavolume-management","title":"DataVolume Management","text":""},{"location":"architecture/storage/#datavolume-crd","title":"DataVolume CRD","text":"<p>DataVolumes are the primary mechanism for managing VM storage in RH OVE:</p> <pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: web-vm-disk\n  namespace: app-web-prod\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 50Gi\n    storageClassName: fast-ssd\n  source:\n    registry:\n      url: \"docker://registry.redhat.io/rhel8/rhel:latest\"\n</code></pre>"},{"location":"architecture/storage/#storage-import-patterns","title":"Storage Import Patterns","text":"<pre><code>graph LR\n    subgraph \"Data Sources\"\n        A[Container Registry]\n        B[HTTP Server]\n        C[S3 Bucket]\n        D[Existing PVC]\n    end\n\n    subgraph \"CDI Processing\"\n        E[Import Pod]\n        F[Data Processing]\n        G[Format Conversion]\n    end\n\n    subgraph \"Target Storage\"\n        H[DataVolume]\n        I[PVC]\n        J[VM Disk]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J</code></pre>"},{"location":"architecture/storage/#storage-classes-and-performance-tiers","title":"Storage Classes and Performance Tiers","text":""},{"location":"architecture/storage/#performance-tiers","title":"Performance Tiers","text":"<pre><code># High Performance SSD Storage Class\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: high-performance-ssd\nprovisioner: kubernetes.io/no-provisioner\nparameters:\n  type: ssd\n  iops: \"10000\"\n  throughput: \"500Mi\"\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n---\n# Standard HDD Storage Class\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: standard-hdd\nprovisioner: kubernetes.io/no-provisioner\nparameters:\n  type: hdd\n  iops: \"1000\"\n  throughput: \"100Mi\"\nreclaimPolicy: Retain\nvolumeBindingMode: WaitForFirstConsumer\n---\n# Archive Storage Class\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: archive-storage\nprovisioner: kubernetes.io/no-provisioner\nparameters:\n  type: archive\n  iops: \"100\"\n  throughput: \"50Mi\"\nreclaimPolicy: Retain\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"architecture/storage/#vm-disk-configuration","title":"VM Disk Configuration","text":""},{"location":"architecture/storage/#vm-with-multiple-disks","title":"VM with Multiple Disks","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: database-vm\n  namespace: app-database-prod\nspec:\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n          - name: datadisk\n            disk:\n              bus: virtio\n          - name: logdisk\n            disk:\n              bus: virtio\n        resources:\n          requests:\n            memory: 8Gi\n            cpu: 4\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: db-vm-root\n      - name: datadisk\n        dataVolume:\n          name: db-vm-data\n      - name: logdisk\n        dataVolume:\n          name: db-vm-logs\n</code></pre>"},{"location":"architecture/storage/#datavolume-for-different-use-cases","title":"DataVolume for Different Use Cases","text":"<pre><code># Boot disk from registry\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: db-vm-root\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 30Gi\n    storageClassName: high-performance-ssd\n  source:\n    registry:\n      url: \"docker://registry.access.redhat.com/rhel8/rhel:latest\"\n---\n# Data disk - blank\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: db-vm-data\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 500Gi\n    storageClassName: standard-hdd\n  source:\n    blank: {}\n---\n# Log disk - blank\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: db-vm-logs\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 100Gi\n    storageClassName: standard-hdd\n  source:\n    blank: {}\n</code></pre>"},{"location":"architecture/storage/#storage-backup-integration","title":"Storage Backup Integration","text":""},{"location":"architecture/storage/#rubrik-integration-for-vm-storage","title":"Rubrik Integration for VM Storage","text":"<p>Based on our research, Rubrik provides certified integration with RH OVE for VM backup:</p> <pre><code>graph TB\n    subgraph \"RH OVE Cluster\"\n        A[Virtual Machines]\n        B[DataVolumes]\n        C[Persistent Volumes]\n        D[Rubrik Agent]\n    end\n\n    subgraph \"Rubrik Platform\"\n        E[Rubrik Cluster]\n        F[Backup Policies]\n        G[Recovery Points]\n        H[Immutable Storage]\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H</code></pre>"},{"location":"architecture/storage/#backup-policy-configuration","title":"Backup Policy Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rubrik-backup-policy\n  namespace: app-database-prod\ndata:\n  policy.yaml: |\n    vm_backup_policy:\n      name: \"database-vm-backup\"\n      frequency: \"daily\"\n      retention: \"30d\"\n      snapshot_consistency: \"crash-consistent\"\n      backup_window: \"02:00-06:00\"\n      exclude_disks:\n        - \"temp-disk\"\n        - \"swap-disk\"\n</code></pre>"},{"location":"architecture/storage/#storage-monitoring-and-performance","title":"Storage Monitoring and Performance","text":""},{"location":"architecture/storage/#storage-metrics","title":"Storage Metrics","text":"<p>Key storage metrics to monitor:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: storage-metrics\nspec:\n  selector:\n    matchLabels:\n      app: cdi-controller\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n</code></pre>"},{"location":"architecture/storage/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>graph LR\n    subgraph \"Storage Metrics\"\n        A[IOPS per VM]\n        B[Throughput per Volume]\n        C[Latency Metrics]\n        D[Storage Utilization]\n    end\n\n    subgraph \"Monitoring Stack\"\n        E[Prometheus]\n        F[Grafana]\n        G[Alert Manager]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F\n    E --&gt; G</code></pre>"},{"location":"architecture/storage/#storage-operations","title":"Storage Operations","text":""},{"location":"architecture/storage/#volume-expansion","title":"Volume Expansion","text":"<pre><code># Expand a DataVolume\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: web-vm-disk\n  namespace: app-web-prod\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 100Gi  # Increased from 50Gi\n    storageClassName: fast-ssd\n  source:\n    pvc:\n      name: web-vm-disk\n      namespace: app-web-prod\n</code></pre>"},{"location":"architecture/storage/#volume-cloning","title":"Volume Cloning","text":"<pre><code># Clone a DataVolume for VM template\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: web-vm-template-clone\n  namespace: vm-templates\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 50Gi\n    storageClassName: fast-ssd\n  source:\n    pvc:\n      name: web-vm-golden-image\n      namespace: vm-templates\n</code></pre>"},{"location":"architecture/storage/#snapshot-management","title":"Snapshot Management","text":"<pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: db-vm-snapshot-pre-upgrade\n  namespace: app-database-prod\nspec:\n  volumeSnapshotClassName: csi-snapshotter\n  source:\n    persistentVolumeClaimName: db-vm-data\n</code></pre>"},{"location":"architecture/storage/#storage-security","title":"Storage Security","text":""},{"location":"architecture/storage/#encryption-at-rest","title":"Encryption at Rest","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: encrypted-storage\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  encrypted: \"true\"\n  kmsKeyId: \"arn:aws:kms:region:account:key/key-id\"\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"architecture/storage/#access-control","title":"Access Control","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: app-database-prod\n  name: storage-admin\nrules:\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"cdi.kubevirt.io\"]\n  resources: [\"datavolumes\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"snapshot.storage.k8s.io\"]\n  resources: [\"volumesnapshots\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n</code></pre>"},{"location":"architecture/storage/#best-practices","title":"Best Practices","text":""},{"location":"architecture/storage/#storage-planning","title":"Storage Planning","text":"<ol> <li>Right-size storage: Match storage performance to workload requirements</li> <li>Use appropriate storage classes: Different tiers for different use cases</li> <li>Plan for growth: Consider volume expansion capabilities</li> <li>Backup strategy: Regular snapshots and external backups</li> </ol>"},{"location":"architecture/storage/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Use SSD for high-IOPS workloads: Database and application storage</li> <li>Separate storage by function: OS, data, logs, and temp on different volumes</li> <li>Monitor storage metrics: Track IOPS, throughput, and latency</li> <li>Implement storage quotas: Prevent storage exhaustion</li> </ol>"},{"location":"architecture/storage/#security-considerations","title":"Security Considerations","text":"<ol> <li>Enable encryption at rest: For sensitive data storage</li> <li>Implement access controls: RBAC for storage resources</li> <li>Regular security scanning: Check for storage-related vulnerabilities</li> <li>Audit storage access: Monitor who accesses what storage</li> </ol> <p>This storage architecture ensures reliable, performant, and secure storage management for the hybrid VM and container workloads in the RH OVE environment.</p>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/","title":"ADR-001: Multi-Cluster Architecture Pattern","text":""},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#context","title":"Context","text":"<p>The RH OVE ecosystem needs to support multiple environments (production, staging, development) while maintaining centralized governance, security, and operational oversight. The organization requires scalable infrastructure that can grow horizontally and support geographic distribution.</p>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#decision","title":"Decision","text":"<p>We will implement a multi-cluster architecture pattern with: - One Management Cluster: Centralized control plane for governance, GitOps, security, and monitoring - Multiple Application Clusters: Dedicated workload execution environments per environment type</p>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#advantages","title":"Advantages","text":"<ol> <li>Separation of Concerns: Clear boundaries between management and workload execution</li> <li>Scalability: Horizontal scaling by adding application clusters as needed</li> <li>Security: Network-level isolation between environments</li> <li>Operational Efficiency: Centralized management reduces operational overhead</li> <li>Fault Isolation: Issues in one cluster don't affect others</li> <li>Resource Optimization: Right-size clusters based on workload requirements</li> </ol>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Single Large Cluster: Rejected due to blast radius and resource contention</li> <li>Completely Separate Clusters: Rejected due to operational complexity and lack of centralized governance</li> <li>Namespace-based Multi-tenancy: Rejected due to insufficient isolation for production workloads</li> </ol>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#management-cluster-components","title":"Management Cluster Components","text":"<ul> <li>Red Hat Advanced Cluster Management (RHACM)</li> <li>ArgoCD Hub for GitOps</li> <li>Red Hat Advanced Cluster Security (RHACS)</li> <li>Federated Prometheus for monitoring</li> <li>Centralized logging aggregation</li> <li>Rubrik backup management</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#application-cluster-types","title":"Application Cluster Types","text":"<ul> <li>Production: High-availability, performance-optimized</li> <li>Staging: Production-like for testing</li> <li>Development: Resource-optimized for development workflows</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#network-architecture","title":"Network Architecture","text":"<ul> <li>Dedicated network segments per cluster type</li> <li>VPN/Private connectivity between management and application clusters</li> <li>Zero-trust network principles</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#positive","title":"Positive","text":"<ul> <li>Improved security posture through cluster-level isolation</li> <li>Simplified compliance and audit processes</li> <li>Better resource utilization and cost optimization</li> <li>Enhanced disaster recovery capabilities</li> <li>Reduced blast radius for security incidents</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#negative","title":"Negative","text":"<ul> <li>Increased network complexity</li> <li>Additional operational overhead for cluster lifecycle management</li> <li>Potential data synchronization challenges</li> <li>Learning curve for multi-cluster operations</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>Meets enterprise security requirements for environment isolation</li> <li>Supports regulatory compliance through audit trail separation</li> <li>Enables data residency requirements through geographic cluster placement</li> </ul>"},{"location":"architecture/adr/adr-001-multi-cluster-pattern/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Centralized metrics collection via Prometheus federation</li> <li>Unified logging through log forwarding to management cluster</li> <li>Cross-cluster distributed tracing capabilities</li> <li>Centralized alerting and incident management</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/","title":"ADR-002: GitOps with ArgoCD Hub Architecture","text":""},{"location":"architecture/adr/adr-002-gitops-argocd/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-002-gitops-argocd/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-002-gitops-argocd/#context","title":"Context","text":"<p>The multi-cluster RH OVE ecosystem requires a consistent, auditable, and scalable approach to application deployment and configuration management across multiple clusters. Traditional CI/CD approaches with push-based deployments create security concerns and operational complexity in multi-cluster environments.</p>"},{"location":"architecture/adr/adr-002-gitops-argocd/#decision","title":"Decision","text":"<p>We will implement GitOps using ArgoCD in a hub-spoke pattern: - ArgoCD Hub: Centralized ArgoCD instance in the management cluster - ArgoCD Agents: Lightweight agents in each application cluster - Git-based Configuration: All infrastructure and application configurations stored in Git repositories</p>"},{"location":"architecture/adr/adr-002-gitops-argocd/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-002-gitops-argocd/#advantages","title":"Advantages","text":"<ol> <li>Declarative Configuration: Infrastructure and applications defined as code</li> <li>Audit Trail: Complete Git history of all changes</li> <li>Security: Pull-based model eliminates need for external access to clusters</li> <li>Consistency: Identical deployment processes across all environments</li> <li>Rollback Capability: Easy rollback using Git revert operations</li> <li>Self-Healing: Automatic drift correction and reconciliation</li> </ol>"},{"location":"architecture/adr/adr-002-gitops-argocd/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Jenkins-based CI/CD: Rejected due to security concerns with push-based deployments</li> <li>Tekton Pipelines: Rejected due to complexity in multi-cluster scenarios</li> <li>Fleet by Rancher: Rejected due to vendor lock-in concerns</li> <li>Flux: Rejected due to preference for ArgoCD's UI and workflow capabilities</li> </ol>"},{"location":"architecture/adr/adr-002-gitops-argocd/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-002-gitops-argocd/#argocd-hub-configuration","title":"ArgoCD Hub Configuration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ArgoCD\nmetadata:\n  name: argocd-hub\n  namespace: argocd\nspec:\n  server:\n    replicas: 3\n    route:\n      enabled: true\n      tls:\n        termination: reencrypt\n  ha:\n    enabled: true\n  dex:\n    openShiftOAuth: true\n  rbac:\n    defaultPolicy: 'role:readonly'\n</code></pre>"},{"location":"architecture/adr/adr-002-gitops-argocd/#repository-structure","title":"Repository Structure","text":"<pre><code>gitops-repo/\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 management/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 development/\n\u251c\u2500\u2500 applications/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 overlays/\n\u2514\u2500\u2500 infrastructure/\n    \u251c\u2500\u2500 networking/\n    \u251c\u2500\u2500 storage/\n    \u2514\u2500\u2500 monitoring/\n</code></pre>"},{"location":"architecture/adr/adr-002-gitops-argocd/#application-of-applications-pattern","title":"Application of Applications Pattern","text":"<ul> <li>Root ArgoCD Application manages cluster-specific applications</li> <li>Environment-specific overlays using Kustomize</li> <li>Automated sync policies for non-production environments</li> <li>Manual sync for production deployments</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-002-gitops-argocd/#positive","title":"Positive","text":"<ul> <li>Enhanced Security: No direct cluster access required for deployments</li> <li>Improved Compliance: Complete audit trail through Git history</li> <li>Reduced Operational Overhead: Automated deployment and drift correction</li> <li>Better Collaboration: Git-based workflows familiar to development teams</li> <li>Disaster Recovery: Easy recreation of cluster state from Git</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/#negative","title":"Negative","text":"<ul> <li>Learning Curve: Teams need to adapt to GitOps workflows</li> <li>Git Repository Complexity: Large repositories can become difficult to manage</li> <li>Network Dependencies: Requires reliable connectivity to Git repositories</li> <li>Secret Management: Additional complexity for managing sensitive data</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/#security-considerations","title":"Security Considerations","text":"<ul> <li>ArgoCD service accounts use minimal required permissions</li> <li>Private Git repositories with SSH key authentication</li> <li>RBAC integration with OpenShift OAuth</li> <li>Secret management through External Secrets Operator</li> <li>Network policies restrict ArgoCD communication</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<ul> <li>ArgoCD application health monitoring</li> <li>Git repository sync status tracking</li> <li>Deployment success/failure notifications</li> <li>Performance metrics for sync operations</li> <li>Custom dashboards for GitOps workflows</li> </ul>"},{"location":"architecture/adr/adr-002-gitops-argocd/#migration-strategy","title":"Migration Strategy","text":"<ol> <li>Phase 1: Deploy ArgoCD hub in management cluster</li> <li>Phase 2: Migrate existing applications to Git repositories</li> <li>Phase 3: Configure ArgoCD applications for each cluster</li> <li>Phase 4: Implement automated sync for non-production</li> <li>Phase 5: Train teams on GitOps workflows</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/","title":"ADR-003: Namespace-Based Cluster Topology","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-003-cluster-topology/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-003-cluster-topology/#context","title":"Context","text":"<p>The RH OVE ecosystem requires an efficient organizational strategy for managing mixed VM and container workloads within clusters. We need to balance isolation, security, resource management, and operational simplicity while supporting multi-tenant use cases.</p>"},{"location":"architecture/adr/adr-003-cluster-topology/#decision","title":"Decision","text":"<p>We will implement an application namespace-based topology where resources are organized by business application or domain rather than by resource type or technology stack.</p>"},{"location":"architecture/adr/adr-003-cluster-topology/#topology-structure","title":"Topology Structure","text":"<pre><code>Cluster\n\u251c\u2500\u2500 app-web-prod (namespace)\n\u2502   \u251c\u2500\u2500 VMs (web servers)\n\u2502   \u251c\u2500\u2500 Containers (microservices)\n\u2502   \u251c\u2500\u2500 Storage (PVCs, DataVolumes)\n\u2502   \u2514\u2500\u2500 Network (NetworkPolicies, NADs)\n\u251c\u2500\u2500 app-database-prod (namespace)\n\u2502   \u251c\u2500\u2500 VMs (database servers)\n\u2502   \u251c\u2500\u2500 Containers (database operators)\n\u2502   \u251c\u2500\u2500 Storage (high-performance storage)\n\u2502   \u2514\u2500\u2500 Network (isolated database networks)\n\u2514\u2500\u2500 app-monitoring-prod (namespace)\n    \u251c\u2500\u2500 VMs (legacy monitoring tools)\n    \u251c\u2500\u2500 Containers (modern observability stack)\n    \u251c\u2500\u2500 Storage (metrics and logs storage)\n    \u2514\u2500\u2500 Network (monitoring networks)\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#advantages","title":"Advantages","text":"<ol> <li>Strong Isolation: Each application has its own security boundary</li> <li>Simplified RBAC: Teams get namespace-level access aligned with their applications</li> <li>Clear Resource Attribution: Easy to track costs and resource usage per application</li> <li>Network Microsegmentation: Network policies can be application-specific</li> <li>Operational Clarity: Troubleshooting and maintenance scoped to business context</li> <li>Compliance Alignment: Audit boundaries match business applications</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#1-technology-based-topology","title":"1. Technology-Based Topology","text":"<ul> <li>Structure: Separate namespaces for VMs vs containers</li> <li>Rejected: Creates artificial barriers between related workloads</li> <li>Issues: Complex cross-namespace communication, unclear ownership</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#2-environment-based-topology-only","title":"2. Environment-Based Topology Only","text":"<ul> <li>Structure: Single namespace per environment (prod, staging, dev)</li> <li>Rejected: Poor isolation between different applications</li> <li>Issues: Resource contention, security boundary concerns</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#3-team-based-topology","title":"3. Team-Based Topology","text":"<ul> <li>Structure: Namespaces per team/department</li> <li>Rejected: Teams often work on multiple applications</li> <li>Issues: Unclear application boundaries, resource conflicts</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#namespace-naming-convention","title":"Namespace Naming Convention","text":"<pre><code># Pattern: {app-name}-{environment}\n# Examples:\n- app-web-prod\n- app-web-staging  \n- app-web-dev\n- app-database-prod\n- app-analytics-prod\n- shared-monitoring-prod\n- shared-storage-prod\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#standard-namespace-template","title":"Standard Namespace Template","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: app-web-prod\n  labels:\n    application: web\n    environment: production\n    tier: frontend\n    cost-center: \"12345\"\n    owner: web-team\n  annotations:\n    backup-policy: \"daily\"\n    monitoring-enabled: \"true\"\n    network-policy: \"strict\"\n    compliance-level: \"high\"\nspec:\n  finalizers:\n  - kubernetes\n---\n# Resource Quota per namespace\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: app-web-prod\nspec:\n  hard:\n    requests.cpu: \"20\"\n    requests.memory: 40Gi\n    limits.cpu: \"40\"\n    limits.memory: 80Gi\n    persistentvolumeclaims: \"20\"\n    services: \"10\"\n    secrets: \"20\"\n    configmaps: \"20\"\n---\n# Limit Range per namespace\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: resource-limits\n  namespace: app-web-prod\nspec:\n  limits:\n  - default:\n      cpu: \"2\"\n      memory: 4Gi\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    type: Container\n  - default:\n      cpu: \"8\"\n      memory: 16Gi\n    defaultRequest:\n      cpu: \"2\"\n      memory: 4Gi\n    type: PersistentVolumeClaim\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#cross-namespace-communication-policy","title":"Cross-Namespace Communication Policy","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-app-communication\n  namespace: app-web-prod\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow ingress from gateway namespace\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: shared-gateway-prod\n  egress:\n  # Allow egress to database namespace\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          application: database\n          environment: production\n    ports:\n    - protocol: TCP\n      port: 5432\n  # Allow egress to shared services\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          tier: shared-services\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#governance-and-management","title":"Governance and Management","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#namespace-lifecycle-management","title":"Namespace Lifecycle Management","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: namespace-manager\n  namespace: argocd\nspec:\n  project: infrastructure\n  source:\n    repoURL: https://git.company.com/infrastructure/namespaces\n    targetRevision: HEAD\n    path: namespaces\n  destination:\n    server: https://kubernetes.default.svc\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#rbac-template-per-namespace","title":"RBAC Template per Namespace","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: app-web-prod\n  name: app-admin\nrules:\n- apiGroups: [\"\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"apps\", \"extensions\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"cdi.kubevirt.io\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-admin-binding\n  namespace: app-web-prod\nsubjects:\n- kind: Group\n  name: web-team\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: app-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"architecture/adr/adr-003-cluster-topology/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#positive","title":"Positive","text":"<ul> <li>Clear Ownership: Each namespace has a clear business owner</li> <li>Improved Security: Strong isolation boundaries between applications</li> <li>Simplified Operations: Easier to manage, monitor, and troubleshoot</li> <li>Better Resource Management: Clear resource attribution and quota management</li> <li>Compliance Ready: Audit boundaries align with business applications</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#negative","title":"Negative","text":"<ul> <li>Initial Complexity: Requires careful planning of namespace boundaries</li> <li>Cross-App Dependencies: Need clear policies for inter-namespace communication</li> <li>Shared Services Challenge: Need strategy for common services (monitoring, logging)</li> <li>Learning Curve: Teams need to understand namespace-based organization</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#phase-1-planning-and-design","title":"Phase 1: Planning and Design","text":"<ol> <li>Inventory existing applications and their dependencies</li> <li>Define namespace naming conventions and standards</li> <li>Create RBAC and network policy templates</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/#phase-2-shared-services-migration","title":"Phase 2: Shared Services Migration","text":"<ol> <li>Create shared services namespaces (monitoring, logging, gateway)</li> <li>Migrate common infrastructure components</li> <li>Establish cross-namespace communication patterns</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/#phase-3-application-migration","title":"Phase 3: Application Migration","text":"<ol> <li>Start with least critical applications</li> <li>Create application-specific namespaces with proper quotas and policies</li> <li>Migrate workloads and validate functionality</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/#phase-4-governance-implementation","title":"Phase 4: Governance Implementation","text":"<ol> <li>Implement automated namespace provisioning</li> <li>Enable monitoring and alerting per namespace</li> <li>Create operational runbooks for namespace management</li> </ol>"},{"location":"architecture/adr/adr-003-cluster-topology/#monitoring-and-compliance","title":"Monitoring and Compliance","text":""},{"location":"architecture/adr/adr-003-cluster-topology/#namespace-level-metrics","title":"Namespace-Level Metrics","text":"<ul> <li>Resource utilization per namespace</li> <li>Cost attribution per application</li> <li>Security policy violations</li> <li>Cross-namespace communication patterns</li> </ul>"},{"location":"architecture/adr/adr-003-cluster-topology/#compliance-reporting","title":"Compliance Reporting","text":"<ul> <li>Resource usage reports per business unit</li> <li>Security posture per application</li> <li>Audit logs scoped to business context</li> <li>Data residency compliance per namespace</li> </ul> <p>This topology provides a solid foundation for managing complex multi-tenant RH OVE environments while maintaining security, operational clarity, and business alignment.</p>"},{"location":"architecture/adr/adr-004-admission-controller/","title":"ADR-004: Admission Controller Strategy","text":""},{"location":"architecture/adr/adr-004-admission-controller/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-004-admission-controller/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-004-admission-controller/#context","title":"Context","text":"<p>The RH OVE ecosystem requires a flexible, secure, and policy-driven approach for managing resource admission and validation within clusters. Implementing appropriate admission control policies ensures compliance, security, and operational consistency.</p>"},{"location":"architecture/adr/adr-004-admission-controller/#decision","title":"Decision","text":"<p>We will implement a layered admission control strategy utilizing OpenShift's built-in admission controllers, KubeVirt webhooks, Kyverno policies, and OIDC-integrated RBAC enforcement via Keycloak.</p>"},{"location":"architecture/adr/adr-004-admission-controller/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-004-admission-controller/#advantages","title":"Advantages","text":"<ol> <li>Centralized Policy Management: Simplify governance with cluster-wide policies</li> <li>Dynamic Policy Application: Adjust policies without redeploying cluster components</li> <li>Security Enforcement: Validate resource configurations before persistence</li> <li>Prevention of Misconfiguration: Guard against policy violations</li> <li>Identity-based Access Management: Integration with OIDC providers for enhanced user identity verification</li> <li>Extensibility: Easy to introduce new policies as needs evolve</li> </ol>"},{"location":"architecture/adr/adr-004-admission-controller/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Legacy Admission Controllers: Rejected due to limited flexibility and poor integration</li> <li>Custom Webhooks: Rejected due to complexity in management and maintenance</li> <li>Third-Party Solutions: Rejected due to integration difficulties and vendor lock-in</li> </ol>"},{"location":"architecture/adr/adr-004-admission-controller/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-004-admission-controller/#openshift-built-in-admission","title":"OpenShift Built-in Admission","text":"<ul> <li>Security Context Constraints: Default and custom SCCs for VM and container workloads</li> <li>RBAC Enforcements: Actionable role- and label-based access controls</li> <li>Quotas and Limit Ranges: Ensuring fair resource allocation per team</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#kubevirt-webhooks","title":"KubeVirt Webhooks","text":"<ul> <li>Validation Webhooks: Enforce configuration standards for VM specs</li> <li>Mutation Webhooks: Apply defaults and constraints to VM definitions</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#kyverno-policies","title":"Kyverno Policies","text":"<ul> <li>Configuration Validation: Ensure compliance with organization best practices</li> <li>Resource Constraints: Limit what configurations may be used / deployed</li> <li>Dynamic Policies: Automate policy reapplication based on changes</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>Auditability: Policy applications and violation logging</li> <li>Policy-as-Code: Centralized version control and history of policy changes</li> <li>Enforcement vs Warning: Progressive policy application based on audit</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-004-admission-controller/#positive","title":"Positive","text":"<ul> <li>Improved Security Posture: Clusters protected from non-compliant configurations</li> <li>Enhanced Compliance Auditability: Documentation and reporting of policy compliance</li> <li>Reduced Operational Risk: Guard against human error and unsafe configurations</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#negative","title":"Negative","text":"<ul> <li>Complex Rule Management: Need mature processes to handle policy lifecycle</li> <li>Performance Overhead: May introduce latency to resource creation/update</li> <li>Learning Curve: Required training for policy authors</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#implementation-plan","title":"Implementation Plan","text":""},{"location":"architecture/adr/adr-004-admission-controller/#phase-1-planning","title":"Phase 1: Planning","text":"<ol> <li>Identify key compliance and security requirements</li> <li>Design initial policy set and test environment</li> <li>Engage stakeholders to define policy boundaries</li> </ol>"},{"location":"architecture/adr/adr-004-admission-controller/#phase-2-rollout","title":"Phase 2: Rollout","text":"<ol> <li>Deploy core admission controllers with policy-as-code principles, leveraging IAM for authentication and authorization</li> <li>Begin enforcement in non-production environments</li> <li>Gradually extend to production with monitoring and logging</li> </ol>"},{"location":"architecture/adr/adr-004-admission-controller/#phase-3-monitoring-and-adjustment","title":"Phase 3: Monitoring and Adjustment","text":"<ol> <li>Enable continuous policy evaluation and audit logging</li> <li>Conduct regular policy reviews and updates</li> <li>Train teams on policy creation and maintenance</li> </ol>"},{"location":"architecture/adr/adr-004-admission-controller/#compliance-and-observability","title":"Compliance and Observability","text":""},{"location":"architecture/adr/adr-004-admission-controller/#monitoring","title":"Monitoring","text":"<ul> <li>Policy applicability and compliance dashboards</li> <li>Alerts for policy violations and enforcement actions</li> </ul>"},{"location":"architecture/adr/adr-004-admission-controller/#logging-and-reporting","title":"Logging and Reporting","text":"<ul> <li>Centralized logging of admission requests and results</li> <li>Automated compliance reports tied to policy adherence</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/","title":"ADR-005: Cilium CNI with Multus Multi-Network Strategy","text":""},{"location":"architecture/adr/adr-005-network-cni/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-005-network-cni/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-005-network-cni/#context","title":"Context","text":"<p>The RH OVE ecosystem requires advanced networking capabilities to support both container and VM workloads with enterprise-grade security, performance, and observability. Traditional iptables-based CNI solutions lack the performance and security features needed for modern hybrid workloads.</p>"},{"location":"architecture/adr/adr-005-network-cni/#decision","title":"Decision","text":"<p>We will implement Cilium as the primary CNI with Multus for multi-network support, providing eBPF-powered networking with advanced security and observability capabilities.</p>"},{"location":"architecture/adr/adr-005-network-cni/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-005-network-cni/#advantages-of-cilium","title":"Advantages of Cilium","text":"<ol> <li>eBPF Performance: Superior performance compared to iptables-based solutions</li> <li>Identity-Aware Security: Security policies based on workload identity, not IP addresses</li> <li>L7 Security: Application-layer security without sidecar proxies</li> <li>Service Mesh Capabilities: Built-in service mesh functionality</li> <li>Red Hat Certification: Certified CNI plugin for OpenShift</li> <li>Hubble Observability: Deep network visibility and monitoring</li> <li>Transparent Encryption: Built-in WireGuard and IPsec support</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#advantages-of-multus-integration","title":"Advantages of Multus Integration","text":"<ol> <li>Multi-Network Support: Attach multiple network interfaces to VMs</li> <li>Legacy Network Integration: Support for existing VLAN-based networks</li> <li>Performance Networks: SR-IOV for high-performance workloads</li> <li>Network Segmentation: Separate management, storage, and data networks</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/adr/adr-005-network-cni/#1-ovn-kubernetes-openshift-default","title":"1. OVN-Kubernetes (OpenShift Default)","text":"<ul> <li>Pros: Native OpenShift integration, mature</li> <li>Cons: Limited eBPF features, performance overhead</li> <li>Rejected: Cilium provides superior performance and security</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#2-calico","title":"2. Calico","text":"<ul> <li>Pros: Strong network policies, eBPF support</li> <li>Cons: No built-in service mesh, complex multi-network setup</li> <li>Rejected: Cilium offers better integrated solution</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#3-flannel","title":"3. Flannel","text":"<ul> <li>Pros: Simple, lightweight</li> <li>Cons: Limited security features, no eBPF support</li> <li>Rejected: Insufficient for enterprise requirements</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-005-network-cni/#cilium-configuration","title":"Cilium Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  # Enable Cilium features\n  enable-ipv4: \"true\"\n  enable-ipv6: \"false\"\n\n  # eBPF configuration\n  enable-bpf-masquerade: \"true\"\n  enable-host-reachable-services: \"true\"\n\n  # Security features\n  enable-l7-proxy: \"true\"\n  enable-policy: \"default\"\n  policy-enforcement-mode: \"default\"\n\n  # Service mesh capabilities\n  enable-envoy-config: \"true\"\n\n  # Encryption\n  enable-wireguard: \"true\"\n  wireguard-userspace-fallback: \"true\"\n\n  # Observability\n  enable-hubble: \"true\"\n  hubble-listen-address: \":4244\"\n  hubble-metrics-server: \":9091\"\n  hubble-metrics: |\n    dns:query;ignoreAAAA\n    drop\n    tcp\n    flow\n    icmp\n    http\n\n  # Performance optimizations\n  enable-bandwidth-manager: \"true\"\n  enable-local-redirect-policy: \"true\"\n  kube-proxy-replacement: \"strict\"\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#multus-installation","title":"Multus Installation","text":"<pre><code>apiVersion: operator.openshift.io/v1\nkind: Network\nmetadata:\n  name: cluster\nspec:\n  additionalNetworks:\n  - name: management-network\n    namespace: default\n    type: Raw\n    rawCNIConfig: |\n      {\n        \"cniVersion\": \"0.3.1\",\n        \"name\": \"management-network\",\n        \"type\": \"macvlan\",\n        \"master\": \"ens192\",\n        \"mode\": \"bridge\",\n        \"ipam\": {\n          \"type\": \"static\"\n        }\n      }\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#network-attachment-definitions","title":"Network Attachment Definitions","text":"<pre><code># Management Network\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: management-net\n  namespace: vm-infrastructure\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"management-net\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"static\",\n        \"addresses\": [\n          {\n            \"address\": \"192.168.100.0/24\",\n            \"gateway\": \"192.168.100.1\"\n          }\n        ]\n      }\n    }\n---\n# High-Performance SR-IOV Network\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: sriov-net\n  namespace: vm-production\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"sriov-net\",\n      \"type\": \"sriov\",\n      \"deviceID\": \"1017\",\n      \"vf\": 0,\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#identity-aware-network-policies","title":"Identity-Aware Network Policies","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: web-to-database-policy\n  namespace: app-web-prod\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-frontend\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        app: database\n        environment: production\n    toPorts:\n    - ports:\n      - port: \"5432\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/health\"\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#l7-security-policies","title":"L7 Security Policies","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: api-security-policy\n  namespace: app-api-prod\nspec:\n  endpointSelector:\n    matchLabels:\n      app: api-server\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: web-frontend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"\n        - method: \"POST\"\n          path: \"/api/v1/users\"\n          headers:\n          - \"Content-Type: application/json\"\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#vm-multi-network-configuration","title":"VM Multi-Network Configuration","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: multi-network-vm\n  namespace: vm-infrastructure\n  annotations:\n    k8s.v1.cni.cncf.io/networks: |\n      [\n        {\n          \"name\": \"management-net\",\n          \"ips\": [\"192.168.100.10/24\"]\n        },\n        {\n          \"name\": \"storage-net\",\n          \"ips\": [\"10.0.1.10/24\"]\n        }\n      ]\nspec:\n  running: true\n  template:\n    spec:\n      domain:\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: management\n            bridge: {}\n          - name: storage\n            bridge: {}\n      networks:\n      - name: default\n        pod: {}\n      - name: management\n        multus:\n          networkName: management-net\n      - name: storage\n        multus:\n          networkName: storage-net\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#security-implementation","title":"Security Implementation","text":""},{"location":"architecture/adr/adr-005-network-cni/#transparent-encryption","title":"Transparent Encryption","text":"<pre><code># WireGuard encryption configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-wireguard: \"true\"\n  wireguard-userspace-fallback: \"true\"\n  encryption-node: \"true\"\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#zero-trust-network-policies","title":"Zero Trust Network Policies","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: app-web-prod\nspec:\n  endpointSelector: {}\n  ingress: []\n  egress:\n  # Allow DNS\n  - toEndpoints:\n    - matchLabels:\n        k8s:io.kubernetes.pod.namespace: kube-system\n        k8s:k8s-app: kube-dns\n    toPorts:\n    - ports:\n      - port: \"53\"\n        protocol: UDP\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#observability-with-hubble","title":"Observability with Hubble","text":""},{"location":"architecture/adr/adr-005-network-cni/#hubble-relay-configuration","title":"Hubble Relay Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: hubble-config\n  namespace: kube-system\ndata:\n  config.yaml: |\n    server:\n      address: 0.0.0.0:4245\n    relay:\n      address: hubble-relay.kube-system.svc.cluster.local:80\n    tls:\n      enabled: false\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#network-flow-monitoring","title":"Network Flow Monitoring","text":"<pre><code># Monitor network flows\nhubble observe --namespace app-web-prod\n\n# Check policy violations\nhubble observe --verdict DENIED\n\n# Monitor specific VM traffic\nhubble observe --pod vm-database-xxx\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/adr/adr-005-network-cni/#ebpf-host-routing","title":"eBPF Host Routing","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-host-routing: \"true\"\n  enable-external-ips: \"true\"\n  enable-node-port: \"true\"\n  enable-host-port: \"true\"\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#bandwidth-management","title":"Bandwidth Management","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumBandwidthPolicy\nmetadata:\n  name: bandwidth-limit\n  namespace: app-web-prod\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-frontend\n  egress:\n  - bandwidth: \"100M\"\n  - bandwidth: \"1G\"\n    dscp: 46  # High priority traffic\n</code></pre>"},{"location":"architecture/adr/adr-005-network-cni/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-005-network-cni/#positive","title":"Positive","text":"<ul> <li>Superior Performance: eBPF provides 10-100x better performance than iptables</li> <li>Enhanced Security: Identity-aware policies and L7 security without sidecars</li> <li>Deep Observability: Hubble provides comprehensive network visibility</li> <li>Future-Proof: eBPF is the future of Linux networking</li> <li>Multi-Network Support: Seamless integration with legacy and high-performance networks</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#negative","title":"Negative","text":"<ul> <li>Learning Curve: Teams need to learn eBPF concepts and Cilium specifics</li> <li>Debugging Complexity: eBPF programs can be harder to debug than traditional networking</li> <li>Resource Requirements: Higher memory usage compared to simpler CNI solutions</li> <li>Compatibility Concerns: Some legacy applications may need network policy adjustments</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/adr/adr-005-network-cni/#phase-1-preparation","title":"Phase 1: Preparation","text":"<ol> <li>Audit existing network policies and requirements</li> <li>Set up test clusters with Cilium/Multus</li> <li>Train operations team on eBPF and Cilium concepts</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#phase-2-non-production-deployment","title":"Phase 2: Non-Production Deployment","text":"<ol> <li>Deploy Cilium in development and staging clusters</li> <li>Migrate network policies to Cilium format</li> <li>Implement Hubble monitoring and alerting</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#phase-3-production-migration","title":"Phase 3: Production Migration","text":"<ol> <li>Schedule maintenance window for CNI migration</li> <li>Deploy Cilium with careful monitoring</li> <li>Gradually enable advanced features (encryption, L7 policies)</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":"<ol> <li>Enable service mesh capabilities</li> <li>Implement advanced security policies</li> <li>Optimize performance settings based on workload patterns</li> </ol>"},{"location":"architecture/adr/adr-005-network-cni/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"architecture/adr/adr-005-network-cni/#key-metrics","title":"Key Metrics","text":"<ul> <li>Network throughput per namespace/pod</li> <li>Policy enforcement latency</li> <li>eBPF program load and execution time</li> <li>Hubble flow processing rate</li> <li>Encryption overhead metrics</li> </ul>"},{"location":"architecture/adr/adr-005-network-cni/#alerting-rules","title":"Alerting Rules","text":"<pre><code>groups:\n- name: cilium-alerts\n  rules:\n  - alert: CiliumAgentDown\n    expr: up{job=\"cilium-agent\"} == 0\n    for: 5m\n    labels:\n      severity: critical\n  - alert: NetworkPolicyViolation\n    expr: increase(cilium_policy_verdicts_total{verdict=\"DENIED\"}[5m]) &gt; 10\n    labels:\n      severity: warning\n</code></pre> <p>This network architecture provides enterprise-grade performance, security, and observability for the RH OVE ecosystem while supporting both modern cloud-native applications and traditional VM workloads.</p>"},{"location":"architecture/adr/adr-006-backup-strategy/","title":"ADR-006: Backup Strategy for RH OVE Ecosystem","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-006-backup-strategy/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-006-backup-strategy/#context","title":"Context","text":"<p>Ensuring data protection and recovery is crucial for the RH OVE multi-cluster environment. The solution must support frequent, secure, and efficient backups across clusters, aligning with business continuity and compliance requirements.</p>"},{"location":"architecture/adr/adr-006-backup-strategy/#decision","title":"Decision","text":"<p>Adopt a centralized backup strategy using Rubrik for VM and containerized workloads, providing consistency, compliance, and ease of management.</p>"},{"location":"architecture/adr/adr-006-backup-strategy/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#advantages","title":"Advantages","text":"<ol> <li>Unified Management: Single pane of glass for managing backups across clusters</li> <li>Policy-Driven: Flexibility in configuring backup policies per application/business need</li> <li>Deduplication and Compression: Reduce storage costs by minimizing redundant data</li> <li>Cloud Integration: Support for hybrid cloud scenarios and long-term data retention</li> <li>Application Consistency: Automated application-aware snapshot management</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Velero: Open-source alternative</li> <li>Pros: Strong integration with Kubernetes ecosystems</li> <li>Cons: Complexity in VM integration and limited cloud support</li> <li> <p>Rejected: Difficulties in ensuring VM workload consistency and enterprise support</p> </li> <li> <p>DIY Scripting Solutions: Custom in-house scripts</p> </li> <li>Pros: Potentially customizable</li> <li>Cons: Highly error-prone, difficult to manage at scale</li> <li>Rejected: Lack of enterprise features, consistency guarantees, and support</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#backup-policy-design","title":"Backup Policy Design","text":"<ul> <li>Daily Backups: RPO of 24 hours for critical workloads</li> <li>Weekly Full Backup with Daily Incrementals: Optimizes storage usage</li> <li>Data Encryption: Both in transit and at rest using AES-256</li> </ul>"},{"location":"architecture/adr/adr-006-backup-strategy/#backup-architecture","title":"Backup Architecture","text":"<pre><code>Backup Architecture\n\u251c\u2500\u2500 Management Cluster\n\u2502   \u251c\u2500\u2500 Rubrik Management Node\n\u2502   \u2514\u2500\u2500 Centralized Backup Policy Management\n\u251c\u2500\u2500 Application Clusters\n\u2502   \u251c\u2500\u2500 Rubrik Edge Devices\n\u2502   \u251c\u2500\u2500 Local Snapshot Agents\n\u2502   \u2514\u2500\u2500 Data Replica Agents\n\u2514\u2500\u2500 Cloud Archive\n    \u251c\u2500\u2500 Long Term Retention Storage\n    \u2514\u2500\u2500 Cross-Region DR Copies\n</code></pre>"},{"location":"architecture/adr/adr-006-backup-strategy/#configuration-example","title":"Configuration Example","text":"<pre><code>apiVersion: backup.rubrik.com/v1alpha1\nkind: BackupPolicy\nmetadata:\n  name: application-backup-policy\n  namespace: backup\nspec:\n  frequency: \"24h\"\n  retention:\n    local: \"30d\"\n    cloud: \"365d\"\n  snapshotConsistency: \"crash-consistent\"\n  includeVolumes: \"all\"\n  excludeVolumes:\n  - \"scratch\"\n  encryption: enabled\n  replication:\n    target: cloud-archive\n    frequency: \"12h\"\n</code></pre>"},{"location":"architecture/adr/adr-006-backup-strategy/#security-and-compliance-considerations","title":"Security and Compliance Considerations","text":"<ul> <li>Data Encryption: All backup data encrypted with AES-256</li> <li>Access Control: Role-based access for backup management and retrieval</li> <li>Audit Trails: Detailed logging of backup and restore operations</li> <li>Compliance Alignment: Meets GDPR, HIPAA, and SOC 2 requirements</li> </ul>"},{"location":"architecture/adr/adr-006-backup-strategy/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#positive","title":"Positive","text":"<ul> <li>Reduced Risk: Comprehensive DR strategy minimizes impact of data loss events</li> <li>Operational Visibility: Centralized monitoring and alerting of backup status</li> <li>Strategic Flexibility: Support for hybrid cloud and multi-region deployments</li> </ul>"},{"location":"architecture/adr/adr-006-backup-strategy/#negative","title":"Negative","text":"<ul> <li>Cost Considerations: Could incur higher upfront Costa than open-source alternatives</li> <li>Training Requirement: Backup administrators require training in Rubrik solutions</li> </ul>"},{"location":"architecture/adr/adr-006-backup-strategy/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#phase-1-planning","title":"Phase 1: Planning","text":"<ol> <li>Define business-critical systems and RPO/RTO requirements</li> <li>Design initial backup policy and architecture</li> <li>Identify data sovereignty and compliance requirements</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#phase-2-non-production-deployment","title":"Phase 2: Non-Production Deployment","text":"<ol> <li>Pilot Rubrik deployment in development environment</li> <li>Test backup and restore operations thoroughly</li> <li>Validate compliance alignment with internal and external audits</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#phase-3-production-rollout","title":"Phase 3: Production Rollout","text":"<ol> <li>Deploy Rubrik management in the production environment</li> <li>Migrate to production backup policies with minimal downtime</li> <li>Enable monitoring and alerting on backup status</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#phase-4-continuous-improvement","title":"Phase 4: Continuous Improvement","text":"<ol> <li>Regular policy reviews to adapt to changing business needs</li> <li>Leverage Rubrik analytics for optimizations and reporting</li> <li>Update DR plans based on lessons learned and testing</li> </ol>"},{"location":"architecture/adr/adr-006-backup-strategy/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"architecture/adr/adr-006-backup-strategy/#key-monitoring-metrics","title":"Key Monitoring Metrics","text":"<ul> <li>Backup success/failure rates</li> <li>Storage consumption over time</li> <li>Deduplication and compression efficiency</li> <li>RPO and RTO performance</li> </ul>"},{"location":"architecture/adr/adr-006-backup-strategy/#alerting-setup","title":"Alerting Setup","text":"<pre><code>groups:\n- name: backup-alerts\n  rules:\n  - alert: BackupFailure\n    expr: rubrik_backup_failed{job=\"rubrik-agent\"} \u007f 0\n    for: 10m\n    labels:\n      severity: critical\n  - alert: RPOViolation\n    expr: rubrik_backup_rpo{target=\"24h\"} \u007f=r 24 * 60 * 60\n    labels:\n      severity: warning\n</code></pre> <p>This comprehensive backup strategy ensures that RH OVE can achieve high data availability, integrity, and compliance, aligning with enterprise best practices for data protection and disaster recovery.</p>"},{"location":"architecture/adr/adr-007-monitoring-strategy/","title":"ADR-007: Monitoring Strategy for RH OVE Ecosystem","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#context","title":"Context","text":"<p>For the RH OVE multi-cluster setup, a comprehensive monitoring solution is necessary to ensure operational visibility, performance management, and incident response capability for both containerized and VM-based workloads.</p>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#decision","title":"Decision","text":"<p>Implement an integrated monitoring solution using Prometheus and Grafana for metrics collection and visualization, enhanced by Dynatrace for application performance monitoring and Hubble for network observability.</p>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#prometheus-grafana","title":"Prometheus &amp; Grafana","text":"<ol> <li>Scalability: Native Kubernetes support, able to scale for large environments</li> <li>Flexibility: Customizable dashboards and extensibility with plugins</li> <li>Community Support: Active ecosystem with numerous exporters and integrations</li> <li>Real-time Metrics: Capable of handling thousands of unique time-series metrics</li> <li>Alerting: Integrated alert management with Prometheus Alertmanager</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#dynatrace","title":"Dynatrace","text":"<ol> <li>Full-Stack Monitoring: Covers both infrastructure and application layers</li> <li>AI-Powered Analytics: Automated anomaly detection and root cause analysis</li> <li>Cloud-Native Support: Strong support for Kubernetes and container environments</li> <li>Unified Observability: Centralized insights across microservices and legacy apps</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#hubble","title":"Hubble","text":"<ol> <li>eBPF-powered Network Insights: Detailed flow visibility and security audits</li> <li>High Throughput: Capable of capturing thousands of network flows per second</li> <li>Deployment Simplicity: Out-of-the-box integration with Cilium</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>OpenShift Monitoring Stack</li> <li>Pros: Native solution, well-integrated</li> <li>Cons: Lacks depth in application performance monitoring</li> <li> <p>Rejected: Chosen instead for basic cluster health visibility</p> </li> <li> <p>Elastic Stack</p> </li> <li>Pros: Full-text search capabilities</li> <li>Cons: Complexity and resource consumption</li> <li> <p>Rejected: Simplified requirements focused on metrics</p> </li> <li> <p>DataDog</p> </li> <li>Pros: Comprehensive feature set, SaaS model</li> <li>Cons: Cost concerns for large-scale deployment</li> <li>Rejected: Cost prohibitive compared to chosen solutions</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#prometheus-configuration","title":"Prometheus Configuration","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: global-prometheus\n  namespace: monitoring\nspec:\n  replicas: 3\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchLabels:\n      team: observability\n  storage:\n    volumeClaimTemplate:\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 500Gi\n</code></pre>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#grafana-setup","title":"Grafana Setup","text":"<ul> <li>Dashboards: Pre-configured dashboards for cluster health, application performance, VM metrics</li> <li>Themes: Custom theming for alignment with corporate branding</li> <li>User Access Control: Integrated with OAuth for SSO</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#dynatrace-integration","title":"Dynatrace Integration","text":"<ul> <li>Deployment of OneAgent across clusters for full-stack visibility</li> <li>Integration with CI/CD pipelines for real-time performance feedback</li> <li>Automated tagging for dynamic cloud workloads</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#hubble-configuration","title":"Hubble Configuration","text":"<ul> <li>Enable flow aggregation and analysis for detailed network observability</li> <li>Real-time flow filtering and visualization of network policies</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#security-and-compliance-considerations","title":"Security and Compliance Considerations","text":"<ul> <li>Data Encryption: All telemetry data encrypted in transit</li> <li>Role-Based Access Control: Segmented access to monitoring data</li> <li>Compliance Monitoring: Automated checks for regulatory compliance</li> <li>Audit Logging: Capture all configuration and access attempts</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#positive","title":"Positive","text":"<ul> <li>Operational Efficiency: Reduce MTTR with real-time insights and alerting</li> <li>Proactive Performance Management: Identify and resolve issues before impacting users</li> <li>Unified Observability: Single-pane monitoring across clusters and applications</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#negative","title":"Negative","text":"<ul> <li>Complexity of Integration: Requires coordination across multiple tools</li> <li>Resource Overhead: Higher costs in terms of storage and compute resources</li> <li>Training Requirements: Teams need to become familiar with monitoring tools</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#phase-1-initial-setup-and-configuration","title":"Phase 1: Initial Setup and Configuration","text":"<ol> <li>Deploy base Prometheus and Grafana setup in the management cluster</li> <li>Establish Dynatrace integration for application monitoring</li> <li>Enable Hubble for network flow visibility</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#phase-2-metrics-and-dashboard-customization","title":"Phase 2: Metrics and Dashboard Customization","text":"<ol> <li>Design and implement custom dashboards for key performance indicators</li> <li>Configure alerting thresholds and incident response playbooks</li> <li>Integrate monitoring data with existing ITSM tools</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#phase-3-continuous-optimization","title":"Phase 3: Continuous Optimization","text":"<ol> <li>Conduct regular review of metrics and dashboards for continuous improvement</li> <li>Leverage Dynatrace AI insights for proactive tuning and capacity planning</li> <li>Regularly assess network flow policies for efficiency and security</li> </ol>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"architecture/adr/adr-007-monitoring-strategy/#key-performance-indicators","title":"Key Performance Indicators","text":"<ul> <li>CPU, memory, and storage utilization</li> <li>Network latency and throughput</li> <li>Application response times and error rates</li> <li>VM and container health</li> </ul>"},{"location":"architecture/adr/adr-007-monitoring-strategy/#alerting-rules","title":"Alerting Rules","text":"<ul> <li>Resource exhaustion (CPU, Memory, Storage)</li> <li>Network policy violations</li> <li>Anomalous application behavior</li> </ul> <p>This robust monitoring strategy ensures RH OVE achieves operational excellence, rapid issue resolution, and strategic insight into both infrastructure performance and applications across the multi-cluster environment.</p>"},{"location":"architecture/adr/adr-008-iam-strategy/","title":"ADR-008: Identity and Access Management (IAM) Strategy","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/adr/adr-008-iam-strategy/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"architecture/adr/adr-008-iam-strategy/#context","title":"Context","text":"<p>The RH OVE multi-cluster ecosystem requires enterprise-grade identity and access management to ensure secure authentication and authorization across all clusters and services. Traditional cluster-specific authentication creates operational complexity and security risks in multi-cluster environments.</p>"},{"location":"architecture/adr/adr-008-iam-strategy/#decision","title":"Decision","text":"<p>We will implement a comprehensive IAM strategy using OpenID Connect (OIDC) providers with Keycloak as the primary identity provider, integrated with Kubernetes-native RBAC and service mesh authentication.</p>"},{"location":"architecture/adr/adr-008-iam-strategy/#rationale","title":"Rationale","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#advantages","title":"Advantages","text":"<ol> <li>Centralized Identity Management: Single source of truth for user identities across all clusters</li> <li>Single Sign-On (SSO): Seamless authentication across all services and clusters</li> <li>Enterprise Integration: Native integration with existing LDAP/Active Directory infrastructure</li> <li>Multi-Factor Authentication: Enhanced security with mandatory MFA for administrative accounts</li> <li>Audit and Compliance: Complete audit trail for SOC 2, GDPR, and HIPAA compliance</li> <li>Zero Trust Security: Identity-aware access controls with least-privilege principles</li> <li>Scalability: Supports horizontal scaling across multiple clusters and regions</li> </ol>"},{"location":"architecture/adr/adr-008-iam-strategy/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#1-basic-kubernetes-serviceaccount-authentication","title":"1. Basic Kubernetes ServiceAccount Authentication","text":"<ul> <li>Pros: Simple, native Kubernetes integration</li> <li>Cons: No centralized management, limited audit capabilities, poor user experience</li> <li>Rejected: Insufficient for enterprise requirements</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#2-ldap-direct-integration","title":"2. LDAP Direct Integration","text":"<ul> <li>Pros: Direct integration with existing directory services</li> <li>Cons: No OIDC compliance, limited multi-cluster support, poor web service integration</li> <li>Rejected: Limited modern authentication capabilities</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#3-commercial-solutions-auth0-okta","title":"3. Commercial Solutions (Auth0, Okta)","text":"<ul> <li>Pros: Feature-rich, managed service</li> <li>Cons: Vendor lock-in, higher costs, limited customization</li> <li>Alternative: Considered for specific use cases but Keycloak preferred for primary solution</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#core-components","title":"Core Components","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#identity-provider-keycloak-red-hat-sso","title":"Identity Provider: Keycloak (Red Hat SSO)","text":"<pre><code># Keycloak Deployment Configuration\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keycloak\n  namespace: auth-system\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: keycloak\n  template:\n    spec:\n      containers:\n      - name: keycloak\n        image: quay.io/keycloak/keycloak:20.0\n        env:\n        - name: KC_DB\n          value: postgres\n        - name: KC_DB_URL\n          value: jdbc:postgresql://postgres:5432/keycloak\n        - name: KC_HOSTNAME_STRICT\n          value: \"false\"\n        - name: KC_HTTP_ENABLED\n          value: \"true\"\n        - name: KC_PROXY\n          value: edge\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#oidc-integration-with-openshift","title":"OIDC Integration with OpenShift","text":"<pre><code>apiVersion: config.openshift.io/v1\nkind: OAuth\nmetadata:\n  name: cluster\nspec:\n  identityProviders:\n  - name: keycloak-oidc\n    mappingMethod: claim\n    type: OpenID\n    openID:\n      clientID: openshift-cluster\n      clientSecret:\n        name: oidc-client-secret\n      issuer: https://keycloak.company.com/auth/realms/openshift\n      claims:\n        preferredUsername: [preferred_username]\n        name: [name]\n        email: [email]\n        groups: [groups]\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#dex-oidc-proxy-for-service-authentication","title":"Dex OIDC Proxy for Service Authentication","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dex-config\n  namespace: auth-system\ndata:\n  config.yaml: |\n    issuer: https://dex.company.com\n    storage:\n      type: kubernetes\n      config:\n        inCluster: true\n    connectors:\n    - type: oidc\n      id: keycloak\n      name: Keycloak\n      config:\n        issuer: https://keycloak.company.com/auth/realms/company\n        clientID: dex-client\n        clientSecret: $DEX_CLIENT_SECRET\n        scopes: [openid, profile, email, groups]\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#authentication-flow","title":"Authentication Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Browser\n    participant Dex\n    participant Keycloak\n    participant K8s API\n    participant ArgoCD\n\n    User-&gt;&gt;Browser: Access ArgoCD\n    Browser-&gt;&gt;ArgoCD: Request access\n    ArgoCD-&gt;&gt;Dex: Redirect to OIDC\n    Dex-&gt;&gt;Keycloak: Authentication request\n    Keycloak-&gt;&gt;User: MFA challenge\n    User-&gt;&gt;Keycloak: Credentials + MFA\n    Keycloak-&gt;&gt;Dex: ID Token + Access Token\n    Dex-&gt;&gt;ArgoCD: Authorization code\n    ArgoCD-&gt;&gt;Dex: Exchange for tokens\n    ArgoCD-&gt;&gt;K8s API: API calls with user context\n    K8s API-&gt;&gt;ArgoCD: Authorized response\n    ArgoCD-&gt;&gt;Browser: Application access granted</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#authorization-model","title":"Authorization Model","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#group-based-rbac","title":"Group-Based RBAC","text":"<pre><code># Keycloak Groups mapped to Kubernetes RBAC\ngroups:\n  platform-admins:\n    kubernetes-role: cluster-admin\n    argocd-role: admin\n  web-developers:\n    kubernetes-role: developer\n    kubernetes-namespaces: [app-web-prod, app-web-staging, app-web-dev]\n    argocd-role: web-app-admin\n  database-admins:\n    kubernetes-role: developer\n    kubernetes-namespaces: [app-database-prod, app-database-staging]\n    argocd-role: database-admin\n  security-auditors:\n    kubernetes-role: view\n    argocd-role: readonly\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#kubernetes-rbac-integration","title":"Kubernetes RBAC Integration","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: platform-admin-binding\nsubjects:\n- kind: Group\n  name: \"platform-admins\"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#security-controls","title":"Security Controls","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#token-management","title":"Token Management","text":"<ul> <li>Access Tokens: 15-minute expiration</li> <li>Refresh Tokens: 1-hour expiration</li> <li>ID Tokens: 5-minute expiration</li> <li>Service Account Tokens: 2-hour expiration with projected volumes</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#multi-factor-authentication","title":"Multi-Factor Authentication","text":"<pre><code># Mandatory MFA Flow in Keycloak\nauthenticationFlows:\n  - alias: \"browser-with-mfa\"\n    description: \"Browser flow with mandatory MFA\"\n    authenticationExecutions:\n      - authenticator: \"auth-username-password-form\"\n        requirement: \"REQUIRED\"\n      - authenticator: \"auth-otp-form\"\n        requirement: \"REQUIRED\"\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#network-security","title":"Network Security","text":"<ul> <li>Network policies restricting authentication service communication</li> <li>TLS 1.3 encryption for all authentication traffic</li> <li>Certificate-based mutual TLS for service-to-service communication</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#user-lifecycle-management","title":"User Lifecycle Management","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#automated-provisioning","title":"Automated Provisioning","text":"<ul> <li>SCIM integration for user provisioning/deprovisioning</li> <li>Automated group assignment based on organizational roles</li> <li>ServiceAccount creation for automated systems</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#self-service-capabilities","title":"Self-Service Capabilities","text":"<ul> <li>Password reset and account recovery</li> <li>MFA device management</li> <li>Access request workflows</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#monitoring-and-audit","title":"Monitoring and Audit","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<pre><code># Prometheus ServiceMonitor for Authentication Metrics\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: dex-metrics\n  namespace: auth-system\nspec:\n  selector:\n    matchLabels:\n      app: dex\n  endpoints:\n  - port: metrics\n    interval: 30s\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#audit-logging","title":"Audit Logging","text":"<ul> <li>Complete authentication audit trail</li> <li>RBAC change tracking</li> <li>Failed authentication monitoring</li> <li>Compliance reporting automation</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#key-metrics","title":"Key Metrics","text":"<ul> <li>Authentication success/failure rates</li> <li>Active user sessions</li> <li>Token expiration and renewal rates</li> <li>MFA adoption rates</li> <li>Policy violation incidents</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#high-availability","title":"High Availability","text":"<ul> <li>Multi-replica Keycloak deployment with database clustering</li> <li>Cross-region identity provider replication</li> <li>Automated failover with health checks</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#backup-strategy","title":"Backup Strategy","text":"<pre><code># Daily Keycloak Database Backup\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: keycloak-backup\n  namespace: auth-system\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: postgres:13\n            command: [\"/bin/bash\", \"-c\"]\n            args:\n            - pg_dump -h keycloak-db -U keycloak keycloak &gt; /backup/keycloak-$(date +%Y%m%d).sql &amp;&amp; aws s3 cp /backup/keycloak-$(date +%Y%m%d).sql s3://iam-backups/\n</code></pre>"},{"location":"architecture/adr/adr-008-iam-strategy/#consequences","title":"Consequences","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#positive","title":"Positive","text":"<ul> <li>Enhanced Security: Zero trust identity-aware access controls</li> <li>Operational Efficiency: Centralized identity management reduces operational overhead</li> <li>Compliance Ready: Built-in audit trails and regulatory framework alignment</li> <li>Developer Experience: Single sign-on with self-service capabilities</li> <li>Scalability: Supports multi-cluster and multi-region deployments</li> <li>Cost Optimization: Reduced manual identity management tasks</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#negative","title":"Negative","text":"<ul> <li>Initial Complexity: Setup and configuration require specialized knowledge</li> <li>Dependencies: Additional infrastructure components to manage and monitor</li> <li>Learning Curve: Teams need training on OIDC and Keycloak administration</li> <li>Single Point of Failure: Identity provider availability critical for system access</li> <li>Token Management: Requires careful handling of token lifecycle and security</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#phase-1-infrastructure-setup-2-3-weeks","title":"Phase 1: Infrastructure Setup (2-3 weeks)","text":"<ol> <li>Deploy Keycloak in management cluster with HA configuration</li> <li>Configure LDAP/AD integration and user federation</li> <li>Set up Dex OIDC proxy in all clusters</li> <li>Implement network policies and security controls</li> </ol>"},{"location":"architecture/adr/adr-008-iam-strategy/#phase-2-service-integration-3-4-weeks","title":"Phase 2: Service Integration (3-4 weeks)","text":"<ol> <li>Configure OpenShift OAuth with OIDC</li> <li>Integrate ArgoCD with OIDC authentication</li> <li>Configure Grafana and Prometheus with OIDC</li> <li>Set up RBAC mappings and group assignments</li> </ol>"},{"location":"architecture/adr/adr-008-iam-strategy/#phase-3-user-migration-2-3-weeks","title":"Phase 3: User Migration (2-3 weeks)","text":"<ol> <li>Migrate existing users to new identity system</li> <li>Configure MFA for all administrative accounts</li> <li>Test authentication flows and access controls</li> <li>Train users on new authentication experience</li> </ol>"},{"location":"architecture/adr/adr-008-iam-strategy/#phase-4-monitoring-and-optimization-1-2-weeks","title":"Phase 4: Monitoring and Optimization (1-2 weeks)","text":"<ol> <li>Deploy monitoring and alerting for authentication services</li> <li>Configure audit logging and compliance reporting</li> <li>Optimize token lifecycle and security policies</li> <li>Document operational procedures and runbooks</li> </ol>"},{"location":"architecture/adr/adr-008-iam-strategy/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"architecture/adr/adr-008-iam-strategy/#regulatory-alignment","title":"Regulatory Alignment","text":"<ul> <li>SOC 2 Type II: Automated audit trails and access controls</li> <li>GDPR: Right to be forgotten and data minimization</li> <li>HIPAA: PHI access controls and audit logging</li> <li>SOX: Financial system access controls and separation of duties</li> </ul>"},{"location":"architecture/adr/adr-008-iam-strategy/#audit-requirements","title":"Audit Requirements","text":"<ul> <li>All authentication events logged with timestamps</li> <li>RBAC changes tracked with user attribution</li> <li>Failed authentication attempts monitored and alerted</li> <li>Regular access reviews automated with reporting</li> </ul> <p>This IAM strategy provides enterprise-grade identity and access management for the RH OVE multi-cluster ecosystem, ensuring security, compliance, and operational efficiency through modern OIDC-based authentication and Kubernetes-native authorization.</p>"},{"location":"architecture/adr/adr-table/","title":"Architecture Decision Records (ADR) Table","text":"<p>This document provides a comprehensive overview of all architectural decisions made for the RH OVE multi-cluster ecosystem.</p>"},{"location":"architecture/adr/adr-table/#adr-summary-table","title":"ADR Summary Table","text":"ADR Title Status Date Context Decision ADR-001 Multi-Cluster Architecture Pattern Accepted 2024-12-01 Need to support multiple environments with centralized governance and scalable infrastructure Implement multi-cluster pattern with one management cluster and multiple application clusters ADR-002 GitOps with ArgoCD Hub Architecture Accepted 2024-12-01 Require consistent, auditable, scalable deployment approach across multiple clusters Implement GitOps using ArgoCD in hub-spoke pattern with Git-based configuration ADR-003 Namespace-Based Cluster Topology Accepted 2024-12-01 Need efficient organizational strategy for mixed VM and container workloads with isolation and security Implement application namespace-based topology organized by business application ADR-004 Admission Controller Strategy Accepted 2024-12-01 Require flexible, secure, policy-driven approach for resource admission and validation Implement layered admission control using OpenShift built-in controllers, KubeVirt webhooks, and Kyverno policies ADR-005 Cilium CNI with Multus Multi-Network Strategy Accepted 2024-12-01 Need advanced networking for container and VM workloads with enterprise-grade security and performance Implement Cilium as primary CNI with Multus for multi-network support using eBPF-powered networking ADR-006 Backup Strategy for RH OVE Ecosystem Accepted 2024-12-01 Ensure data protection and recovery across multi-cluster environment with business continuity requirements Adopt centralized backup strategy using Rubrik for VM and containerized workloads ADR-007 Monitoring Strategy for RH OVE Ecosystem Accepted 2024-12-01 Need comprehensive monitoring for operational visibility, performance management, and incident response Implement integrated monitoring using Prometheus/Grafana, Dynatrace, and Hubble ADR-008 Identity and Access Management (IAM) Strategy Accepted 2024-12-01 Need enterprise-grade identity and access management across multi-cluster ecosystem Implement comprehensive IAM using OIDC providers with Keycloak, integrated with Kubernetes RBAC"},{"location":"architecture/adr/adr-table/#detailed-adr-information","title":"Detailed ADR Information","text":""},{"location":"architecture/adr/adr-table/#adr-001-multi-cluster-architecture-pattern","title":"ADR-001: Multi-Cluster Architecture Pattern","text":"<p>Key Components: - Management Cluster: RHACM, ArgoCD Hub, RHACS, Federated Prometheus, Centralized logging, Rubrik backup management - Application Clusters: Production (HA, performance-optimized), Staging (production-like), Development (resource-optimized) - Network Architecture: Dedicated segments per cluster, VPN/private connectivity, zero-trust principles</p> <p>Benefits: Separation of concerns, horizontal scalability, security isolation, operational efficiency, fault isolation, resource optimization</p> <p>Trade-offs: Increased network complexity, additional operational overhead, potential data sync challenges</p>"},{"location":"architecture/adr/adr-table/#adr-002-gitops-with-argocd-hub-architecture","title":"ADR-002: GitOps with ArgoCD Hub Architecture","text":"<p>Key Components: - ArgoCD Hub: Centralized instance in management cluster with HA (3 replicas) - ArgoCD Agents: Lightweight agents in application clusters - Repository Structure: Clusters, applications (base/overlays), infrastructure (networking/storage/monitoring) - Application of Applications Pattern: Root ArgoCD app manages cluster-specific applications</p> <p>Benefits: Declarative configuration, complete audit trail, pull-based security, consistency, easy rollbacks, self-healing</p> <p>Trade-offs: Learning curve for GitOps workflows, Git repository complexity, network dependencies, secret management complexity</p>"},{"location":"architecture/adr/adr-table/#adr-003-namespace-based-cluster-topology","title":"ADR-003: Namespace-Based Cluster Topology","text":"<p>Key Components: - Naming Convention: <code>{app-name}-{environment}</code> (e.g., app-web-prod, app-database-staging) - Standard Templates: Namespace with labels, ResourceQuota, LimitRange, NetworkPolicies - Cross-Namespace Communication: Controlled via NetworkPolicies with explicit allow rules - RBAC Integration: Namespace-level roles aligned with application teams</p> <p>Benefits: Strong isolation, simplified RBAC, clear resource attribution, network microsegmentation, operational clarity, compliance alignment</p> <p>Trade-offs: Initial complexity in planning boundaries, cross-app dependency management, shared services challenges</p>"},{"location":"architecture/adr/adr-table/#adr-004-admission-controller-strategy","title":"ADR-004: Admission Controller Strategy","text":"<p>Key Components: - OpenShift Built-in: Security Context Constraints, RBAC enforcement, quotas and limits - KubeVirt Webhooks: Validation and mutation webhooks for VM specifications - Kyverno Policies: Configuration validation, resource constraints, dynamic policy application</p> <p>Benefits: Centralized policy management, dynamic policy application, security enforcement, misconfiguration prevention, extensibility</p> <p>Trade-offs: Complex rule management, performance overhead, learning curve for policy authors</p>"},{"location":"architecture/adr/adr-table/#adr-005-cilium-cni-with-multus-multi-network-strategy","title":"ADR-005: Cilium CNI with Multus Multi-Network Strategy","text":"<p>Key Components: - Cilium Features: eBPF performance, identity-aware security, L7 security, service mesh capabilities, WireGuard encryption - Multus Integration: Multi-network support, legacy network integration, SR-IOV for high performance, network segmentation - Hubble Observability: Network flow monitoring, policy violation detection, security auditing - NetworkAttachmentDefinitions: Management, storage, and data networks with VLAN support</p> <p>Benefits: Superior eBPF performance (10-100x better than iptables), identity-aware policies, L7 security without sidecars, deep observability, multi-network support</p> <p>Trade-offs: Learning curve for eBPF concepts, debugging complexity, higher memory usage, potential compatibility issues</p>"},{"location":"architecture/adr/adr-table/#adr-006-backup-strategy-for-rh-ove-ecosystem","title":"ADR-006: Backup Strategy for RH OVE Ecosystem","text":"<p>Key Components: - Rubrik Management: Centralized in management cluster with unified policy management - Backup Architecture: Management cluster (Rubrik node), Application clusters (Edge devices, agents), Cloud archive (long-term retention) - Policy Configuration: Daily backups (24h RPO), weekly full with daily incrementals, AES-256 encryption, cloud replication - Compliance: GDPR, HIPAA, SOC 2 alignment with audit trails and access control</p> <p>Benefits: Unified management, policy-driven, deduplication/compression, cloud integration, application consistency</p> <p>Trade-offs: Higher upfront costs than open-source alternatives, training requirements for administrators</p>"},{"location":"architecture/adr/adr-table/#adr-007-monitoring-strategy-for-rh-ove-ecosystem","title":"ADR-007: Monitoring Strategy for RH OVE Ecosystem","text":"<p>Key Components: - Prometheus/Grafana: Scalable metrics collection, customizable dashboards, real-time metrics, integrated alerting - Dynatrace: Full-stack monitoring, AI-powered analytics, cloud-native support, unified observability - Hubble: eBPF-powered network insights, high throughput flow capture, Cilium integration - Integration: Federated Prometheus (3 replicas, 500Gi storage), OAuth SSO, automated tagging</p> <p>Benefits: Operational efficiency with reduced MTTR, proactive performance management, unified observability across clusters</p> <p>Trade-offs: Integration complexity, resource overhead, training requirements for multiple tools</p>"},{"location":"architecture/adr/adr-table/#adr-008-identity-and-access-management-iam-strategy","title":"ADR-008: Identity and Access Management (IAM) Strategy","text":"<p>Key Components: - Keycloak (Red Hat SSO): Primary OIDC provider with LDAP/AD integration, MFA support, group-based access control - Dex OIDC Proxy: Service authentication across clusters with static client configuration - OpenShift OAuth Integration: Native cluster authentication with OIDC claims mapping - RBAC Integration: Kubernetes-native authorization with group-based role assignments - Service Account Management: Time-limited tokens with projected volumes and automated lifecycle</p> <p>Benefits: Centralized identity management, single sign-on across all services, enterprise LDAP/AD integration, MFA enforcement, zero trust security, complete audit trails, compliance ready (SOC 2, GDPR, HIPAA)</p> <p>Trade-offs: Initial setup complexity, additional infrastructure dependencies, OIDC/Keycloak learning curve, identity provider availability critical, token lifecycle management complexity</p>"},{"location":"architecture/adr/adr-table/#cross-adr-dependencies","title":"Cross-ADR Dependencies","text":"<pre><code>graph TD\n    ADR001[ADR-001: Multi-Cluster] --&gt; ADR002[ADR-002: GitOps ArgoCD]\n    ADR001 --&gt; ADR003[ADR-003: Namespace Topology]\n    ADR001 --&gt; ADR006[ADR-006: Backup Strategy]\n    ADR001 --&gt; ADR007[ADR-007: Monitoring]\n    ADR001 --&gt; ADR008[ADR-008: IAM Strategy]\n\n    ADR003 --&gt; ADR004[ADR-004: Admission Control]\n    ADR003 --&gt; ADR005[ADR-005: Network CNI]\n    ADR003 --&gt; ADR008\n\n    ADR008 --&gt; ADR002\n    ADR008 --&gt; ADR004\n    ADR008 --&gt; ADR007\n\n    ADR005 --&gt; ADR007\n    ADR002 --&gt; ADR004\n\n    style ADR001 fill:#ff9999\n    style ADR002 fill:#99ccff\n    style ADR003 fill:#99ff99\n    style ADR004 fill:#ffcc99\n    style ADR005 fill:#cc99ff\n    style ADR006 fill:#ffff99\n    style ADR007 fill:#ff99cc\n    style ADR008 fill:#99ffcc</code></pre>"},{"location":"architecture/adr/adr-table/#implementation-timeline","title":"Implementation Timeline","text":"Phase ADRs Duration Dependencies Key Deliverables Phase 1: Foundation ADR-001, ADR-003 4-6 weeks Infrastructure setup Multi-cluster setup, namespace topology Phase 2: Identity &amp; Access ADR-008 3-4 weeks Foundation complete Keycloak deployment, OIDC integration, MFA setup Phase 3: GitOps &amp; Policy ADR-002, ADR-004 3-4 weeks Foundation + IAM complete ArgoCD hub with OIDC auth, admission controllers Phase 4: Networking ADR-005 2-3 weeks Foundation complete Cilium CNI, Multus, network policies Phase 5: Operations ADR-006, ADR-007 4-5 weeks All previous phases Backup strategy, monitoring with IAM integration"},{"location":"architecture/adr/adr-table/#detailed-phase-dependencies","title":"Detailed Phase Dependencies","text":"<pre><code>gantt\n    title ADR Implementation Timeline\n    dateFormat  YYYY-MM-DD\n    section Phase 1: Foundation\n    ADR-001 Multi-Cluster     :done, foundation1, 2024-01-01, 4w\n    ADR-003 Namespace Topology :done, foundation2, 2024-01-15, 3w\n\n    section Phase 2: Identity\n    ADR-008 IAM Strategy       :active, iam, after foundation1, 4w\n\n    section Phase 3: GitOps\n    ADR-002 GitOps ArgoCD      :gitops, after iam, 3w\n    ADR-004 Admission Control  :admission, after iam, 3w\n\n    section Phase 4: Network\n    ADR-005 Network CNI        :network, after foundation2, 3w\n\n    section Phase 5: Operations\n    ADR-006 Backup Strategy    :backup, after gitops, 4w\n    ADR-007 Monitoring         :monitoring, after network, 4w</code></pre>"},{"location":"architecture/adr/adr-table/#critical-path-analysis","title":"Critical Path Analysis","text":"<p>Critical Dependencies: - ADR-008 (IAM) must be completed before GitOps and Admission Control implementation - ADR-003 (Namespace Topology) enables proper RBAC integration with IAM - ADR-007 (Monitoring) requires IAM integration for authentication and authorization - ADR-002 (GitOps) requires IAM for secure access control and audit trails</p> <p>Parallel Implementation Opportunities: - ADR-005 (Network CNI) can be implemented in parallel with IAM setup - ADR-006 (Backup) and ADR-007 (Monitoring) can be implemented concurrently in final phase</p> <p>This comprehensive table provides a complete overview of all architectural decisions, their relationships, and implementation considerations for the RH OVE multi-cluster ecosystem.</p>"},{"location":"architecture/requirements/fr/","title":"Functional Requirements","text":""},{"location":"architecture/requirements/fr/#overview","title":"Overview","text":"<p>This document outlines the functional requirements for the RH OVE multi-cluster ecosystem, derived from the architectural decisions documented in our ADRs.</p>"},{"location":"architecture/requirements/fr/#fr-001-multi-cluster-management","title":"FR-001: Multi-Cluster Management","text":""},{"location":"architecture/requirements/fr/#fr-0011-cluster-topology-management","title":"FR-001.1: Cluster Topology Management","text":"<ul> <li>Requirement: The system must implement application namespace-based topology for workload organization</li> <li>Rationale: Based on ADR-003, ensures strong isolation, simplified RBAC, and clear resource attribution</li> <li>Acceptance Criteria:</li> <li>Support namespace patterns: <code>{app-name}-{environment}</code></li> <li>Implement cross-namespace communication policies</li> <li>Enable namespace-level resource quotas and limits</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0012-multi-cluster-governance","title":"FR-001.2: Multi-Cluster Governance","text":"<ul> <li>Requirement: The system must support centralized governance across multiple application clusters from a management cluster</li> <li>Rationale: Based on ADR-001, provides separation of concerns and operational efficiency</li> <li>Acceptance Criteria:</li> <li>Deploy and manage policies from management cluster to application clusters</li> <li>Support cluster lifecycle management through RHACM</li> <li>Enable centralized monitoring and logging aggregation</li> </ul>"},{"location":"architecture/requirements/fr/#fr-002-gitops-integration","title":"FR-002: GitOps Integration","text":""},{"location":"architecture/requirements/fr/#fr-0021-argocd-hub-architecture","title":"FR-002.1: ArgoCD Hub Architecture","text":"<ul> <li>Requirement: The system must implement GitOps using ArgoCD in a hub-spoke pattern</li> <li>Rationale: Based on ADR-002, provides declarative configuration and audit trails</li> <li>Acceptance Criteria:</li> <li>Deploy ArgoCD Hub in management cluster with high availability</li> <li>Support Application of Applications pattern</li> <li>Enable automated sync for non-production, manual sync for production</li> <li>Integrate with Git repositories for all infrastructure and application configurations</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0022-configuration-management","title":"FR-002.2: Configuration Management","text":"<ul> <li>Requirement: All infrastructure and application configurations must be stored in Git repositories</li> <li>Rationale: Ensures version control, auditability, and rollback capabilities</li> <li>Acceptance Criteria:</li> <li>Support environment-specific overlays using Kustomize</li> <li>Enable automated deployment pipeline with proper approval workflows</li> <li>Provide drift detection and automatic remediation</li> </ul>"},{"location":"architecture/requirements/fr/#fr-003-admission-control-and-policy-management","title":"FR-003: Admission Control and Policy Management","text":""},{"location":"architecture/requirements/fr/#fr-0031-layered-admission-control","title":"FR-003.1: Layered Admission Control","text":"<ul> <li>Requirement: The system must implement layered admission control using OpenShift built-in controllers, KubeVirt webhooks, and Kyverno policies</li> <li>Rationale: Based on ADR-004, provides flexible, secure, and policy-driven resource validation</li> <li>Acceptance Criteria:</li> <li>Deploy Kyverno for custom policy management</li> <li>Implement Security Context Constraints for VM and container workloads</li> <li>Support policy-as-code with version control and automated deployment</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0032-security-policy-enforcement","title":"FR-003.2: Security Policy Enforcement","text":"<ul> <li>Requirement: The system must validate and enforce security policies before resource persistence</li> <li>Rationale: Prevents misconfiguration and ensures compliance</li> <li>Acceptance Criteria:</li> <li>Support both blocking and warning policy modes</li> <li>Provide detailed policy violation reporting</li> <li>Enable policy exemptions with proper approval workflows</li> </ul>"},{"location":"architecture/requirements/fr/#fr-004-networking-and-connectivity","title":"FR-004: Networking and Connectivity","text":""},{"location":"architecture/requirements/fr/#fr-0041-cilium-cni-implementation","title":"FR-004.1: Cilium CNI Implementation","text":"<ul> <li>Requirement: The system must use Cilium as the primary CNI with eBPF-powered networking</li> <li>Rationale: Based on ADR-005, provides superior performance, identity-aware security, and L7 capabilities</li> <li>Acceptance Criteria:</li> <li>Deploy Cilium with Hubble for network observability</li> <li>Implement identity-aware network policies</li> <li>Support transparent encryption using WireGuard</li> <li>Enable service mesh capabilities without sidecar proxies</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0042-multi-network-support","title":"FR-004.2: Multi-Network Support","text":"<ul> <li>Requirement: The system must support Multus for multi-network configurations</li> <li>Rationale: Enables legacy network integration, SR-IOV, and network segmentation</li> <li>Acceptance Criteria:</li> <li>Support NetworkAttachmentDefinitions for management, storage, and data networks</li> <li>Enable multiple network interfaces for VMs</li> <li>Support VLAN-based network segmentation</li> <li>Provide high-performance networking with SR-IOV</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0043-zero-trust-security","title":"FR-004.3: Zero Trust Security","text":"<ul> <li>Requirement: The system must implement zero trust network principles</li> <li>Rationale: Ensures security by default with explicit allow policies</li> <li>Acceptance Criteria:</li> <li>Implement default-deny network policies</li> <li>Support L7 HTTP/HTTPS policy enforcement</li> <li>Enable network flow monitoring and policy violation alerting</li> </ul>"},{"location":"architecture/requirements/fr/#fr-005-backup-and-disaster-recovery","title":"FR-005: Backup and Disaster Recovery","text":""},{"location":"architecture/requirements/fr/#fr-0051-centralized-backup-management","title":"FR-005.1: Centralized Backup Management","text":"<ul> <li>Requirement: The system must implement centralized backup using Rubrik for unified VM and container workload protection</li> <li>Rationale: Based on ADR-006, provides consistency, compliance, and operational efficiency</li> <li>Acceptance Criteria:</li> <li>Deploy Rubrik management in management cluster</li> <li>Support policy-driven backup scheduling and retention</li> <li>Enable application-consistent snapshots for VMs</li> <li>Provide deduplication and compression for storage optimization</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0052-disaster-recovery-capabilities","title":"FR-005.2: Disaster Recovery Capabilities","text":"<ul> <li>Requirement: The system must support cross-region disaster recovery and failover</li> <li>Rationale: Ensures business continuity and meets compliance requirements</li> <li>Acceptance Criteria:</li> <li>Support automated backup replication to cloud storage</li> <li>Enable point-in-time recovery for critical workloads</li> <li>Provide recovery testing and validation capabilities</li> <li>Support cross-cluster failover scenarios</li> </ul>"},{"location":"architecture/requirements/fr/#fr-006-monitoring-and-observability","title":"FR-006: Monitoring and Observability","text":""},{"location":"architecture/requirements/fr/#fr-0061-integrated-monitoring-stack","title":"FR-006.1: Integrated Monitoring Stack","text":"<ul> <li>Requirement: The system must implement integrated monitoring using Prometheus, Grafana, Dynatrace, and Hubble</li> <li>Rationale: Based on ADR-007, provides comprehensive observability across infrastructure and applications</li> <li>Acceptance Criteria:</li> <li>Deploy federated Prometheus for metrics collection across clusters</li> <li>Implement custom Grafana dashboards for infrastructure and application metrics</li> <li>Integrate Dynatrace for full-stack application performance monitoring</li> <li>Enable Hubble for network flow visibility and security monitoring</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0062-proactive-monitoring-and-alerting","title":"FR-006.2: Proactive Monitoring and Alerting","text":"<ul> <li>Requirement: The system must provide proactive monitoring with intelligent alerting</li> <li>Rationale: Enables early detection and resolution of issues</li> <li>Acceptance Criteria:</li> <li>Support threshold-based and anomaly detection alerting</li> <li>Integrate with incident management systems</li> <li>Provide runbook automation for common issues</li> <li>Enable custom alerting rules per application namespace</li> </ul>"},{"location":"architecture/requirements/fr/#fr-007-virtual-machine-management","title":"FR-007: Virtual Machine Management","text":""},{"location":"architecture/requirements/fr/#fr-0071-vm-lifecycle-management","title":"FR-007.1: VM Lifecycle Management","text":"<ul> <li>Requirement: The system must support complete VM lifecycle management using KubeVirt</li> <li>Rationale: Provides unified management of VMs and containers</li> <li>Acceptance Criteria:</li> <li>Support VM creation, scaling, and termination</li> <li>Enable VM live migration for maintenance</li> <li>Provide VM template management and cloning</li> <li>Support both Windows and Linux guest operating systems</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0072-storage-management","title":"FR-007.2: Storage Management","text":"<ul> <li>Requirement: The system must provide flexible storage management for VMs using DataVolumes and CDI</li> <li>Rationale: Enables efficient storage provisioning and management</li> <li>Acceptance Criteria:</li> <li>Support multiple storage classes for different performance tiers</li> <li>Enable volume expansion and snapshotting</li> <li>Provide image import from registries, HTTP, and S3 sources</li> <li>Support persistent volume cloning for template workflows</li> </ul>"},{"location":"architecture/requirements/fr/#fr-008-security-and-compliance","title":"FR-008: Security and Compliance","text":""},{"location":"architecture/requirements/fr/#fr-0081-identity-and-access-management","title":"FR-008.1: Identity and Access Management","text":"<ul> <li>Requirement: The system must integrate with enterprise identity providers using OIDC for centralized authentication and authorization</li> <li>Rationale: Based on ADR-008, ensures consistent security policies, audit trails, and enterprise integration</li> <li>Acceptance Criteria:</li> <li>Deploy Keycloak (Red Hat SSO) as primary OIDC provider with HA configuration</li> <li>Support LDAP/Active Directory federation for existing user directories</li> <li>Implement Single Sign-On (SSO) across all clusters and services</li> <li>Deploy Dex OIDC proxy for service authentication</li> <li>Integrate OpenShift OAuth with OIDC claims mapping</li> <li>Implement namespace-based RBAC with OIDC group delegation</li> <li>Enable service account automation with time-limited tokens (15min-2hours)</li> <li>Provide comprehensive audit logging for all authentication and authorization events</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0082-multi-factor-authentication-and-security-controls","title":"FR-008.2: Multi-Factor Authentication and Security Controls","text":"<ul> <li>Requirement: The system must enforce multi-factor authentication and implement comprehensive security controls</li> <li>Rationale: Based on ADR-008, ensures zero trust security principles and enhanced threat protection</li> <li>Acceptance Criteria:</li> <li>Enforce mandatory MFA for all administrative accounts using TOTP/SMS</li> <li>Implement token security with proper expiration policies (access: 15min, refresh: 1hour, ID: 5min)</li> <li>Deploy network policies restricting authentication service communication</li> <li>Support TLS 1.3 encryption for all authentication traffic</li> <li>Enable certificate-based mutual TLS for service-to-service communication</li> <li>Implement automated user provisioning/deprovisioning via SCIM integration</li> <li>Support self-service password reset and MFA device management</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0083-compliance-management","title":"FR-008.3: Compliance Management","text":"<ul> <li>Requirement: The system must support automated compliance checking and reporting with IAM integration</li> <li>Rationale: Ensures adherence to regulatory requirements with comprehensive identity audit trails</li> <li>Acceptance Criteria:</li> <li>Support SOC 2, GDPR, and HIPAA compliance frameworks with identity-aware controls</li> <li>Enable automated compliance scanning and reporting with authentication metrics</li> <li>Provide policy violation tracking and remediation workflows</li> <li>Support data classification and handling policies</li> <li>Generate automated access review reports with RBAC analysis</li> <li>Track failed authentication attempts and security incidents</li> <li>Support \"right to be forgotten\" with automated user data deletion</li> </ul>"},{"location":"architecture/requirements/fr/#fr-009-developer-experience","title":"FR-009: Developer Experience","text":""},{"location":"architecture/requirements/fr/#fr-0091-self-service-capabilities","title":"FR-009.1: Self-Service Capabilities","text":"<ul> <li>Requirement: The system must provide self-service capabilities for development teams</li> <li>Rationale: Improves developer productivity and reduces operational overhead</li> <li>Acceptance Criteria:</li> <li>Provide web-based interfaces for resource management</li> <li>Support CLI tools for automation and scripting</li> <li>Enable template-based resource provisioning</li> <li>Provide comprehensive documentation and tutorials</li> </ul>"},{"location":"architecture/requirements/fr/#fr-0092-cicd-integration","title":"FR-009.2: CI/CD Integration","text":"<ul> <li>Requirement: The system must integrate with existing CI/CD pipelines</li> <li>Rationale: Enables automated testing and deployment workflows</li> <li>Acceptance Criteria:</li> <li>Support webhook integration with Git repositories</li> <li>Enable automated image building and scanning</li> <li>Provide integration with popular CI/CD tools (Jenkins, GitLab CI, etc.)</li> <li>Support blue-green and canary deployment strategies</li> </ul>"},{"location":"architecture/requirements/nfr/","title":"Non-Functional Requirements","text":""},{"location":"architecture/requirements/nfr/#overview","title":"Overview","text":"<p>This document outlines the non-functional requirements for the RH OVE multi-cluster ecosystem, defining quality attributes and constraints.</p>"},{"location":"architecture/requirements/nfr/#performance-requirements","title":"Performance Requirements","text":""},{"location":"architecture/requirements/nfr/#latency","title":"Latency","text":"<ul> <li>FR-NFR-001: API response time must be &lt; 200ms for 95% of requests</li> <li>FR-NFR-002: Cross-cluster communication latency must be &lt; 50ms</li> <li>FR-NFR-003: VM startup time must be &lt; 60 seconds for standard workloads</li> </ul>"},{"location":"architecture/requirements/nfr/#throughput","title":"Throughput","text":"<ul> <li>FR-NFR-004: System must support deployment of 100+ VMs per hour per cluster</li> <li>FR-NFR-005: Monitoring system must handle 10,000+ metrics per second</li> <li>FR-NFR-006: Log aggregation must process 1GB+ of logs per hour</li> </ul>"},{"location":"architecture/requirements/nfr/#resource-utilization","title":"Resource Utilization","text":"<ul> <li>FR-NFR-007: CPU utilization must not exceed 80% under normal load</li> <li>FR-NFR-008: Memory utilization must not exceed 85% under normal load</li> <li>FR-NFR-009: Storage utilization must not exceed 90% capacity</li> </ul>"},{"location":"architecture/requirements/nfr/#availability-requirements","title":"Availability Requirements","text":""},{"location":"architecture/requirements/nfr/#uptime","title":"Uptime","text":"<ul> <li>FR-NFR-010: Production clusters must achieve 99.9% uptime (&lt; 8.77 hours downtime/year)</li> <li>FR-NFR-011: Management cluster must achieve 99.95% uptime (&lt; 4.38 hours downtime/year)</li> <li>FR-NFR-012: Critical services must have &lt; 30 seconds failover time</li> </ul>"},{"location":"architecture/requirements/nfr/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>FR-NFR-013: RPO (Recovery Point Objective) must be &lt; 4 hours</li> <li>FR-NFR-014: RTO (Recovery Time Objective) must be &lt; 8 hours</li> <li>FR-NFR-015: Cross-region failover must complete within 15 minutes</li> </ul>"},{"location":"architecture/requirements/nfr/#scalability-requirements","title":"Scalability Requirements","text":""},{"location":"architecture/requirements/nfr/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>FR-NFR-016: System must support minimum 10 application clusters</li> <li>FR-NFR-017: Each cluster must support 1000+ pods</li> <li>FR-NFR-018: System must scale to 50,000+ containers across all clusters</li> </ul>"},{"location":"architecture/requirements/nfr/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>FR-NFR-019: Individual VMs must scale up to 64 vCPUs and 512GB RAM</li> <li>FR-NFR-020: Storage volumes must scale up to 100TB per volume</li> <li>FR-NFR-021: Network bandwidth must scale to 25Gbps per node</li> </ul>"},{"location":"architecture/requirements/nfr/#security-requirements","title":"Security Requirements","text":""},{"location":"architecture/requirements/nfr/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>FR-NFR-022: All API access must use multi-factor authentication</li> <li>FR-NFR-023: RBAC must be enforced across all cluster components</li> <li>FR-NFR-024: Service accounts must use time-limited tokens</li> </ul>"},{"location":"architecture/requirements/nfr/#data-protection","title":"Data Protection","text":"<ul> <li>FR-NFR-025: All data in transit must be encrypted (TLS 1.3+)</li> <li>FR-NFR-026: All data at rest must be encrypted (AES-256)</li> <li>FR-NFR-027: Encryption keys must be rotated every 90 days</li> </ul>"},{"location":"architecture/requirements/nfr/#network-security","title":"Network Security","text":"<ul> <li>FR-NFR-028: All inter-cluster communication must be encrypted</li> <li>FR-NFR-029: Network policies must deny by default</li> <li>FR-NFR-030: Network segmentation must isolate environments</li> </ul>"},{"location":"architecture/requirements/nfr/#compliance","title":"Compliance","text":"<ul> <li>FR-NFR-031: System must maintain SOC 2 Type II compliance</li> <li>FR-NFR-032: Audit logs must be retained for 7 years</li> <li>FR-NFR-033: Security scanning must occur daily</li> </ul>"},{"location":"architecture/requirements/nfr/#reliability-requirements","title":"Reliability Requirements","text":""},{"location":"architecture/requirements/nfr/#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>FR-NFR-034: System must survive single node failures without service interruption</li> <li>FR-NFR-035: System must survive single AZ failures in multi-AZ deployments</li> <li>FR-NFR-036: Data must be replicated across minimum 3 nodes</li> </ul>"},{"location":"architecture/requirements/nfr/#error-handling","title":"Error Handling","text":"<ul> <li>FR-NFR-037: All errors must be logged with appropriate severity levels</li> <li>FR-NFR-038: Transient errors must be retried with exponential backoff</li> <li>FR-NFR-039: Critical errors must trigger automated alerting</li> </ul>"},{"location":"architecture/requirements/nfr/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>FR-NFR-040: System health must be monitored with 99% coverage</li> <li>FR-NFR-041: Metrics retention must be minimum 1 year</li> <li>FR-NFR-042: Distributed tracing must be enabled for all services</li> </ul>"},{"location":"architecture/requirements/nfr/#maintainability-requirements","title":"Maintainability Requirements","text":""},{"location":"architecture/requirements/nfr/#deployment-updates","title":"Deployment &amp; Updates","text":"<ul> <li>FR-NFR-043: Zero-downtime deployments must be supported</li> <li>FR-NFR-044: Rollback capability must be available within 5 minutes</li> <li>FR-NFR-045: Automated testing must cover 90%+ of functionality</li> </ul>"},{"location":"architecture/requirements/nfr/#configuration-management","title":"Configuration Management","text":"<ul> <li>FR-NFR-046: All configuration must be version controlled</li> <li>FR-NFR-047: Configuration changes must be auditable</li> <li>FR-NFR-048: Infrastructure as Code must be used for all deployments</li> </ul>"},{"location":"architecture/requirements/nfr/#documentation","title":"Documentation","text":"<ul> <li>FR-NFR-049: All APIs must have OpenAPI specifications</li> <li>FR-NFR-050: Runbooks must be available for all operational procedures</li> <li>FR-NFR-051: Architecture decisions must be documented in ADRs</li> </ul>"},{"location":"architecture/requirements/nfr/#usability-requirements","title":"Usability Requirements","text":""},{"location":"architecture/requirements/nfr/#user-interface","title":"User Interface","text":"<ul> <li>FR-NFR-052: Web UI must be responsive and mobile-friendly</li> <li>FR-NFR-053: API must follow RESTful design principles</li> <li>FR-NFR-054: CLI tools must provide comprehensive help documentation</li> </ul>"},{"location":"architecture/requirements/nfr/#accessibility","title":"Accessibility","text":"<ul> <li>FR-NFR-055: Web interfaces must meet WCAG 2.1 AA standards</li> <li>FR-NFR-056: Multi-language support must be available</li> <li>FR-NFR-057: High contrast mode must be supported</li> </ul>"},{"location":"architecture/requirements/nfr/#capacity-requirements","title":"Capacity Requirements","text":""},{"location":"architecture/requirements/nfr/#storage","title":"Storage","text":"<ul> <li>FR-NFR-058: Minimum 100TB usable storage per production cluster</li> <li>FR-NFR-059: Storage must support 10,000+ IOPS per cluster</li> <li>FR-NFR-060: Backup storage must retain data for 5 years</li> </ul>"},{"location":"architecture/requirements/nfr/#network","title":"Network","text":"<ul> <li>FR-NFR-061: Minimum 10Gbps connectivity between clusters</li> <li>FR-NFR-062: Maximum 5ms latency within cluster networks</li> <li>FR-NFR-063: Network must support jumbo frames (9000 MTU)</li> </ul>"},{"location":"architecture/requirements/nfr/#compute","title":"Compute","text":"<ul> <li>FR-NFR-064: Minimum 1000 vCPUs available per production cluster</li> <li>FR-NFR-065: Minimum 4TB RAM available per production cluster</li> <li>FR-NFR-066: Support for GPU workloads (minimum 8 GPUs per cluster)</li> </ul>"},{"location":"architecture/requirements/nfr/#compliance-regulatory-requirements","title":"Compliance &amp; Regulatory Requirements","text":""},{"location":"architecture/requirements/nfr/#data-governance","title":"Data Governance","text":"<ul> <li>FR-NFR-067: Data must be classified and labeled appropriately</li> <li>FR-NFR-068: PII data must be encrypted and access-controlled</li> <li>FR-NFR-069: Data retention policies must be automatically enforced</li> </ul>"},{"location":"architecture/requirements/nfr/#audit-logging","title":"Audit &amp; Logging","text":"<ul> <li>FR-NFR-070: All administrative actions must be logged</li> <li>FR-NFR-071: Logs must be tamper-proof and timestamped</li> <li>FR-NFR-072: Compliance reports must be generated automatically</li> </ul>"},{"location":"architecture/requirements/nfr/#privacy","title":"Privacy","text":"<ul> <li>FR-NFR-073: Right to be forgotten must be supported</li> <li>FR-NFR-074: Data minimization principles must be enforced</li> <li>FR-NFR-075: Privacy by design must be implemented</li> </ul>"},{"location":"architecture/requirements/requirements-table/","title":"Requirements Summary Table","text":"<p>This document provides a comprehensive overview of all functional and non-functional requirements for the RH OVE multi-cluster ecosystem.</p>"},{"location":"architecture/requirements/requirements-table/#functional-requirements-summary","title":"Functional Requirements Summary","text":"Requirement ID Category Title ADR Reference Acceptance Criteria Summary FR-001.1 Multi-Cluster Management Cluster Topology Management ADR-003 Namespace patterns <code>{app-name}-{environment}</code>, cross-namespace policies, resource quotas FR-001.2 Multi-Cluster Management Multi-Cluster Governance ADR-001 RHACM cluster lifecycle, centralized policies, monitoring aggregation FR-002.1 GitOps Integration ArgoCD Hub Architecture ADR-002 HA ArgoCD Hub, Application of Applications, automated/manual sync FR-002.2 GitOps Integration Configuration Management ADR-002 Git-based config, Kustomize overlays, drift detection FR-003.1 Policy Management Layered Admission Control ADR-004 Kyverno, Security Context Constraints, policy-as-code FR-003.2 Policy Management Security Policy Enforcement ADR-004 Blocking/warning modes, violation reporting, exemption workflows FR-004.1 Networking Cilium CNI Implementation ADR-005 Cilium with Hubble, identity-aware policies, WireGuard encryption FR-004.2 Networking Multi-Network Support ADR-005 Multus NADs, multiple VM interfaces, VLAN segmentation, SR-IOV FR-004.3 Networking Zero Trust Security ADR-005 Default-deny policies, L7 enforcement, flow monitoring FR-005.1 Backup &amp; DR Centralized Backup Management ADR-006 Rubrik management, policy-driven scheduling, VM snapshots FR-005.2 Backup &amp; DR Disaster Recovery Capabilities ADR-006 Cloud replication, point-in-time recovery, cross-cluster failover FR-006.1 Monitoring Integrated Monitoring Stack ADR-007 Federated Prometheus, Grafana, Dynatrace, Hubble integration FR-006.2 Monitoring Proactive Monitoring and Alerting ADR-007 Threshold/anomaly detection, ITSM integration, runbook automation FR-007.1 VM Management VM Lifecycle Management KubeVirt VM CRUD operations, live migration, template management FR-007.2 VM Management Storage Management KubeVirt Multiple storage classes, volume expansion, image import FR-008.1 Security Identity and Access Management ADR-008 Keycloak OIDC, LDAP/AD federation, SSO, Dex proxy, OAuth integration, RBAC FR-008.2 Security Multi-Factor Authentication and Security Controls ADR-008 Mandatory MFA, token security, TLS 1.3, mutual TLS, SCIM automation FR-008.3 Security Compliance Management ADR-008 SOC 2/GDPR/HIPAA with IAM, automated access reviews, incident tracking FR-009.1 Developer Experience Self-Service Capabilities DevEx Web interfaces, CLI tools, template provisioning FR-009.2 Developer Experience CI/CD Integration DevOps Git webhooks, image building, deployment strategies"},{"location":"architecture/requirements/requirements-table/#non-functional-requirements-summary","title":"Non-Functional Requirements Summary","text":""},{"location":"architecture/requirements/requirements-table/#performance-requirements","title":"Performance Requirements","text":"Requirement ID Category Metric Target Value Rationale FR-NFR-001 Latency API Response Time &lt; 200ms (95th percentile) User experience optimization FR-NFR-002 Latency Cross-Cluster Communication &lt; 50ms Real-time data synchronization FR-NFR-003 Latency VM Startup Time &lt; 60 seconds Rapid workload provisioning FR-NFR-004 Throughput VM Deployment Rate 100+ VMs/hour/cluster Support for scaling events FR-NFR-005 Throughput Monitoring Metrics 10,000+ metrics/second Comprehensive observability FR-NFR-006 Throughput Log Processing 1GB+ logs/hour Adequate logging capacity FR-NFR-007 Resource Utilization CPU Usage \u2264 80% under normal load Performance headroom FR-NFR-008 Resource Utilization Memory Usage \u2264 85% under normal load Stability margin FR-NFR-009 Resource Utilization Storage Usage \u2264 90% capacity Growth accommodation"},{"location":"architecture/requirements/requirements-table/#availability-requirements","title":"Availability Requirements","text":"Requirement ID Category Metric Target Value Business Impact FR-NFR-010 Uptime Production Clusters 99.9% (&lt; 8.77 hours/year downtime) Business continuity FR-NFR-011 Uptime Management Cluster 99.95% (&lt; 4.38 hours/year downtime) Central control availability FR-NFR-012 Failover Critical Services &lt; 30 seconds failover time Minimal service disruption FR-NFR-013 Disaster Recovery RPO (Recovery Point Objective) &lt; 4 hours Data loss tolerance FR-NFR-014 Disaster Recovery RTO (Recovery Time Objective) &lt; 8 hours Recovery time tolerance FR-NFR-015 Disaster Recovery Cross-Region Failover &lt; 15 minutes Geographic resilience"},{"location":"architecture/requirements/requirements-table/#scalability-requirements","title":"Scalability Requirements","text":"Requirement ID Category Metric Target Value Scalability Dimension FR-NFR-016 Horizontal Scaling Application Clusters Minimum 10 clusters Multi-environment support FR-NFR-017 Horizontal Scaling Pods per Cluster 1000+ pods Workload density FR-NFR-018 Horizontal Scaling Total Containers 50,000+ across all clusters Enterprise scale FR-NFR-019 Vertical Scaling VM Resources Up to 64 vCPUs, 512GB RAM Large workload support FR-NFR-020 Vertical Scaling Storage Volumes Up to 100TB per volume Big data capabilities FR-NFR-021 Vertical Scaling Network Bandwidth Up to 25Gbps per node High-performance networking"},{"location":"architecture/requirements/requirements-table/#security-requirements","title":"Security Requirements","text":"Requirement ID Category Requirement Implementation Compliance Impact FR-NFR-022 Authentication Multi-Factor Authentication All API access Enhanced security posture FR-NFR-023 Authorization RBAC Enforcement All cluster components Access control consistency FR-NFR-024 Authentication Service Account Tokens Time-limited tokens Reduced credential exposure FR-NFR-025 Data Protection Encryption in Transit TLS 1.3+ Data confidentiality FR-NFR-026 Data Protection Encryption at Rest AES-256 Data protection compliance FR-NFR-027 Key Management Key Rotation Every 90 days Security best practices FR-NFR-028 Network Security Inter-Cluster Encryption All communications End-to-end security FR-NFR-029 Network Security Network Policies Deny by default Zero trust implementation FR-NFR-030 Network Security Environment Isolation Network segmentation Security boundaries FR-NFR-031 Compliance SOC 2 Compliance Type II certification Regulatory adherence FR-NFR-032 Audit Log Retention 7 years Compliance requirements FR-NFR-033 Security Vulnerability Scanning Daily scans Proactive security"},{"location":"architecture/requirements/requirements-table/#reliability-requirements","title":"Reliability Requirements","text":"Requirement ID Category Requirement Target Reliability Impact FR-NFR-034 Fault Tolerance Single Node Failure No service interruption High availability FR-NFR-035 Fault Tolerance Single AZ Failure Continued operation Geographic resilience FR-NFR-036 Data Replication Minimum Replicas 3 nodes Data durability FR-NFR-037 Error Handling Error Logging All errors with severity Operational visibility FR-NFR-038 Error Handling Retry Logic Exponential backoff Resilient operations FR-NFR-039 Error Handling Critical Error Alerting Automated notifications Rapid incident response FR-NFR-040 Monitoring Health Coverage 99% system coverage Comprehensive monitoring FR-NFR-041 Monitoring Metrics Retention Minimum 1 year Historical analysis FR-NFR-042 Observability Distributed Tracing All services End-to-end visibility"},{"location":"architecture/requirements/requirements-table/#maintainability-requirements","title":"Maintainability Requirements","text":"Requirement ID Category Requirement Implementation Operational Impact FR-NFR-043 Deployment Zero-Downtime Updates Rolling deployments Service continuity FR-NFR-044 Deployment Rollback Capability &lt; 5 minutes Rapid recovery FR-NFR-045 Testing Automated Test Coverage 90%+ functionality Quality assurance FR-NFR-046 Configuration Version Control All configuration Change management FR-NFR-047 Configuration Change Auditability All changes tracked Compliance and debugging FR-NFR-048 Infrastructure Infrastructure as Code All deployments Consistency and repeatability FR-NFR-049 Documentation API Specifications OpenAPI for all APIs Developer experience FR-NFR-050 Documentation Operational Runbooks All procedures Operational consistency FR-NFR-051 Documentation Architecture Decisions ADR documentation Knowledge management"},{"location":"architecture/requirements/requirements-table/#usability-requirements","title":"Usability Requirements","text":"Requirement ID Category Requirement Standard/Implementation User Impact FR-NFR-052 User Interface Responsive Design Mobile-friendly web UI Multi-device access FR-NFR-053 API Design RESTful Principles Standard REST API Developer experience FR-NFR-054 CLI Tools Help Documentation Comprehensive help User self-service FR-NFR-055 Accessibility WCAG Compliance 2.1 AA standards Inclusive design FR-NFR-056 Localization Multi-Language Support International users Global accessibility FR-NFR-057 Accessibility High Contrast Mode Visual accessibility Enhanced usability"},{"location":"architecture/requirements/requirements-table/#capacity-requirements","title":"Capacity Requirements","text":"Requirement ID Category Resource Minimum Capacity Scaling Consideration FR-NFR-058 Storage Usable Storage 100TB per production cluster Data growth accommodation FR-NFR-059 Storage IOPS Performance 10,000+ IOPS per cluster High-performance workloads FR-NFR-060 Storage Backup Retention 5 years Compliance and recovery FR-NFR-061 Network Inter-Cluster Connectivity 10Gbps minimum High-bandwidth applications FR-NFR-062 Network Intra-Cluster Latency \u2264 5ms maximum Real-time applications FR-NFR-063 Network MTU Support 9000 bytes (jumbo frames) Network optimization FR-NFR-064 Compute CPU Capacity 1000 vCPUs per production cluster Computational workloads FR-NFR-065 Compute Memory Capacity 4TB RAM per production cluster Memory-intensive applications FR-NFR-066 Compute GPU Support 8+ GPUs per cluster AI/ML workloads"},{"location":"architecture/requirements/requirements-table/#compliance-regulatory-requirements","title":"Compliance &amp; Regulatory Requirements","text":"Requirement ID Category Requirement Implementation Regulatory Framework FR-NFR-067 Data Governance Data Classification Automated labeling Privacy regulations FR-NFR-068 Data Protection PII Handling Encryption and access control GDPR, HIPAA FR-NFR-069 Data Governance Retention Policies Automated enforcement Legal requirements FR-NFR-070 Audit Administrative Logging All admin actions SOX, SOC 2 FR-NFR-071 Audit Log Integrity Tamper-proof, timestamped Legal evidence FR-NFR-072 Compliance Automated Reporting Compliance dashboards Regulatory reporting FR-NFR-073 Privacy Right to be Forgotten Data deletion capability GDPR Article 17 FR-NFR-074 Privacy Data Minimization Principle enforcement Privacy by design FR-NFR-075 Privacy Privacy by Design Built-in privacy controls Proactive compliance"},{"location":"architecture/requirements/requirements-table/#requirements-traceability-matrix","title":"Requirements Traceability Matrix","text":""},{"location":"architecture/requirements/requirements-table/#adr-to-requirements-mapping","title":"ADR to Requirements Mapping","text":"ADR Related Functional Requirements Related Non-Functional Requirements ADR-001: Multi-Cluster Pattern FR-001.1, FR-001.2 FR-NFR-010, FR-NFR-011, FR-NFR-016, FR-NFR-034, FR-NFR-035 ADR-002: GitOps ArgoCD FR-002.1, FR-002.2 FR-NFR-043, FR-NFR-044, FR-NFR-046, FR-NFR-047, FR-NFR-048 ADR-003: Namespace Topology FR-001.1, FR-008.1 FR-NFR-023, FR-NFR-030, FR-NFR-067, FR-NFR-068 ADR-004: Admission Controller FR-003.1, FR-003.2 FR-NFR-022, FR-NFR-023, FR-NFR-031, FR-NFR-037, FR-NFR-070 ADR-005: Cilium CNI FR-004.1, FR-004.2, FR-004.3 FR-NFR-002, FR-NFR-021, FR-NFR-028, FR-NFR-029, FR-NFR-061, FR-NFR-062 ADR-006: Backup Strategy FR-005.1, FR-005.2 FR-NFR-013, FR-NFR-014, FR-NFR-015, FR-NFR-032, FR-NFR-060 ADR-007: Monitoring Strategy FR-006.1, FR-006.2 FR-NFR-005, FR-NFR-040, FR-NFR-041, FR-NFR-042, FR-NFR-072 ADR-008: IAM Strategy FR-008.1, FR-008.2, FR-008.3 FR-NFR-022, FR-NFR-023, FR-NFR-024, FR-NFR-025, FR-NFR-031, FR-NFR-070, FR-NFR-073"},{"location":"architecture/requirements/requirements-table/#requirements-coverage-analysis","title":"Requirements Coverage Analysis","text":"Category Total Requirements Critical Requirements Implementation Status Functional Requirements 19 sub-requirements 12 critical Architecture defined Performance Requirements 9 requirements 6 critical Targets established Availability Requirements 6 requirements 6 critical SLAs defined Scalability Requirements 6 requirements 4 critical Capacity planned Security Requirements 12 requirements 12 critical Controls specified Reliability Requirements 9 requirements 6 critical Patterns established Maintainability Requirements 9 requirements 5 critical Processes defined Usability Requirements 6 requirements 2 critical Standards adopted Capacity Requirements 9 requirements 6 critical Minimums established Compliance Requirements 9 requirements 9 critical Frameworks aligned"},{"location":"architecture/requirements/requirements-table/#requirements-validation-criteria","title":"Requirements Validation Criteria","text":""},{"location":"architecture/requirements/requirements-table/#functional-requirements-validation","title":"Functional Requirements Validation","text":"<ul> <li>FR Validation Method: Acceptance criteria testing, integration testing, user acceptance testing</li> <li>FR Success Metrics: All acceptance criteria met, integration tests pass, user stories completed</li> <li>FR Review Frequency: Sprint reviews, milestone assessments, release validation</li> </ul>"},{"location":"architecture/requirements/requirements-table/#non-functional-requirements-validation","title":"Non-Functional Requirements Validation","text":"<ul> <li>NFR Validation Method: Performance testing, load testing, security testing, compliance auditing</li> <li>NFR Success Metrics: All targets met or exceeded, benchmarks achieved, compliance verified</li> <li>NFR Review Frequency: Continuous monitoring, quarterly assessments, annual compliance reviews</li> </ul> <p>This comprehensive requirements table provides complete traceability from high-level business needs through detailed technical requirements, enabling effective validation and implementation tracking for the RH OVE multi-cluster ecosystem.</p>"},{"location":"deployment/configuration/","title":"Deployment Configuration","text":""},{"location":"deployment/configuration/#overview","title":"Overview","text":"<p>This document provides configuration guidelines for the RH OVE deployment, focusing on customization and parameters essential for adapting the solution to your specific environment.</p>"},{"location":"deployment/configuration/#openshift-configuration","title":"OpenShift Configuration","text":""},{"location":"deployment/configuration/#cluster-configuration","title":"Cluster Configuration","text":"<p>Customize your OpenShift cluster with the necessary configurations to optimize performance and security:</p> <pre><code>apiVersion: config.openshift.io/v1\nkind: ClusterVersion\nmetadata:\n  name: version\nspec:\n  channel: stable\n  upstream: https://api.openshift.com/api/upgrades_info/v1/graph\n\n# Customization to networking\napiVersion: operator.openshift.io/v1\nkind: Network\nmetadata:\n  name: cluster\nspec:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  serviceNetwork:\n  - 172.30.0.0/16\n</code></pre>"},{"location":"deployment/configuration/#node-configuration","title":"Node Configuration","text":"<p>Optimize your nodes for workload management:</p> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  name: worker\nspec:\n  machineConfigSelector:\n    matchExpressions:\n    - key: machineconfiguration.openshift.io/role\n      operator: In\n      values:\n      - worker\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/worker: \"\"\n\n# Taints to manage workloads effectively.\napiVersion: v1\nkind: Node\nmetadata:\n  name: node-1\nspec:\n  taints:\n  - key: app\n    value: high-performing\n    effect: NoSchedule\n</code></pre>"},{"location":"deployment/configuration/#network-configuration","title":"Network Configuration","text":"<p>Customize your Cilium CNI settings:</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkConfig\nmetadata:\n  name: cilium-config\nspec:\n  endpointRoutes: true\n  devices:\n  - eth0\n  autoDirectNodeRoutes: true\n\n# Policy for specific namespace isolation requirements\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: namespace-isolation-policy\n  namespace: critical-apps\nspec:\n  endpointSelector:\n    matchLabels:\n      app: critical-environment\n  ingress:\n    fromEndpoints:\n    - matchLabels:\n        access: dedicated\n</code></pre>"},{"location":"deployment/configuration/#storage-configuration","title":"Storage Configuration","text":"<p>Manage your storage setups efficiently:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: performance-storage\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: io1\n  iopsPerGB: \"50\"\n  encrypted: \"true\"\nreclaimPolicy: Retain\nvolumeBindingMode: WaitForFirstConsumer\n\n# PVC for critical workloads needing high IOPS\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: critical-workload-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: performance-storage\n</code></pre>"},{"location":"deployment/configuration/#security-configuration","title":"Security Configuration","text":"<p>Strengthen the security of your deployment:</p> <pre><code># Role-based access control\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: secure-namespace\n  name: critical-role\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n\n# Pod Security Policies\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: restricted-psp\nspec:\n  privileged: false\n  allowPrivilegeEscalation: false\n  requiredDropCapabilities:\n  - ALL\n  volumes:\n  - 'configMap'\n  - 'emptyDir'\n  - 'persistentVolumeClaim'\n</code></pre>"},{"location":"deployment/configuration/#conclusion","title":"Conclusion","text":"<p>By properly configuring these parameters, you can ensure that your RH OVE deployment is optimized for performance, security, and operational effectiveness. Adjust configurations based on specific organizational policies and workload demands.</p>"},{"location":"deployment/installation/","title":"Installation Guide","text":""},{"location":"deployment/installation/#overview","title":"Overview","text":"<p>This installation guide provides step-by-step instructions to deploy the RH OVE ecosystem using a multi-cluster architecture. The deployment follows a hub-and-spoke pattern with one management cluster and multiple application clusters for different environments (production, staging, development).</p>"},{"location":"deployment/installation/#multi-cluster-architecture","title":"Multi-Cluster Architecture","text":"<p>The RH OVE ecosystem consists of:</p> <ul> <li> <p>1 Management Cluster: Centralized control plane for governance, policy, monitoring, and GitOps</p> </li> <li> <p>N Application Clusters: Dedicated workload execution environments for virtual machines and containers</p> </li> </ul>"},{"location":"deployment/installation/#installation-flow","title":"Installation Flow","text":"<pre><code>graph TD\n    A[Prerequisites Check] --&gt; B[Management Cluster Setup]\n    B --&gt; C[Install RHACM Hub]\n    C --&gt; D[Deploy ArgoCD Hub]\n    D --&gt; E[Setup RHACS Central]\n    E --&gt; F[Configure Observability Stack]\n    F --&gt; G[Application Cluster Provisioning]\n    G --&gt; H[Deploy RH OVE to App Clusters]\n    H --&gt; I[Configure Multi-Network]\n    I --&gt; J[Setup Backup &amp; DR]\n    J --&gt; K[Validation &amp; Testing]</code></pre>"},{"location":"deployment/installation/#core-component-installation","title":"Core Component Installation","text":""},{"location":"deployment/installation/#openshift-cluster-setup","title":"OpenShift Cluster Setup","text":"<ol> <li>Install OpenShift</li> <li>Follow OpenShift Installation Docs to set up the cluster.</li> <li> <p>Choose between IPI or UPI depending on your infrastructure.</p> </li> <li> <p>Verify Cluster Health <pre><code>oc get nodes\noc get pods -n openshift-apiserver\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#virtualization-operator","title":"Virtualization Operator","text":"<ol> <li> <p>Install OpenShift Virtualization <pre><code>oc apply -f https://path/to/virtualization-operator.yaml\n</code></pre></p> </li> <li> <p>Verify Installation <pre><code>oc get pods -n openshift-cnv\noc get kubevirt.kubevirt.io/kubevirt -n openshift-cnv\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#cilium-cni","title":"Cilium CNI","text":"<ol> <li> <p>Install Cilium <pre><code>helm repo add cilium https://helm.cilium.io/\nhelm install cilium cilium/cilium --namespace kube-system\n</code></pre></p> </li> <li> <p>Verify Cilium Status <pre><code>cilium status\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#kyverno-policy-engine","title":"Kyverno Policy Engine","text":"<ol> <li> <p>Install Kyverno <pre><code>kubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.5.2/install.yaml\n</code></pre></p> </li> <li> <p>Apply Policies <pre><code>kubectl apply -f /path/to/policy-files\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#monitoring-setup","title":"Monitoring Setup","text":""},{"location":"deployment/installation/#dynatrace-integration","title":"Dynatrace Integration","text":"<ol> <li> <p>Install Dynatrace Operator <pre><code>oc apply -f https://path/to/dynatrace-operator.yaml\n</code></pre></p> </li> <li> <p>Configure DynaKube <pre><code>oc apply -f /path/to/dynakube-config.yaml\n</code></pre></p> </li> <li> <p>Verify Monitoring <pre><code>oc get pods -n dynatrace\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#prometheus-and-grafana","title":"Prometheus and Grafana","text":"<ol> <li> <p>Install Prometheus Operator <pre><code>oc apply -f https://path/to/prometheus-operator.yaml\n</code></pre></p> </li> <li> <p>Setup Grafana <pre><code>oc apply -f https://path/to/grafana-deployment.yaml\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#backup-configuration","title":"Backup Configuration","text":""},{"location":"deployment/installation/#rubrik-integration","title":"Rubrik Integration","text":"<ol> <li> <p>Install Rubrik Operator <pre><code>oc apply -f https://path/to/rubrik-operator.yaml\n</code></pre></p> </li> <li> <p>Verify Backup <pre><code>oc get pods -n rubrik\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#gitops-setup","title":"GitOps Setup","text":""},{"location":"deployment/installation/#argo-cd-installation","title":"Argo CD Installation","text":"<ol> <li> <p>Install Argo CD <pre><code>oc apply -n argocd -f https://path/to/argocd-install.yaml\n</code></pre></p> </li> <li> <p>Access Argo CD UI</p> </li> <li> <p>Forward Argo CD API server port:      <pre><code>oc port-forward svc/argocd-server -n argocd 8080:443\n</code></pre></p> </li> <li> <p>Login to Argo CD</p> </li> <li> <p>Open https://localhost:8080 in your browser.</p> </li> <li> <p>Deploy Applications <pre><code>argocd app create my-app --repo https://git.example.com/my-app --path ./\nargocd app sync my-app\n</code></pre></p> </li> </ol>"},{"location":"deployment/installation/#security-hardening","title":"Security Hardening","text":"<ol> <li> <p>Configure RBAC <pre><code>oc apply -f /path/to/rbac-config.yaml\n</code></pre></p> </li> <li> <p>Enable Pod Security <pre><code>oc apply -f /path/to/pod-security.yaml\n</code></pre></p> </li> <li> <p>Firewall Adjustments</p> </li> <li>Ensure only necessary ports are open (refer to prerequisites).</li> </ol>"},{"location":"deployment/installation/#validation-steps","title":"Validation Steps","text":""},{"location":"deployment/installation/#verify-all-deployments","title":"Verify All Deployments","text":"<pre><code>oc get all --all-namespaces\n</code></pre>"},{"location":"deployment/installation/#check-monitoring-dashboards","title":"Check Monitoring Dashboards","text":"<ul> <li>Confirm metrics collection in Grafana and Dynatrace.</li> </ul>"},{"location":"deployment/installation/#post-installation-tasks","title":"Post-Installation Tasks","text":""},{"location":"deployment/installation/#documentation","title":"Documentation","text":"<ul> <li>Update MkDocs with new components.</li> </ul>"},{"location":"deployment/installation/#backup-verification","title":"Backup Verification","text":"<ul> <li>Test Rubrik backups for VM and container data.</li> </ul>"},{"location":"deployment/installation/#conclusion","title":"Conclusion","text":"<p>This guide ensures a smooth installation process for RH OVE, covering all critical steps and components necessary for successful deployment and operation. Follow each section carefully to complete the installation.</p>"},{"location":"deployment/prerequisites/","title":"Prerequisites","text":""},{"location":"deployment/prerequisites/#overview","title":"Overview","text":"<p>This document outlines the prerequisites for deploying the RH OVE ecosystem, including infrastructure requirements, software dependencies, and configuration prerequisites.</p>"},{"location":"deployment/prerequisites/#infrastructure-requirements","title":"Infrastructure Requirements","text":""},{"location":"deployment/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<pre><code>graph TB\n    subgraph \"Master Nodes (3 minimum)\"\n        A[CPU: 4+ cores per node]\n        B[Memory: 16GB+ per node]\n        C[Storage: 120GB+ per node]\n        D[Network: 1Gbps+]\n    end\n\n    subgraph \"Worker Nodes (3+ minimum)\"\n        E[CPU: 8+ cores per node]\n        F[Memory: 32GB+ per node]\n        G[Storage: 500GB+ per node]\n        H[Network: 10Gbps+]\n        I[Virtualization: Intel VT-x/AMD-V]\n    end\n\n    subgraph \"Storage Backend\"\n        J[High-performance SSD]\n        K[Network-attached storage]\n        L[Block storage support]\n    end</code></pre>"},{"location":"deployment/prerequisites/#virtualization-support","title":"Virtualization Support","text":"<p>Ensure hardware virtualization is enabled:</p> <pre><code># Check for Intel VT-x\ngrep -E \"(vmx|svm)\" /proc/cpuinfo\n\n# Check if virtualization is enabled in BIOS\nlscpu | grep Virtualization\n\n# Verify KVM modules are loaded\nlsmod | grep kvm\n</code></pre>"},{"location":"deployment/prerequisites/#network-requirements","title":"Network Requirements","text":"<ul> <li>Cluster Network: Internal cluster communication</li> <li>Service Network: Service-to-service communication  </li> <li>Pod Network: Pod-to-pod communication</li> <li>External Access: Load balancer and ingress traffic</li> </ul> <pre><code># Network configuration example\ncluster_network:\n  cidr: \"10.128.0.0/14\"\n  host_prefix: 23\n\nservice_network:\n  - \"172.30.0.0/16\"\n\nmachine_networks:\n  - cidr: \"192.168.1.0/24\"\n</code></pre>"},{"location":"deployment/prerequisites/#software-prerequisites","title":"Software Prerequisites","text":""},{"location":"deployment/prerequisites/#openshift-container-platform","title":"OpenShift Container Platform","text":"<ul> <li>Version: OpenShift 4.12+ (recommended 4.14+)</li> <li>Installation Method: IPI (Installer Provisioned Infrastructure) or UPI (User Provisioned Infrastructure)</li> <li>Cluster Admin Access: Required for operator installation</li> </ul>"},{"location":"deployment/prerequisites/#required-operators","title":"Required Operators","text":"<pre><code>graph TB\n    subgraph \"Core Operators\"\n        A[OpenShift Virtualization Operator]\n        B[Red Hat OpenShift GitOps]\n        C[Web Terminal Operator]\n    end\n\n    subgraph \"Networking\"\n        D[Cilium CNI Operator]\n        E[Multus CNI]\n    end\n\n    subgraph \"Security &amp; Policy\"\n        F[Kyverno Operator]\n        G[Compliance Operator]\n    end\n\n    subgraph \"Monitoring &amp; Observability\"\n        H[Dynatrace Operator]\n        I[Prometheus Operator]\n    end\n\n    subgraph \"Backup &amp; Storage\"\n        J[Rubrik Operator]\n        K[CSI Operators]\n    end</code></pre>"},{"location":"deployment/prerequisites/#storage-requirements","title":"Storage Requirements","text":""},{"location":"deployment/prerequisites/#container-storage-interface-csi-drivers","title":"Container Storage Interface (CSI) Drivers","text":"<pre><code># Example CSI StorageClass\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rh-ove-ssd\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  encrypted: \"true\"\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\n</code></pre>"},{"location":"deployment/prerequisites/#storage-classes-required","title":"Storage Classes Required","text":"<ul> <li>Fast SSD: For VM boot disks and high-IOPS workloads</li> <li>Standard HDD: For data storage and backup</li> <li>Archive: For long-term storage and compliance</li> </ul>"},{"location":"deployment/prerequisites/#network-prerequisites","title":"Network Prerequisites","text":""},{"location":"deployment/prerequisites/#dns-configuration","title":"DNS Configuration","text":"<pre><code># DNS configuration for cluster\ndns:\n  base_domain: \"ove.example.com\"\n  cluster_domain: \"cluster.local\"\nmetadata:\n  name: \"rh-ove-cluster\"\n</code></pre>"},{"location":"deployment/prerequisites/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<pre><code>graph LR\n    subgraph \"External Load Balancer\"\n        A[API Load Balancer]\n        B[Ingress Load Balancer]\n    end\n\n    subgraph \"OpenShift Cluster\"\n        C[Master Nodes]\n        D[Worker Nodes]\n        E[Ingress Controllers]\n    end\n\n    A --&gt; C\n    B --&gt; E\n    E --&gt; D</code></pre>"},{"location":"deployment/prerequisites/#firewall-rules","title":"Firewall Rules","text":"<p>Required ports for RH OVE:</p> Port Range Protocol Purpose 6443 TCP Kubernetes API server 22623 TCP Machine config server 80/443 TCP HTTP/HTTPS ingress 9000-9999 TCP Host level services 10250-10259 TCP Kubernetes node ports 30000-32767 TCP NodePort services"},{"location":"deployment/prerequisites/#security-prerequisites","title":"Security Prerequisites","text":""},{"location":"deployment/prerequisites/#certificate-management","title":"Certificate Management","text":"<pre><code># TLS certificate configuration\ntls:\n  ca_cert: |\n    -----BEGIN CERTIFICATE-----\n    # CA certificate content\n    -----END CERTIFICATE-----\n\n  api_cert: |\n    -----BEGIN CERTIFICATE-----\n    # API server certificate\n    -----END CERTIFICATE-----\n</code></pre>"},{"location":"deployment/prerequisites/#rbac-configuration","title":"RBAC Configuration","text":"<p>Prepare service accounts and roles:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rh-ove-admin\n  namespace: openshift-cnv\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: rh-ove-admin-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: rh-ove-admin\n  namespace: openshift-cnv\n</code></pre>"},{"location":"deployment/prerequisites/#external-system-prerequisites","title":"External System Prerequisites","text":""},{"location":"deployment/prerequisites/#git-repository-setup","title":"Git Repository Setup","text":"<p>For GitOps implementation:</p> <pre><code># Create GitOps repository structure\nmkdir -p rh-ove-gitops/{applications,infrastructure,policies}\n\n# Initialize Git repository\ncd rh-ove-gitops\ngit init\ngit remote add origin https://git.example.com/rh-ove-gitops.git\n</code></pre>"},{"location":"deployment/prerequisites/#rubrik-backup-system","title":"Rubrik Backup System","text":"<p>Prerequisites for Rubrik integration:</p> <ul> <li>Rubrik cluster: Version 5.0+</li> <li>Network connectivity: Cluster to Rubrik management network</li> <li>Service account: With backup and restore permissions</li> <li>API access: Rubrik REST API credentials</li> </ul> <pre><code># Rubrik connection configuration\napiVersion: v1\nkind: Secret\nmetadata:\n  name: rubrik-credentials\n  namespace: rubrik-system\ntype: Opaque\nstringData:\n  username: \"rubrik-service-account\"\n  password: \"secure-password\"\n  cluster-address: \"rubrik.example.com\"\n</code></pre>"},{"location":"deployment/prerequisites/#dynatrace-monitoring","title":"Dynatrace Monitoring","text":"<p>Prerequisites for Dynatrace integration:</p> <ul> <li>Dynatrace tenant: SaaS or Managed</li> <li>API tokens: With required permissions</li> <li>Network access: Cluster to Dynatrace endpoints</li> </ul> <pre><code># Dynatrace API token secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dynakube\n  namespace: dynatrace\ntype: Opaque\nstringData:\n  apiToken: \"dt0c01.xxxxx\"\n  dataIngestToken: \"dt0c01.yyyyy\"\n</code></pre>"},{"location":"deployment/prerequisites/#servicenow-cmdb","title":"ServiceNow CMDB","text":"<p>For CMDB integration:</p> <ul> <li>ServiceNow instance: With CMDB module</li> <li>Service account: With CMDB read/write permissions</li> <li>API access: REST API and webhooks configured</li> </ul>"},{"location":"deployment/prerequisites/#validation-checklist","title":"Validation Checklist","text":""},{"location":"deployment/prerequisites/#pre-installation-checks","title":"Pre-Installation Checks","text":"<pre><code># Validation script example\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pre-install-checks\ndata:\n  validate.sh: |\n    #!/bin/bash\n\n    # Check OpenShift version\n    oc version\n\n    # Verify cluster resources\n    oc get nodes\n    oc get storageclass\n\n    # Check virtualization support\n    oc get nodes -o json | jq '.items[].status.allocatable'\n\n    # Validate network connectivity\n    curl -k https://registry.redhat.io/health\n</code></pre>"},{"location":"deployment/prerequisites/#resource-verification","title":"Resource Verification","text":"<pre><code># Check available resources\noc adm top nodes\n\n# Verify storage classes\noc get storageclass\n\n# Check network plugins\noc get network.config/cluster -o yaml\n\n# Validate image registry access\noc get imagestreams -n openshift\n</code></pre>"},{"location":"deployment/prerequisites/#installation-timeline","title":"Installation Timeline","text":"<pre><code>gantt\n    title RH OVE Deployment Timeline\n    dateFormat  YYYY-MM-DD\n    section Infrastructure\n    Hardware Setup           :done, infra1, 2024-01-01, 2024-01-07\n    OpenShift Installation   :done, infra2, 2024-01-08, 2024-01-14\n\n    section Core Components\n    Virtualization Operator  :active, core1, 2024-01-15, 2024-01-21\n    Cilium CNI               :core2, 2024-01-22, 2024-01-28\n\n    section Security &amp; Policy\n    Kyverno Installation     :policy1, 2024-01-29, 2024-02-04\n    Security Policies        :policy2, 2024-02-05, 2024-02-11\n\n    section Monitoring\n    Dynatrace Setup          :monitor1, 2024-02-12, 2024-02-18\n    Backup Configuration     :backup1, 2024-02-19, 2024-02-25\n\n    section GitOps\n    Argo CD Setup           :gitops1, 2024-02-26, 2024-03-04\n    Application Deployment  :gitops2, 2024-03-05, 2024-03-11</code></pre> <p>This comprehensive prerequisites guide ensures all necessary components and configurations are in place before beginning the RH OVE deployment process.</p>"},{"location":"management/admission-control/","title":"Admission Control Strategy","text":""},{"location":"management/admission-control/#overview","title":"Overview","text":"<p>The RH OVE solution implements a comprehensive admission control strategy that combines OpenShift's built-in controllers with KubeVirt-specific webhooks and Kyverno policy engine for enhanced security and governance.</p>"},{"location":"management/admission-control/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    A[API Request] --&gt; B{Admission Control Pipeline}\n    B --&gt; C[OpenShift Built-in Controllers]\n    B --&gt; D[KubeVirt Webhooks]\n    B --&gt; E[Kyverno Policy Engine]\n\n    C --&gt; F[Mutating Admission]\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt; G[Validating Admission]\n    C --&gt; G\n    D --&gt; G\n    E --&gt; G\n\n    G --&gt; H[etcd Storage]\n    G --&gt; I[Denied Request]</code></pre>"},{"location":"management/admission-control/#default-openshift-admission-controllers","title":"Default OpenShift Admission Controllers","text":"<p>RH OVE inherits all standard OpenShift admission controllers including:</p> <ul> <li>LimitRanger: Enforces resource limits and requests</li> <li>ServiceAccount: Manages service account tokens</li> <li>PodNodeSelector: Controls node placement</li> <li>SecurityContextConstraint (SCC): Enforces security policies</li> <li>ResourceQuota: Manages namespace-level resource limits</li> <li>MutatingAdmissionWebhook: Allows custom mutation logic</li> <li>ValidatingAdmissionWebhook: Allows custom validation logic</li> </ul>"},{"location":"management/admission-control/#kubevirt-specific-admission-webhooks","title":"KubeVirt-Specific Admission Webhooks","text":"<p>KubeVirt automatically registers webhooks for virtualization resources:</p> <ul> <li>VirtualMachine validation: Ensures VM configurations are valid</li> <li>VirtualMachineInstance validation: Validates running VM instances</li> <li>DataVolume validation: Verifies storage configurations</li> <li>Migration validation: Checks migration prerequisites</li> </ul>"},{"location":"management/admission-control/#kyverno-policy-engine-integration","title":"Kyverno Policy Engine Integration","text":""},{"location":"management/admission-control/#installation","title":"Installation","text":"<p>Deploy Kyverno via Helm or manifests:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: kyverno\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno\n  namespace: kyverno\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kyverno\n  template:\n    metadata:\n      labels:\n        app: kyverno\n    spec:\n      containers:\n      - name: kyverno\n        image: ghcr.io/kyverno/kyverno:latest\n</code></pre>"},{"location":"management/admission-control/#vm-specific-policies","title":"VM-Specific Policies","text":""},{"location":"management/admission-control/#resource-limits-policy","title":"Resource Limits Policy","text":"<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: vm-resource-limits\nspec:\n  validationFailureAction: enforce\n  background: true\n  rules:\n  - name: require-vm-resource-limits\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n    validate:\n      message: \"VirtualMachine must have CPU and memory limits defined\"\n      pattern:\n        spec:\n          template:\n            spec:\n              domain:\n                cpu:\n                  cores: \"&gt;0\"\n                memory:\n                  guest: \"&gt;0\"\n</code></pre>"},{"location":"management/admission-control/#vm-security-policy","title":"VM Security Policy","text":"<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: vm-security-policy\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: disallow-privileged-vms\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n    validate:\n      message: \"Privileged VMs are not allowed\"\n      pattern:\n        spec:\n          template:\n            spec:\n              domain:\n                features:\n                  smm:\n                    enabled: \"false\"\n</code></pre>"},{"location":"management/admission-control/#namespace-isolation-policy","title":"Namespace Isolation Policy","text":"<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: vm-namespace-isolation\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: require-namespace-labels\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n    validate:\n      message: \"VirtualMachines must be in properly labeled namespaces\"\n      pattern:\n        metadata:\n          namespace: \"!default\"\n</code></pre>"},{"location":"management/admission-control/#multi-tenant-policies","title":"Multi-Tenant Policies","text":""},{"location":"management/admission-control/#application-based-namespace-policy","title":"Application-Based Namespace Policy","text":"<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: application-namespace-policy\nspec:\n  validationFailureAction: enforce\n  rules:\n  - name: enforce-app-namespace-pattern\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n          - VirtualMachineInstance\n    validate:\n      message: \"VMs must be deployed in application-specific namespaces\"\n      pattern:\n        metadata:\n          namespace: \"app-*\"\n</code></pre>"},{"location":"management/admission-control/#policy-enforcement-flow","title":"Policy Enforcement Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant API as OpenShift API\n    participant OSA as OpenShift Admission\n    participant KVA as KubeVirt Admission\n    participant KYV as Kyverno\n    participant etcd\n\n    User-&gt;&gt;API: Create VirtualMachine\n    API-&gt;&gt;OSA: Mutating Admission\n    OSA-&gt;&gt;KVA: KubeVirt Mutations\n    KVA-&gt;&gt;KYV: Kyverno Mutations\n    KYV-&gt;&gt;OSA: Validating Admission\n    OSA-&gt;&gt;KVA: KubeVirt Validation\n    KVA-&gt;&gt;KYV: Kyverno Validation\n    KYV-&gt;&gt;etcd: Store Resource\n    etcd-&gt;&gt;User: Success Response</code></pre>"},{"location":"management/admission-control/#best-practices","title":"Best Practices","text":""},{"location":"management/admission-control/#policy-development","title":"Policy Development","text":"<ol> <li>Start with monitoring mode: Use <code>validationFailureAction: audit</code> initially</li> <li>Test thoroughly: Validate policies in non-production environments</li> <li>Use exceptions sparingly: Avoid broad policy exceptions</li> <li>Version control: Store policies in Git repositories</li> </ol>"},{"location":"management/admission-control/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Optimize policy matching: Use specific resource selectors</li> <li>Minimize policy overlap: Avoid redundant validation rules</li> <li>Monitor admission latency: Track policy evaluation performance</li> <li>Use background processing: Enable background validation where appropriate</li> </ol>"},{"location":"management/admission-control/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"management/admission-control/#policy-violations-dashboard","title":"Policy Violations Dashboard","text":"<p>Monitor policy violations using Prometheus metrics:</p> <pre><code>apiVersion: v1\nkind: ServiceMonitor\nmetadata:\n  name: kyverno-metrics\nspec:\n  selector:\n    matchLabels:\n      app: kyverno\n  endpoints:\n  - port: metrics\n</code></pre>"},{"location":"management/admission-control/#common-troubleshooting","title":"Common Troubleshooting","text":"<ol> <li>Policy not applying: Check policy syntax and matching criteria</li> <li>Performance issues: Review policy complexity and scope</li> <li>Conflicts: Examine interaction between different admission controllers</li> <li>Debugging: Use <code>kubectl describe</code> to view admission controller events</li> </ol>"},{"location":"management/admission-control/#integration-with-gitops","title":"Integration with GitOps","text":"<p>Store all policies in Git and deploy via Argo CD:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: kyverno-policies\nspec:\n  source:\n    repoURL: https://git.example.com/rh-ove-policies\n    path: policies/\n    targetRevision: main\n  destination:\n    server: https://kubernetes.default.svc\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>This comprehensive admission control strategy ensures that all workloads, both containers and VMs, comply with organizational security and operational policies while maintaining the flexibility needed for modern application development.</p>"},{"location":"management/backup/","title":"Backup &amp; Recovery","text":""},{"location":"management/backup/#overview","title":"Overview","text":"<p>This document outlines the backup and recovery strategies for the RH OVE ecosystem. It highlights the integration with Rubrik, detailing how to efficiently back up and restore both VM and container data.</p>"},{"location":"management/backup/#backup-strategy","title":"Backup Strategy","text":""},{"location":"management/backup/#rubrik-integration","title":"Rubrik Integration","text":"<p>Utilize Rubrik's capabilities to ensure robust data protection:</p> <ul> <li>Certified Integration: Rubrik is certified for RH OVE, providing seamless data protection.</li> <li>Immutable Backups: Ensure data safety with air-gapped, tamper-proof backups.</li> <li>Policy-Driven: Simplify backup management with declarative policies for VM workloads.</li> </ul>"},{"location":"management/backup/#backup-configuration","title":"Backup Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rubrik-backup-config\n  namespace: rubrik-system\ndata:\n  backupPolicy.yaml: |\n    policies:\n      - name: daily-VM-backup\n        frequency: daily\n        retention: 30d\n        exclude: 'temp-volumes'\n\n    schedules:\n      - name: nightly-backup\n        time: '02:00'\n        days:\n          - Monday\n          - Wednesday\n          - Friday\n</code></pre>"},{"location":"management/backup/#data-volume-backup","title":"Data Volume Backup","text":"<p>Backup specific DataVolumes using Rubrik advanced features:</p> <pre><code>apiVersion: rubrik.com/v1\nkind: DataProtectionPolicy\nmetadata:\n  name: data-volume-backup\nspec:\n  dataprotection:\n    enable: true\n  rubrikCluster:\n    name: rubrik-cluster1\n  snapshot:\n    schedule: nightly\n    retention: 31\ndatavolume:\n  selector:\n    matchLabels:\n      app: production\n</code></pre>"},{"location":"management/backup/#recovery-strategy","title":"Recovery Strategy","text":""},{"location":"management/backup/#rubrik-recovery","title":"Rubrik Recovery","text":"<p>Rubrik\u2019s high-speed recovery ensures minimal downtime for critical workloads:</p> <ol> <li>Instant Restore: Quickly recover VMs from snapshots directly on RH OVE.</li> <li>File-Level Restore: Execute rapid recovery at the file level for broad access needs.</li> <li>Automated Recovery Paths: Simplify recovery workflows through automation.</li> </ol>"},{"location":"management/backup/#recovery-plan","title":"Recovery Plan","text":"<p>Define a detailed recovery plan to access Rubrik\u2019s capability.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: recovery-plan\n  namespace: recovery\nspec:\n  paths:\n    critical-apps:\n    - name: app1\n      vm: app1-vm\n      backup: latest\n      action: restore\n    - name: app2\n      vm: app2-vm\n      backup: nightly\n      action: restore\n  hooks:\n    preRestore:\n      - /scripts/pre-restore.sh\n    postRestore:\n      - /scripts/post-restore.sh\n</code></pre>"},{"location":"management/backup/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"management/backup/#backup-verification","title":"Backup Verification","text":"<p>Regularly test backups to ensure integrity and reliability:</p> <ul> <li>Backup Verification Schedules: Conduct routine checks on backup snapshots for quality assurance.</li> <li>Periodic Restore Drills: Simulate restore scenarios to assess recovery time objectives.</li> </ul>"},{"location":"management/backup/#recovery-assurance","title":"Recovery Assurance","text":"<p>Ensure recovery processes are validated and documented:</p> <ul> <li>Recovery Testing: Periodically execute recovery processes within a non-production environment.</li> <li>Documentation: Maintain up-to-date recovery documentation with steps, tools, and responsible parties.</li> </ul>"},{"location":"management/backup/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<p>Utilize monitoring tools to track backup and restore activities:</p> <ul> <li>Alerting Policies: Implement alerts for failed backups, missed schedules, or data integrity issues.</li> <li>Monitoring Dashboards: Use dashboards to visualize backup/recovery activities and efficiency metrics.</li> </ul>"},{"location":"management/backup/#conclusion","title":"Conclusion","text":"<p>By following these backup and recovery strategies, organizations can ensure the safety, integrity, and availability of their critical data within the RH OVE ecosystem. Taking advantage of Rubrik\u2019s robust integration further enhances data resilience, minimizing risks associated with data loss.</p>"},{"location":"management/gitops/","title":"GitOps Operations","text":""},{"location":"management/gitops/#overview","title":"Overview","text":"<p>This document outlines the GitOps approach for managing the multi-cluster RH OVE ecosystem, providing guidelines for implementing Infrastructure as Code (IaC) and application deployment through Git-based workflows across management and application clusters.</p>"},{"location":"management/gitops/#multi-cluster-gitops-principles","title":"Multi-Cluster GitOps Principles","text":"<p>The RH OVE ecosystem implements GitOps across a hub-and-spoke architecture:</p> <ul> <li>Centralized Control: ArgoCD Hub manages deployments to multiple application clusters</li> <li>Single Source of Truth: All cluster and VM configurations stored in Git repositories</li> <li>Declarative Management: Infrastructure as Code for clusters, VMs, and containers</li> <li>Multi-Environment Support: Separate overlays for production, staging, and development clusters</li> <li>Policy Distribution: Centralized policy management with cluster-specific enforcement</li> <li>Automated Rollbacks: Complete change tracking and rollback capabilities across clusters</li> </ul>"},{"location":"management/gitops/#multi-cluster-architecture","title":"Multi-Cluster Architecture","text":"<pre><code>graph TB\n    subgraph \"Git Repository\"\n        A[Application Manifests]\n        B[VM Templates]\n        C[Infrastructure Config]\n        D[Policies]\n    end\n\n    subgraph \"GitOps Engine\"\n        E[Argo CD]\n        F[Application Controller]\n        G[Sync Engine]\n    end\n\n    subgraph \"RH OVE Cluster\"\n        H[Virtual Machines]\n        I[Container Workloads]\n        J[Storage Resources]\n        K[Network Policies]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    G --&gt; I\n    G --&gt; J\n    G --&gt; K</code></pre>"},{"location":"management/gitops/#repository-structure","title":"Repository Structure","text":""},{"location":"management/gitops/#recommended-git-repository-layout","title":"Recommended Git Repository Layout","text":"<pre><code>rh-ove-gitops/\n\u251c\u2500\u2500 applications/\n\u2502   \u251c\u2500\u2500 web-app/\n\u2502   \u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 service.yaml\n\u2502   \u2502   \u2514\u2500\u2500 overlays/\n\u2502   \u2502       \u251c\u2500\u2500 dev/\n\u2502   \u2502       \u251c\u2500\u2500 staging/\n\u2502   \u2502       \u2514\u2500\u2500 prod/\n\u2502   \u2514\u2500\u2500 database-vm/\n\u2502       \u251c\u2500\u2500 vm-definition.yaml\n\u2502       \u251c\u2500\u2500 datavolume.yaml\n\u2502       \u2514\u2500\u2500 service.yaml\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 storage-classes/\n\u2502   \u251c\u2500\u2500 network-policies/\n\u2502   \u2514\u2500\u2500 rbac/\n\u251c\u2500\u2500 vm-templates/\n\u2502   \u251c\u2500\u2500 rhel8-template.yaml\n\u2502   \u251c\u2500\u2500 windows-template.yaml\n\u2502   \u2514\u2500\u2500 ubuntu-template.yaml\n\u2514\u2500\u2500 policies/\n    \u251c\u2500\u2500 kyverno/\n    \u2514\u2500\u2500 gatekeeper/\n</code></pre>"},{"location":"management/gitops/#argo-cd-configuration","title":"Argo CD Configuration","text":""},{"location":"management/gitops/#application-definition","title":"Application Definition","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: database-vm-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://git.example.com/rh-ove-gitops\n    targetRevision: main\n    path: applications/database-vm\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: app-database-prod\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"management/gitops/#appproject-for-vm-workloads","title":"AppProject for VM Workloads","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: vm-workloads\n  namespace: argocd\nspec:\n  description: Project for VM-based applications\n  sourceRepos:\n  - https://git.example.com/rh-ove-gitops\n  destinations:\n  - namespace: 'app-*'\n    server: https://kubernetes.default.svc\n  clusterResourceWhitelist:\n  - group: ''\n    kind: Namespace\n  - group: 'kubevirt.io'\n    kind: VirtualMachine\n  - group: 'cdi.kubevirt.io'\n    kind: DataVolume\n  namespaceResourceWhitelist:\n  - group: ''\n    kind: Service\n  - group: ''\n    kind: ConfigMap\n  - group: ''\n    kind: Secret\n</code></pre>"},{"location":"management/gitops/#vm-management-through-gitops","title":"VM Management through GitOps","text":""},{"location":"management/gitops/#virtual-machine-definition","title":"Virtual Machine Definition","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: web-server-vm\n  namespace: app-web-prod\n  labels:\n    app: web-server\n    managed-by: argocd\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        app: web-server\n        version: v1.2.0\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 4Gi\n        devices:\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n          - name: datadisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: web-server-root\n      - name: datadisk\n        dataVolume:\n          name: web-server-data\n</code></pre>"},{"location":"management/gitops/#datavolume-configuration","title":"DataVolume Configuration","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: web-server-root\n  namespace: app-web-prod\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 30Gi\n    storageClassName: fast-ssd\n  source:\n    registry:\n      url: \"docker://registry.redhat.io/rhel8/rhel:latest\"\n---\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: web-server-data\n  namespace: app-web-prod\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 100Gi\n    storageClassName: standard-hdd\n  source:\n    blank: {}\n</code></pre>"},{"location":"management/gitops/#multi-environment-management","title":"Multi-Environment Management","text":""},{"location":"management/gitops/#environment-specific-overlays","title":"Environment-Specific Overlays","text":"<pre><code># overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n- ../../base\n\npatchesStrategicMerge:\n- vm-resources.yaml\n- storage-config.yaml\n\npatches:\n- target:\n    kind: VirtualMachine\n    name: web-server-vm\n  patch: |-\n    - op: replace\n      path: /spec/template/spec/domain/cpu/cores\n      value: 4\n    - op: replace\n      path: /spec/template/spec/domain/memory/guest\n      value: 8Gi\n</code></pre>"},{"location":"management/gitops/#environment-promotion-workflow","title":"Environment Promotion Workflow","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant Git as Git Repository\n    participant ArgoCD as Argo CD\n    participant DevCluster as Dev Cluster\n    participant ProdCluster as Prod Cluster\n\n    Dev-&gt;&gt;Git: Push changes to dev branch\n    Git-&gt;&gt;ArgoCD: Webhook triggers sync\n    ArgoCD-&gt;&gt;DevCluster: Deploy to dev environment\n    Dev-&gt;&gt;Git: Create PR to main branch\n    Git-&gt;&gt;ArgoCD: Merge triggers prod sync\n    ArgoCD-&gt;&gt;ProdCluster: Deploy to production</code></pre>"},{"location":"management/gitops/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"management/gitops/#gitlab-ci-pipeline","title":"GitLab CI Pipeline","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - validate\n  - test\n  - deploy\n\nvalidate-manifests:\n  stage: validate\n  script:\n    - kubeval manifests/*.yaml\n    - kustomize build overlays/dev | kubeval\n\nvm-integration-test:\n  stage: test\n  script:\n    - kubectl apply --dry-run=client -f vm-definitions/\n    - virtctl validate vm-definitions/\n\ndeploy-to-dev:\n  stage: deploy\n  script:\n    - argocd app sync dev-environment\n  only:\n    - develop\n\ndeploy-to-prod:\n  stage: deploy\n  script:\n    - argocd app sync prod-environment\n  only:\n    - main\n</code></pre>"},{"location":"management/gitops/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"management/gitops/#policy-as-code","title":"Policy as Code","text":"<pre><code># policies/kyverno/vm-security-policy.yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: vm-security-standards\nspec:\n  validationFailureAction: enforce\n  background: true\n  rules:\n  - name: require-vm-labels\n    match:\n      any:\n      - resources:\n          kinds:\n          - VirtualMachine\n    validate:\n      message: \"VMs must have required labels\"\n      pattern:\n        metadata:\n          labels:\n            app: \"?*\"\n            version: \"?*\"\n            managed-by: \"argocd\"\n</code></pre>"},{"location":"management/gitops/#rbac-for-gitops","title":"RBAC for GitOps","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: argocd-application-controller\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n- apiGroups:\n  - \"kubevirt.io\"\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n- apiGroups:\n  - \"cdi.kubevirt.io\"\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n</code></pre>"},{"location":"management/gitops/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"management/gitops/#gitops-metrics","title":"GitOps Metrics","text":"<p>Key metrics to monitor: - Sync success rate - Deployment frequency - Mean time to recovery - Application health status</p>"},{"location":"management/gitops/#dashboard-configuration","title":"Dashboard Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  url: https://argocd.example.com\n  statusbadge.enabled: \"true\"\n  application.instanceLabelKey: argocd.argoproj.io/instance\n</code></pre>"},{"location":"management/gitops/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"management/gitops/#backup-strategy","title":"Backup Strategy","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: backup-config\ndata:\n  backup-strategy.yaml: |\n    applications:\n      - name: critical-vms\n        backup_frequency: \"daily\"\n        retention: \"30d\"\n        git_ref: \"backup/$(date +%Y%m%d)\"\n\n    infrastructure:\n      - name: cluster-config\n        backup_frequency: \"weekly\"\n        retention: \"12w\"\n</code></pre>"},{"location":"management/gitops/#recovery-procedures","title":"Recovery Procedures","text":"<ol> <li> <p>Application Recovery <pre><code># Restore from specific commit\nargocd app set myapp --revision abc123\nargocd app sync myapp\n</code></pre></p> </li> <li> <p>Full Environment Recovery <pre><code># Deploy entire environment from Git\nargocd app create disaster-recovery \\\n  --repo https://git.example.com/rh-ove-gitops \\\n  --path recovery/full-environment \\\n  --dest-server https://kubernetes.default.svc\n</code></pre></p> </li> </ol>"},{"location":"management/gitops/#best-practices","title":"Best Practices","text":""},{"location":"management/gitops/#development-workflow","title":"Development Workflow","text":"<ol> <li>Feature branches: Use feature branches for new VM deployments</li> <li>Pull requests: Require peer review for all changes</li> <li>Automated testing: Validate manifests before merge</li> <li>Progressive deployment: Use staging environments before production</li> </ol>"},{"location":"management/gitops/#operational-guidelines","title":"Operational Guidelines","text":"<ol> <li>Secrets management: Use external secret management (e.g., Vault)</li> <li>Resource limits: Define appropriate CPU/memory limits for VMs</li> <li>Monitoring: Implement comprehensive monitoring for all deployments</li> <li>Documentation: Keep README files updated in each application directory</li> </ol> <p>This GitOps approach ensures consistent, auditable, and automated management of VM and container workloads in the RH OVE environment.</p>"},{"location":"management/monitoring/","title":"Monitoring and Observability","text":""},{"location":"management/monitoring/#overview","title":"Overview","text":"<p>This document provides comprehensive monitoring and observability strategies for the RH OVE ecosystem, covering infrastructure, virtual machines, containers, and application performance monitoring using Dynatrace and other monitoring tools.</p>"},{"location":"management/monitoring/#monitoring-architecture","title":"Monitoring Architecture","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        A[Virtual Machines]\n        B[Container Workloads]\n        C[OpenShift Platform]\n        D[Cilium Network]\n        E[Storage Systems]\n    end\n\n    subgraph \"Collection Layer\"\n        F[Dynatrace OneAgent]\n        G[Prometheus]\n        H[Node Exporter]\n        I[Hubble]\n        J[QEMU Guest Agent]\n    end\n\n    subgraph \"Processing &amp; Storage\"\n        K[Dynatrace Platform]\n        L[Prometheus Server]\n        M[Alert Manager]\n    end\n\n    subgraph \"Visualization &amp; Alerting\"\n        N[Dynatrace Dashboard]\n        O[Grafana]\n        P[OpenShift Console]\n        Q[Alert Notifications]\n    end\n\n    A --&gt; F\n    A --&gt; J\n    B --&gt; F\n    C --&gt; G\n    C --&gt; H\n    D --&gt; I\n    E --&gt; G\n\n    F --&gt; K\n    G --&gt; L\n    H --&gt; L\n    I --&gt; L\n    J --&gt; G\n\n    K --&gt; N\n    L --&gt; O\n    L --&gt; M\n    M --&gt; Q\n    N --&gt; Q</code></pre>"},{"location":"management/monitoring/#dynatrace-integration","title":"Dynatrace Integration","text":"<p>Based on our research, integrating RH OVE monitoring stack with Dynatrace provides comprehensive visibility for VMs and Kubernetes workloads.</p>"},{"location":"management/monitoring/#dynatrace-operator-installation","title":"Dynatrace Operator Installation","text":"<pre><code>apiVersion: dynatrace.com/v1beta1\nkind: DynaKube\nmetadata:\n  name: dynakube\n  namespace: dynatrace\nspec:\n  apiUrl: https://your-environment-id.live.dynatrace.com/api\n  oneAgent:\n    classicFullStack:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      resources:\n        requests:\n          cpu: 100m\n          memory: 512Mi\n        limits:\n          cpu: 300m\n          memory: 1Gi\n  activeGate:\n    capabilities:\n    - kubernetes-monitoring\n    - routing\n    resources:\n      requests:\n        cpu: 150m\n        memory: 512Mi\n      limits:\n        cpu: 500m\n        memory: 1Gi\n</code></pre>"},{"location":"management/monitoring/#vm-specific-monitoring-configuration","title":"VM-Specific Monitoring Configuration","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: monitored-vm\n  namespace: app-prod\n  annotations:\n    dynatrace.com/inject: \"true\"\n    dynatrace.com/vm-monitoring: \"enabled\"\nspec:\n  template:\n    metadata:\n      labels:\n        app: web-server\n        monitoring: enabled\n    spec:\n      domain:\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n        resources:\n          requests:\n            memory: 4Gi\n            cpu: 2\n      volumes:\n      - name: qemu-guest-agent\n        serviceAccount:\n          serviceAccountName: qemu-guest-agent\n</code></pre>"},{"location":"management/monitoring/#prometheus-configuration","title":"Prometheus Configuration","text":""},{"location":"management/monitoring/#servicemonitor-for-vm-metrics","title":"ServiceMonitor for VM Metrics","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: vm-metrics\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: kubevirt-prometheus-metrics\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n</code></pre>"},{"location":"management/monitoring/#custom-metrics-for-vms","title":"Custom Metrics for VMs","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: vm-monitoring-rules\n  namespace: monitoring\nspec:\n  groups:\n  - name: vm.rules\n    rules:\n    - alert: VMHighCPUUsage\n      expr: kubevirt_vm_cpu_usage_percentage &gt; 80\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} has high CPU usage\"\n        description: \"VM {{ $labels.name }} in namespace {{ $labels.namespace }} has CPU usage above 80% for more than 5 minutes.\"\n\n    - alert: VMHighMemoryUsage\n      expr: kubevirt_vm_memory_usage_percentage &gt; 85\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} has high memory usage\"\n        description: \"VM {{ $labels.name }} in namespace {{ $labels.namespace }} has memory usage above 85% for more than 5 minutes.\"\n</code></pre>"},{"location":"management/monitoring/#network-monitoring-with-hubble","title":"Network Monitoring with Hubble","text":""},{"location":"management/monitoring/#hubble-configuration","title":"Hubble Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-hubble: \"true\"\n  hubble-listen-address: \":4244\"\n  hubble-metrics-server: \":9091\"\n  hubble-metrics: |\n    dns:query;ignoreAAAA\n    drop\n    tcp\n    flow\n    icmp\n    http\n</code></pre>"},{"location":"management/monitoring/#network-flow-monitoring","title":"Network Flow Monitoring","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: hubble-metrics\nspec:\n  selector:\n    matchLabels:\n      k8s-app: hubble\n  endpoints:\n  - port: hubble-metrics\n    interval: 30s\n</code></pre>"},{"location":"management/monitoring/#storage-monitoring","title":"Storage Monitoring","text":""},{"location":"management/monitoring/#cdi-and-storage-metrics","title":"CDI and Storage Metrics","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cdi-controller-metrics\nspec:\n  selector:\n    matchLabels:\n      app: cdi-controller\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n</code></pre>"},{"location":"management/monitoring/#storage-performance-alerts","title":"Storage Performance Alerts","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: storage-monitoring-rules\nspec:\n  groups:\n  - name: storage.rules\n    rules:\n    - alert: HighStorageLatency\n      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes &lt; 0.1\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Storage volume {{ $labels.persistentvolumeclaim }} is running out of space\"\n\n    - alert: DataVolumeImportFailed\n      expr: increase(cdi_import_progress_total{phase=\"Failed\"}[5m]) &gt; 0\n      labels:\n        severity: warning\n      annotations:\n        summary: \"DataVolume import failed\"\n</code></pre>"},{"location":"management/monitoring/#application-performance-monitoring","title":"Application Performance Monitoring","text":""},{"location":"management/monitoring/#guest-agent-installation","title":"Guest Agent Installation","text":"<p>For enhanced VM monitoring, install QEMU Guest Agent:</p> <pre><code># Inside RHEL/CentOS VM\nsudo yum install qemu-guest-agent\nsudo systemctl enable qemu-guest-agent\nsudo systemctl start qemu-guest-agent\n\n# Inside Ubuntu VM\nsudo apt-get install qemu-guest-agent\nsudo systemctl enable qemu-guest-agent\nsudo systemctl start qemu-guest-agent\n\n# Inside Windows VM\n# Download and install virtio-win guest tools\n</code></pre>"},{"location":"management/monitoring/#node-exporter-for-vm-guests","title":"Node Exporter for VM Guests","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter-vm\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter-vm\n  template:\n    metadata:\n      labels:\n        app: node-exporter-vm\n    spec:\n      containers:\n      - name: node-exporter\n        image: prom/node-exporter:latest\n        ports:\n        - containerPort: 9100\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n</code></pre>"},{"location":"management/monitoring/#dashboard-configuration","title":"Dashboard Configuration","text":""},{"location":"management/monitoring/#grafana-dashboard-for-vms","title":"Grafana Dashboard for VMs","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"RH OVE Virtual Machine Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"VM CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"kubevirt_vm_cpu_usage_percentage\",\n            \"legendFormat\": \"{{name}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"VM Memory Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"kubevirt_vm_memory_usage_percentage\",\n            \"legendFormat\": \"{{name}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"VM Network I/O\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(kubevirt_vm_network_receive_bytes_total[5m])\",\n            \"legendFormat\": \"{{name}} - RX\"\n          },\n          {\n            \"expr\": \"rate(kubevirt_vm_network_transmit_bytes_total[5m])\",\n            \"legendFormat\": \"{{name}} - TX\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"management/monitoring/#dynatrace-dashboard-configuration","title":"Dynatrace Dashboard Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dynatrace-dashboard-config\ndata:\n  vm-overview.json: |\n    {\n      \"dashboardMetadata\": {\n        \"name\": \"RH OVE VM Overview\",\n        \"shared\": true,\n        \"tags\": [\"rh-ove\", \"virtualization\"]\n      },\n      \"tiles\": [\n        {\n          \"name\": \"VM Performance\",\n          \"tileType\": \"CUSTOM_CHARTING\",\n          \"configured\": true,\n          \"queries\": [\n            {\n              \"metric\": \"builtin:host.cpu.usage\",\n              \"aggregation\": {\n                \"type\": \"AVG\"\n              },\n              \"filterBy\": {\n                \"neType\": \"HOST\",\n                \"tags\": [\"vm:kubevirt\"]\n              }\n            }\n          ]\n        }\n      ]\n    }\n</code></pre>"},{"location":"management/monitoring/#alerting-strategy","title":"Alerting Strategy","text":""},{"location":"management/monitoring/#alert-routing-configuration","title":"Alert Routing Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\ndata:\n  alertmanager.yml: |\n    global:\n      smtp_smarthost: 'smtp.example.com:587'\n      smtp_from: 'alerts@example.com'\n\n    route:\n      group_by: ['alertname', 'cluster', 'service']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'web.hook'\n      routes:\n      - match:\n          severity: critical\n        receiver: 'critical-alerts'\n      - match:\n          service: vm\n        receiver: 'vm-alerts'\n\n    receivers:\n    - name: 'web.hook'\n      webhook_configs:\n      - url: 'http://webhook.example.com/alerts'\n\n    - name: 'critical-alerts'\n      email_configs:\n      - to: 'oncall@example.com'\n        subject: 'CRITICAL: {{ .GroupLabels.alertname }}'\n        body: |\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          {{ end }}\n\n    - name: 'vm-alerts'\n      slack_configs:\n      - api_url: 'https://hooks.slack.com/services/...'\n        channel: '#vm-alerts'\n        title: 'VM Alert: {{ .GroupLabels.alertname }}'\n</code></pre>"},{"location":"management/monitoring/#logging-strategy","title":"Logging Strategy","text":""},{"location":"management/monitoring/#centralized-logging-for-vms","title":"Centralized Logging for VMs","text":"<pre><code>apiVersion: logging.coreos.com/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: vm-logs\n  namespace: openshift-logging\nspec:\n  outputs:\n  - name: vm-logs-output\n    type: elasticsearch\n    url: https://elasticsearch.example.com:9200\n    elasticsearch:\n      index: vm-logs-{.log_type}-{.@timestamp.YYYY.MM.dd}\n  pipelines:\n  - name: vm-logs-pipeline\n    inputRefs:\n    - application\n    filterRefs:\n    - vm-log-filter\n    outputRefs:\n    - vm-logs-output\n</code></pre>"},{"location":"management/monitoring/#performance-optimization","title":"Performance Optimization","text":""},{"location":"management/monitoring/#monitoring-resource-optimization","title":"Monitoring Resource Optimization","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: monitoring-quota\n  namespace: monitoring\nspec:\n  hard:\n    requests.cpu: \"2\"\n    requests.memory: 4Gi\n    limits.cpu: \"4\"\n    limits.memory: 8Gi\n    persistentvolumeclaims: \"5\"\n</code></pre>"},{"location":"management/monitoring/#metrics-retention-policy","title":"Metrics Retention Policy","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 30s\n      evaluation_interval: 30s\n      external_labels:\n        cluster: 'rh-ove-cluster'\n\n    rule_files:\n    - \"vm-monitoring-rules.yml\"\n\n    scrape_configs:\n    - job_name: 'kubevirt-vms'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_label_kubevirt_io]\n        target_label: vm_name\n</code></pre>"},{"location":"management/monitoring/#troubleshooting-monitoring","title":"Troubleshooting Monitoring","text":""},{"location":"management/monitoring/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>OneAgent not reporting VM data <pre><code># Check OneAgent status\noc get pods -n dynatrace\noc describe pod dynatrace-oneagent-xxx\n\n# Verify VM annotations\noc get vm -o yaml | grep -A5 annotations\n</code></pre></p> </li> <li> <p>Missing VM metrics in Prometheus <pre><code># Check ServiceMonitor configuration\noc get servicemonitor -n monitoring\n\n# Verify metrics endpoint\noc port-forward svc/kubevirt-prometheus-metrics 8080:8080\ncurl localhost:8080/metrics\n</code></pre></p> </li> <li> <p>Network flow data not appearing <pre><code># Check Hubble status\ncilium status\nhubble status\n\n# Verify Hubble configuration\noc get configmap cilium-config -n kube-system -o yaml\n</code></pre></p> </li> </ol> <p>This comprehensive monitoring strategy ensures full visibility into the RH OVE ecosystem, covering infrastructure, virtual machines, containers, and application performance.</p>"},{"location":"operations/day2-ops/","title":"Day-2 Operations","text":""},{"location":"operations/day2-ops/#overview","title":"Overview","text":"<p>This document covers day-2 operational activities essential for maintaining the multi-cluster RH OVE ecosystem. It includes guidelines for managing the management cluster and multiple application clusters, covering ongoing maintenance, upgrades, performance tuning, and operational tasks across the entire fleet.</p>"},{"location":"operations/day2-ops/#maintenance-tasks","title":"Maintenance Tasks","text":""},{"location":"operations/day2-ops/#regular-cluster-health-checks","title":"Regular Cluster Health Checks","text":"<ul> <li> <p>Node Status Monitoring: Regularly check node health and availability.   <pre><code>oc get nodes -o wide\n</code></pre></p> </li> <li> <p>Resource Usage Monitoring: Monitor CPU, memory, and storage utilization.   <pre><code>oc adm top nodes\noc adm top pods --all-namespaces\n</code></pre></p> </li> </ul>"},{"location":"operations/day2-ops/#backup-management","title":"Backup Management","text":"<ul> <li> <p>Review Backup Logs: Ensure completion and verify logs for any anomalies.   <pre><code>oc logs -n rubrik rubrik-agent-\n</code></pre></p> </li> <li> <p>Data Integrity Checks: Periodically verify backup integrity and accessibility.</p> </li> </ul>"},{"location":"operations/day2-ops/#upgrades","title":"Upgrades","text":""},{"location":"operations/day2-ops/#openshift-cluster-upgrades","title":"OpenShift Cluster Upgrades","text":"<ul> <li>Plan Your Upgrade: Evaluate impact, and schedule during maintenance windows.</li> <li> <p>Review OpenShift Upgrade Guide</p> </li> <li> <p>In-place Upgrades: Use OpenShift's upgrade capabilities to update cluster components.   <pre><code>oc adm upgrade\n</code></pre></p> </li> </ul>"},{"location":"operations/day2-ops/#component-upgrades","title":"Component Upgrades","text":"<ul> <li> <p>Operator Lifecycle Management (OLM): Upgrade operators using OLM.   <pre><code>oc get clusterserviceversions -n openshift-operators\n</code></pre></p> </li> <li> <p>KubeVirt Upgrades: Follow the KubeVirt upgrade process for virtualization components.</p> </li> <li>Refer to KubeVirt Upgrade Guide</li> </ul>"},{"location":"operations/day2-ops/#performance-tuning","title":"Performance Tuning","text":""},{"location":"operations/day2-ops/#resource-balancing","title":"Resource Balancing","text":"<ul> <li> <p>Node Selector and Affinity Rules: Ensure workloads are distributed evenly.   <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n</code></pre></p> </li> <li> <p>Vertical and Horizontal Scaling: Utilize HPA and VPA for scaling applications.</p> </li> </ul>"},{"location":"operations/day2-ops/#network-optimization","title":"Network Optimization","text":"<ul> <li>Cilium Policy Management: Optimize and tune Cilium network policies for performance.   <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: optimized-policy\nspec:\n  endpointSelector:\n    matchLabels:\n      app: myapp\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: trusted\n</code></pre></li> </ul>"},{"location":"operations/day2-ops/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"operations/day2-ops/#regular-security-audits","title":"Regular Security Audits","text":"<ul> <li> <p>Policy Compliance: Ensure adherence to Kyverno policies and security standards.   <pre><code>kubectl get cpol -o yaml\n</code></pre></p> </li> <li> <p>Vulnerability Scans: Run regular vulnerability assessments on container images and hosts.</p> </li> </ul>"},{"location":"operations/day2-ops/#documentation-and-reporting","title":"Documentation and Reporting","text":""},{"location":"operations/day2-ops/#keeping-documentation-up-to-date","title":"Keeping Documentation Up-to-Date","text":"<ul> <li> <p>Change Logs: Maintain a changelog for all configurations and updates.</p> </li> <li> <p>Operational Runbooks: Create and update runbooks for standard operations.</p> </li> </ul>"},{"location":"operations/day2-ops/#performance-and-utilization-reports","title":"Performance and Utilization Reports","text":"<ul> <li>Utilize Metrics Dashboards: Use Grafana and Prometheus to generate reports.</li> </ul>"},{"location":"operations/day2-ops/#conclusion","title":"Conclusion","text":"<p>Following these day-2 operation guidelines helps maintain a stable, secure, and efficient RH OVE environment. Regular monitoring, updates, optimizations, and documentation ensure long-term success and reliability of the platform.</p>"},{"location":"operations/performance/","title":"Performance Tuning","text":""},{"location":"operations/performance/#overview","title":"Overview","text":"<p>This document provides comprehensive performance tuning guidelines for the RH OVE ecosystem, covering optimization strategies for virtual machines, networking, storage, and cluster-wide performance enhancements.</p>"},{"location":"operations/performance/#performance-optimization-strategy","title":"Performance Optimization Strategy","text":""},{"location":"operations/performance/#performance-monitoring-approach","title":"Performance Monitoring Approach","text":"<pre><code>graph TB\n    A[Baseline Metrics] --&gt; B[Identify Bottlenecks]\n    B --&gt; C[Apply Optimizations]\n    C --&gt; D[Measure Impact]\n    D --&gt; E{Performance Improved?}\n    E --&gt;|Yes| F[Document Changes]\n    E --&gt;|No| G[Try Alternative Approach]\n    G --&gt; C\n    F --&gt; H[Continuous Monitoring]</code></pre>"},{"location":"operations/performance/#key-performance-indicators-kpis","title":"Key Performance Indicators (KPIs)","text":"<ul> <li>VM Performance: CPU utilization, memory usage, disk I/O, network throughput</li> <li>Cluster Performance: Node utilization, pod scheduling latency, API response times</li> <li>Network Performance: Latency, packet loss, bandwidth utilization</li> <li>Storage Performance: IOPS, throughput, latency</li> </ul>"},{"location":"operations/performance/#virtual-machine-performance-tuning","title":"Virtual Machine Performance Tuning","text":""},{"location":"operations/performance/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"operations/performance/#cpu-pinning-for-high-performance-vms","title":"CPU Pinning for High-Performance VMs","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: high-performance-vm\nspec:\n  template:\n    spec:\n      domain:\n        cpu:\n          cores: 4\n          dedicatedCpuPlacement: true\n          isolateEmulatorThread: true\n        resources:\n          requests:\n            cpu: 4\n            memory: 8Gi\n          limits:\n            cpu: 4\n            memory: 8Gi\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n        cpumanager: \"true\"\n</code></pre>"},{"location":"operations/performance/#cpu-manager-configuration","title":"CPU Manager Configuration","text":"<pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: KubeletConfig\nmetadata:\n  name: cpumanager-enabled\nspec:\n  machineConfigPoolSelector:\n    matchLabels:\n      pools.operator.machineconfiguration.openshift.io/worker: \"\"\n  kubeletConfig:\n    cpuManagerPolicy: static\n    cpuManagerReconcilePeriod: 5s\n    reservedSystemCPUs: \"0,1\"\n</code></pre>"},{"location":"operations/performance/#numa-topology-awareness","title":"NUMA Topology Awareness","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: numa-optimized-vm\nspec:\n  template:\n    spec:\n      domain:\n        cpu:\n          cores: 8\n          numa:\n            guestMappingPassthrough: {}\n        memory:\n          guest: 16Gi\n          hugepages:\n            pageSize: 1Gi\n</code></pre>"},{"location":"operations/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"operations/performance/#hugepages-configuration","title":"Hugepages Configuration","text":"<pre><code># Node configuration for hugepages\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: hugepages-worker\n  labels:\n    machineconfiguration.openshift.io/role: worker\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    systemd:\n      units:\n      - name: hugepages-1gi.service\n        enabled: true\n        contents: |\n          [Unit]\n          Description=Configure 1Gi hugepages\n          [Service]\n          Type=oneshot\n          ExecStart=/bin/bash -c 'echo 8 &gt; /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages'\n          [Install]\n          WantedBy=multi-user.target\n</code></pre>"},{"location":"operations/performance/#vm-memory-configuration-with-hugepages","title":"VM Memory Configuration with Hugepages","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: memory-optimized-vm\nspec:\n  template:\n    spec:\n      domain:\n        memory:\n          guest: 8Gi\n          hugepages:\n            pageSize: 1Gi\n        resources:\n          requests:\n            memory: 8Gi\n            hugepages-1Gi: 8Gi\n          limits:\n            memory: 8Gi\n            hugepages-1Gi: 8Gi\n</code></pre>"},{"location":"operations/performance/#storage-performance-optimization","title":"Storage Performance Optimization","text":""},{"location":"operations/performance/#high-performance-storage-configuration","title":"High-Performance Storage Configuration","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: high-performance-ssd\nprovisioner: kubernetes.io/no-provisioner\nparameters:\n  type: ssd\n  fsType: ext4\n  # Optimize for performance\n  mountOptions: \"noatime,nodiratime\"\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"operations/performance/#vm-disk-performance-tuning","title":"VM Disk Performance Tuning","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: storage-optimized-vm\nspec:\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n              # Enable disk cache for better performance\n              cache: writeback\n          - name: datadisk\n            disk:\n              bus: virtio\n              cache: none\n              # Use native I/O for better performance\n              io: native\n        resources:\n          requests:\n            cpu: 2\n            memory: 4Gi\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: vm-root-disk\n      - name: datadisk\n        dataVolume:\n          name: vm-data-disk\n</code></pre>"},{"location":"operations/performance/#storage-io-optimization","title":"Storage I/O Optimization","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: optimized-datavolume\nspec:\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 100Gi\n    storageClassName: high-performance-ssd\n    # Optimize volume for performance\n    volumeMode: Block\n  source:\n    blank: {}\n</code></pre>"},{"location":"operations/performance/#network-performance-tuning","title":"Network Performance Tuning","text":""},{"location":"operations/performance/#cilium-performance-optimization","title":"Cilium Performance Optimization","text":""},{"location":"operations/performance/#ebpf-optimization-configuration","title":"eBPF Optimization Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  # Enable bandwidth manager for better QoS\n  enable-bandwidth-manager: \"true\"\n\n  # Enable local redirect policy for better performance\n  enable-local-redirect-policy: \"true\"\n\n  # Optimize datapath\n  datapath-mode: \"veth\"\n\n  # Enable XDP acceleration where supported\n  enable-xdp-acceleration: \"true\"\n\n  # kube-proxy replacement for better performance\n  kube-proxy-replacement: \"strict\"\n\n  # Optimize for performance\n  enable-cilium-endpoint-slice: \"true\"\n</code></pre>"},{"location":"operations/performance/#network-device-optimization","title":"Network Device Optimization","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  # Specify devices for optimal performance\n  devices: \"eth0\"\n\n  # Enable auto direct node routes\n  auto-direct-node-routes: \"true\"\n\n  # Optimize tunnel protocol\n  tunnel: \"disabled\"\n\n  # Use native routing when possible\n  enable-ipv4-masquerade: \"false\"\n  enable-ipv6-masquerade: \"false\"\n</code></pre>"},{"location":"operations/performance/#vm-network-performance","title":"VM Network Performance","text":""},{"location":"operations/performance/#sr-iov-configuration-for-high-performance-networking","title":"SR-IOV Configuration for High-Performance Networking","text":"<pre><code>apiVersion: sriovnetwork.openshift.io/v1\nkind: SriovNetworkNodePolicy\nmetadata:\n  name: high-performance-network\n  namespace: openshift-sriov-network-operator\nspec:\n  nodeSelector:\n    feature.node.kubernetes.io/network-sriov.capable: \"true\"\n  nicSelector:\n    vendor: \"15b3\"\n    deviceID: \"1017\"\n  numVfs: 8\n  priority: 99\n  resourceName: \"high_perf_nic\"\n</code></pre>"},{"location":"operations/performance/#vm-with-sr-iov-network-attachment","title":"VM with SR-IOV Network Attachment","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: sriov-vm\nspec:\n  template:\n    spec:\n      domain:\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: sriov-network\n            sriov: {}\n        resources:\n          requests:\n            cpu: 4\n            memory: 8Gi\n      networks:\n      - name: default\n        pod: {}\n      - name: sriov-network\n        multus:\n          networkName: high-performance-network\n</code></pre>"},{"location":"operations/performance/#multi-network-performance-with-multus","title":"Multi-Network Performance with Multus","text":""},{"location":"operations/performance/#dedicated-network-interfaces-for-different-traffic-types","title":"Dedicated Network Interfaces for Different Traffic Types","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: multi-interface-performance-vm\n  namespace: high-performance-workloads\n  annotations:\n    k8s.v1.cni.cncf.io/networks: |\n      [\n        {\n          \"name\": \"management-network\",\n          \"ips\": [\"192.168.1.5/24\"]\n        },\n        {\n          \"name\": \"storage-network\",\n          \"ips\": [\"192.168.2.5/24\"]\n        },\n        {\n          \"name\": \"sriov-data-network\",\n          \"ips\": [\"10.0.0.5/24\"]\n        }\n      ]\nspec:\n  running: true\n  template:\n    spec:\n      domain:\n        cpu:\n          cores: 16\n          dedicatedCpuPlacement: true\n          isolateEmulatorThread: true\n        memory:\n          guest: 32Gi\n          hugepages:\n            pageSize: 1Gi\n        devices:\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: management\n            bridge:\n              port: []\n          - name: storage\n            bridge:\n              port: []\n          - name: sriov-data\n            sriov: {}\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n              cache: writeback\n        resources:\n          requests:\n            cpu: 16\n            memory: 32Gi\n            hugepages-1Gi: 32Gi\n          limits:\n            cpu: 16\n            memory: 32Gi\n            hugepages-1Gi: 32Gi\n      networks:\n      - name: default\n        pod: {}\n      - name: management\n        multus:\n          networkName: management-network\n      - name: storage\n        multus:\n          networkName: storage-network\n      - name: sriov-data\n        multus:\n          networkName: sriov-data-network\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: multi-interface-vm-root\n</code></pre>"},{"location":"operations/performance/#high-performance-nad-configurations","title":"High-Performance NAD Configurations","text":"<pre><code># High-performance management network\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: management-network\n  namespace: high-performance-workloads\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"management-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens192\",\n      \"mode\": \"bridge\",\n      \"capabilities\": {\n        \"ips\": true\n      },\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n---\n# Dedicated storage network with optimized MTU\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: storage-network\n  namespace: high-performance-workloads\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"storage-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens224\",\n      \"mode\": \"bridge\",\n      \"mtu\": 9000,\n      \"capabilities\": {\n        \"ips\": true\n      },\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n---\n# SR-IOV high-performance data network\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: sriov-data-network\n  namespace: high-performance-workloads\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"sriov-data-network\",\n      \"type\": \"sriov\",\n      \"deviceID\": \"1017\",\n      \"vf\": 0,\n      \"spoofchk\": \"off\",\n      \"trust\": \"on\",\n      \"capabilities\": {\n        \"ips\": true\n      },\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n</code></pre>"},{"location":"operations/performance/#bond-network-for-high-availability","title":"Bond Network for High Availability","text":"<pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: bond-ha-network\n  namespace: high-performance-workloads\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"bond-ha-network\",\n      \"type\": \"bond\",\n      \"mode\": \"802.3ad\",\n      \"miimon\": \"100\",\n      \"updelay\": \"200\",\n      \"downdelay\": \"200\",\n      \"links\": [\n        {\n          \"name\": \"ens256\"\n        },\n        {\n          \"name\": \"ens257\"\n        }\n      ],\n      \"ipam\": {\n        \"type\": \"static\"\n      }\n    }\n</code></pre>"},{"location":"operations/performance/#cluster-performance-optimization","title":"Cluster Performance Optimization","text":""},{"location":"operations/performance/#node-level-optimizations","title":"Node-Level Optimizations","text":""},{"location":"operations/performance/#performance-profile-for-worker-nodes","title":"Performance Profile for Worker Nodes","text":"<pre><code>apiVersion: performance.openshift.io/v2\nkind: PerformanceProfile\nmetadata:\n  name: high-performance-worker\nspec:\n  cpu:\n    isolated: \"2-47\"\n    reserved: \"0-1\"\n  hugepages:\n    defaultHugepagesSize: 1G\n    pages:\n    - count: 16\n      size: 1G\n  nodeSelector:\n    node-role.kubernetes.io/worker-rt: \"\"\n  realTimeKernel:\n    enabled: true\n  numa:\n    topologyPolicy: \"single-numa-node\"\n</code></pre>"},{"location":"operations/performance/#machine-config-for-kernel-tuning","title":"Machine Config for Kernel Tuning","text":"<pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: performance-tuning\n  labels:\n    machineconfiguration.openshift.io/role: worker\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - path: /etc/sysctl.d/99-performance.conf\n        mode: 0644\n        contents:\n          inline: |\n            # Network performance tuning\n            net.core.rmem_max = 268435456\n            net.core.wmem_max = 268435456\n            net.ipv4.tcp_rmem = 4096 131072 268435456\n            net.ipv4.tcp_wmem = 4096 65536 268435456\n\n            # Virtual memory tuning\n            vm.swappiness = 1\n            vm.dirty_ratio = 15\n            vm.dirty_background_ratio = 5\n\n            # CPU scheduler tuning\n            kernel.sched_migration_cost_ns = 5000000\n</code></pre>"},{"location":"operations/performance/#resource-management-optimization","title":"Resource Management Optimization","text":""},{"location":"operations/performance/#cluster-resource-allocation","title":"Cluster Resource Allocation","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: performance-quota\n  namespace: high-performance-workloads\nspec:\n  hard:\n    requests.cpu: \"100\"\n    requests.memory: 200Gi\n    limits.cpu: \"200\"\n    limits.memory: 400Gi\n    hugepages-1Gi: 64Gi\n    persistentvolumeclaims: \"50\"\n</code></pre>"},{"location":"operations/performance/#priority-classes-for-critical-workloads","title":"Priority Classes for Critical Workloads","text":"<pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-performance-priority\nvalue: 1000\nglobalDefault: false\ndescription: \"Priority class for high-performance VMs\"\n---\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: critical-vm\nspec:\n  template:\n    spec:\n      priorityClassName: high-performance-priority\n      domain:\n        cpu:\n          cores: 8\n        memory:\n          guest: 16Gi\n</code></pre>"},{"location":"operations/performance/#monitoring-performance-optimizations","title":"Monitoring Performance Optimizations","text":""},{"location":"operations/performance/#efficient-metrics-collection","title":"Efficient Metrics Collection","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: performance-metrics\nspec:\n  selector:\n    matchLabels:\n      app: high-performance-app\n  endpoints:\n  - port: metrics\n    interval: 15s  # Reduced interval for better granularity\n    scrapeTimeout: 10s\n    path: /metrics\n    metricRelabelings:\n    - sourceLabels: [__name__]\n      regex: 'go_.*|process_.*'\n      action: drop  # Drop unnecessary metrics\n</code></pre>"},{"location":"operations/performance/#performance-dashboard-configuration","title":"Performance Dashboard Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: performance-dashboard\ndata:\n  dashboard.json: |\n    {\n      \"dashboard\": {\n        \"title\": \"RH OVE Performance Dashboard\",\n        \"panels\": [\n          {\n            \"title\": \"VM CPU Usage\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"rate(kubevirt_vm_cpu_usage_seconds_total[5m]) * 100\",\n                \"legendFormat\": \"{{name}} CPU %\"\n              }\n            ]\n          },\n          {\n            \"title\": \"VM Memory Usage\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"kubevirt_vm_memory_usage_bytes / kubevirt_vm_memory_available_bytes * 100\",\n                \"legendFormat\": \"{{name}} Memory %\"\n              }\n            ]\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"operations/performance/#performance-testing-and-benchmarking","title":"Performance Testing and Benchmarking","text":""},{"location":"operations/performance/#vm-performance-testing","title":"VM Performance Testing","text":"<pre><code>#!/bin/bash\n# VM Performance Test Script\n\nVM_NAME=\"performance-test-vm\"\nNAMESPACE=\"testing\"\n\n# CPU Performance Test\nvirtctl console $VM_NAME &lt;&lt; EOF\n# Install and run CPU benchmark\nyum install -y stress-ng\nstress-ng --cpu 0 --timeout 60s --metrics-brief\nEOF\n\n# Memory Performance Test\nvirtctl console $VM_NAME &lt;&lt; EOF\n# Memory bandwidth test\nstress-ng --vm 1 --vm-bytes 4G --timeout 60s --metrics-brief\nEOF\n\n# Disk I/O Performance Test\nvirtctl console $VM_NAME &lt;&lt; EOF\n# Disk performance test\ndd if=/dev/zero of=/tmp/testfile bs=1G count=1 oflag=direct\ndd if=/tmp/testfile of=/dev/null bs=1G count=1 iflag=direct\nrm /tmp/testfile\nEOF\n</code></pre>"},{"location":"operations/performance/#network-performance-testing","title":"Network Performance Testing","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-performance-test\nspec:\n  containers:\n  - name: iperf-server\n    image: networkstatic/iperf3\n    command: ['iperf3', '-s']\n    ports:\n    - containerPort: 5201\n  - name: iperf-client\n    image: networkstatic/iperf3\n    command: ['sleep', '3600']\n</code></pre>"},{"location":"operations/performance/#performance-troubleshooting","title":"Performance Troubleshooting","text":""},{"location":"operations/performance/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"operations/performance/#high-cpu-usage","title":"High CPU Usage","text":"<pre><code># Identify CPU-intensive processes\noc adm top pods --all-namespaces --sort-by=cpu\n\n# Check node CPU utilization\noc adm top nodes\n\n# Analyze CPU usage patterns\nvirtctl console &lt;vm-name&gt;\ntop -p 1\n</code></pre>"},{"location":"operations/performance/#memory-pressure","title":"Memory Pressure","text":"<pre><code># Check memory usage\noc adm top pods --all-namespaces --sort-by=memory\n\n# Verify hugepages allocation\noc get nodes -o custom-columns=NAME:.metadata.name,HUGEPAGES:.status.allocatable.hugepages-1Gi\n\n# Check for memory leaks in VM\nvirtctl console &lt;vm-name&gt;\nfree -h\ncat /proc/meminfo\n</code></pre>"},{"location":"operations/performance/#storage-performance-issues","title":"Storage Performance Issues","text":"<pre><code># Check storage performance metrics\noc get pvc\noc describe pvc &lt;pvc-name&gt;\n\n# Monitor I/O patterns\nvirtctl console &lt;vm-name&gt;\niostat -x 1\n\n# Check storage backend performance\noc get nodes -o wide\n</code></pre>"},{"location":"operations/performance/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"operations/performance/#vm-performance-best-practices","title":"VM Performance Best Practices","text":"<ol> <li>CPU Optimization</li> <li>Use CPU pinning for latency-sensitive workloads</li> <li>Enable NUMA topology awareness</li> <li> <p>Configure appropriate CPU limits and requests</p> </li> <li> <p>Memory Optimization</p> </li> <li>Use hugepages for memory-intensive applications</li> <li>Configure appropriate memory ballooning</li> <li> <p>Monitor memory usage patterns</p> </li> <li> <p>Storage Optimization</p> </li> <li>Use high-performance storage classes for critical workloads</li> <li>Optimize disk cache settings</li> <li> <p>Consider using block storage for high I/O workloads</p> </li> <li> <p>Network Optimization</p> </li> <li>Use SR-IOV for high-bandwidth applications</li> <li>Optimize Cilium configuration for performance</li> <li>Consider DPDK for packet processing workloads</li> </ol>"},{"location":"operations/performance/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<ol> <li>Regular Performance Reviews</li> <li>Monitor KPIs continuously</li> <li>Perform regular performance testing</li> <li> <p>Document performance baselines</p> </li> <li> <p>Capacity Planning</p> </li> <li>Plan for growth and scaling</li> <li>Monitor resource utilization trends</li> <li> <p>Implement proper resource quotas</p> </li> <li> <p>Optimization Cycles</p> </li> <li>Regular performance tuning reviews</li> <li>Test optimizations in non-production environments</li> <li>Document all performance changes</li> </ol> <p>This performance tuning guide provides comprehensive strategies for optimizing the RH OVE ecosystem. Regular application of these practices ensures optimal performance for virtualized workloads while maintaining system stability and reliability.</p>"},{"location":"operations/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"operations/troubleshooting/#overview","title":"Overview","text":"<p>This comprehensive troubleshooting guide addresses common issues in the RH OVE ecosystem, providing systematic approaches to diagnose and resolve problems across virtualization, networking, storage, and monitoring components.</p>"},{"location":"operations/troubleshooting/#general-troubleshooting-approach","title":"General Troubleshooting Approach","text":""},{"location":"operations/troubleshooting/#diagnostic-flow","title":"Diagnostic Flow","text":"<pre><code>graph TD\n    A[Issue Identified] --&gt; B[Gather Information]\n    B --&gt; C[Check Logs]\n    C --&gt; D[Verify Configuration]\n    D --&gt; E[Test Components]\n    E --&gt; F{Issue Resolved?}\n    F --&gt;|No| G[Escalate/Deep Dive]\n    F --&gt;|Yes| H[Document Solution]\n    G --&gt; I[Advanced Diagnostics]\n    I --&gt; J[Vendor Support]</code></pre>"},{"location":"operations/troubleshooting/#essential-commands","title":"Essential Commands","text":"<pre><code># Cluster overview\noc get nodes\noc get pods --all-namespaces\noc get events --all-namespaces --sort-by='.lastTimestamp'\n\n# Resource utilization\noc adm top nodes\noc adm top pods --all-namespaces\n\n# Detailed investigation\noc describe node &lt;node-name&gt;\noc logs -f &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#virtual-machine-issues","title":"Virtual Machine Issues","text":""},{"location":"operations/troubleshooting/#vm-wont-start","title":"VM Won't Start","text":""},{"location":"operations/troubleshooting/#symptoms","title":"Symptoms","text":"<ul> <li>VM remains in \"Pending\" or \"Scheduling\" state</li> <li>VM fails to boot or crashes during startup</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check VM Definition <pre><code>oc get vm &lt;vm-name&gt; -o yaml\noc describe vm &lt;vm-name&gt;\n</code></pre></p> </li> <li> <p>Verify Node Resources <pre><code>oc describe nodes\noc adm top nodes\n</code></pre></p> </li> <li> <p>Check DataVolume Status <pre><code>oc get datavolume\noc describe datavolume &lt;dv-name&gt;\n</code></pre></p> </li> <li> <p>Review Events <pre><code>oc get events --field-selector involvedObject.name=&lt;vm-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#common-solutions","title":"Common Solutions","text":"<ul> <li>Insufficient Resources: Scale cluster or adjust VM specs</li> <li>DataVolume Issues: Check CDI logs and storage classes</li> <li>Node Affinity: Verify node selector and affinity rules</li> </ul>"},{"location":"operations/troubleshooting/#vm-performance-issues","title":"VM Performance Issues","text":""},{"location":"operations/troubleshooting/#symptoms_1","title":"Symptoms","text":"<ul> <li>Slow VM performance</li> <li>High CPU/memory usage</li> <li>Network latency</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_1","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check VM Resource Allocation <pre><code>oc get vm &lt;vm-name&gt; -o jsonpath='{.spec.template.spec.domain.resources}'\n</code></pre></p> </li> <li> <p>Monitor VM Metrics <pre><code># Use virtctl to access VM console\nvirtctl console &lt;vm-name&gt;\n\n# Check VM performance inside guest\ntop\niostat\niftop\n</code></pre></p> </li> <li> <p>Verify Host Resources <pre><code>oc adm top node &lt;node-name&gt;\noc describe node &lt;node-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#solutions","title":"Solutions","text":"<ul> <li>Adjust VM CPU/memory allocation</li> <li>Enable CPU pinning for critical VMs</li> <li>Check storage performance and IOPS limits</li> </ul>"},{"location":"operations/troubleshooting/#networking-issues","title":"Networking Issues","text":""},{"location":"operations/troubleshooting/#cilium-network-problems","title":"Cilium Network Problems","text":""},{"location":"operations/troubleshooting/#symptoms_2","title":"Symptoms","text":"<ul> <li>Pods cannot communicate</li> <li>Network policies not working</li> <li>DNS resolution failures</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_2","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check Cilium Status <pre><code>cilium status\ncilium connectivity test\n</code></pre></p> </li> <li> <p>Verify Network Policies <pre><code>oc get cnp\noc describe cnp &lt;policy-name&gt;\n</code></pre></p> </li> <li> <p>Monitor Network Flows <pre><code>hubble observe --pod &lt;pod-name&gt;\nhubble observe --verdict DENIED\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#common-solutions_1","title":"Common Solutions","text":"<pre><code># Debug network connectivity\napiVersion: v1\nkind: Pod\nmetadata:\n  name: network-debug\nspec:\n  containers:\n  - name: debug\n    image: nicolaka/netshoot\n    command: ['sleep', '3600']\n</code></pre>"},{"location":"operations/troubleshooting/#vm-network-connectivity","title":"VM Network Connectivity","text":""},{"location":"operations/troubleshooting/#symptoms_3","title":"Symptoms","text":"<ul> <li>VM cannot reach external networks</li> <li>Inter-VM communication failures</li> <li>Service discovery issues</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_3","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check VM Network Configuration <pre><code>oc get vm &lt;vm-name&gt; -o yaml | grep -A 10 networks\n</code></pre></p> </li> <li> <p>Verify Service Configuration <pre><code>oc get svc\noc describe svc &lt;service-name&gt;\n</code></pre></p> </li> <li> <p>Test Connectivity from VM <pre><code>virtctl console &lt;vm-name&gt;\n# Inside VM:\nping &lt;target-ip&gt;\nnslookup &lt;service-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#storage-issues","title":"Storage Issues","text":""},{"location":"operations/troubleshooting/#datavolume-problems","title":"DataVolume Problems","text":""},{"location":"operations/troubleshooting/#symptoms_4","title":"Symptoms","text":"<ul> <li>DataVolume stuck in \"Pending\" state</li> <li>Import/clone operations failing</li> <li>Storage quota exceeded</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_4","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check DataVolume Status <pre><code>oc get datavolume\noc describe datavolume &lt;dv-name&gt;\n</code></pre></p> </li> <li> <p>Review CDI Logs <pre><code>oc logs -n cdi deployment/cdi-controller\noc logs -n cdi deployment/cdi-operator\n</code></pre></p> </li> <li> <p>Verify Storage Classes <pre><code>oc get storageclass\noc describe storageclass &lt;sc-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#solutions_1","title":"Solutions","text":"<pre><code># Debug DataVolume with verbose logging\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: debug-dv\n  annotations:\n    cdi.kubevirt.io/debug: \"true\"\nspec:\n  pvc:\n    accessModes: [ReadWriteOnce]\n    resources:\n      requests:\n        storage: 10Gi\n  source:\n    blank: {}\n</code></pre>"},{"location":"operations/troubleshooting/#storage-performance-issues","title":"Storage Performance Issues","text":""},{"location":"operations/troubleshooting/#symptoms_5","title":"Symptoms","text":"<ul> <li>Slow disk I/O</li> <li>High storage latency</li> <li>VM disk full errors</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_5","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check Storage Metrics <pre><code># Prometheus queries\nkubectl port-forward -n monitoring svc/prometheus 9090:9090\n# Query: kubelet_volume_stats_used_bytes\n</code></pre></p> </li> <li> <p>Verify PVC Usage <pre><code>oc get pvc\noc describe pvc &lt;pvc-name&gt;\n</code></pre></p> </li> <li> <p>Monitor Storage Node Performance <pre><code>oc adm top nodes\niostat -x 1\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#monitoring-issues","title":"Monitoring Issues","text":""},{"location":"operations/troubleshooting/#dynatrace-agent-problems","title":"Dynatrace Agent Problems","text":""},{"location":"operations/troubleshooting/#symptoms_6","title":"Symptoms","text":"<ul> <li>Missing VM metrics in Dynatrace</li> <li>OneAgent not reporting data</li> <li>High resource usage by monitoring</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_6","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check OneAgent Status <pre><code>oc get pods -n dynatrace\noc describe pod &lt;oneagent-pod&gt;\n</code></pre></p> </li> <li> <p>Verify VM Annotations <pre><code>oc get vm -o yaml | grep -A5 annotations\n</code></pre></p> </li> <li> <p>Review Dynatrace Logs <pre><code>oc logs -n dynatrace &lt;oneagent-pod&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#prometheus-metrics-missing","title":"Prometheus Metrics Missing","text":""},{"location":"operations/troubleshooting/#symptoms_7","title":"Symptoms","text":"<ul> <li>Missing metrics in Grafana</li> <li>ServiceMonitor not working</li> <li>Prometheus targets down</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_7","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check ServiceMonitor Configuration <pre><code>oc get servicemonitor\noc describe servicemonitor &lt;sm-name&gt;\n</code></pre></p> </li> <li> <p>Verify Metrics Endpoints <pre><code>oc port-forward svc/&lt;service-name&gt; 8080:8080\ncurl localhost:8080/metrics\n</code></pre></p> </li> <li> <p>Check Prometheus Targets <pre><code># Access Prometheus UI\noc port-forward -n monitoring svc/prometheus 9090:9090\n# Go to Status -&gt; Targets\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#gitops-and-argo-cd-issues","title":"GitOps and Argo CD Issues","text":""},{"location":"operations/troubleshooting/#application-sync-failures","title":"Application Sync Failures","text":""},{"location":"operations/troubleshooting/#symptoms_8","title":"Symptoms","text":"<ul> <li>Applications stuck in \"OutOfSync\" state</li> <li>Sync operations failing</li> <li>Resource conflicts</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_8","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check Application Status <pre><code>argocd app get &lt;app-name&gt;\nargocd app logs &lt;app-name&gt;\n</code></pre></p> </li> <li> <p>Verify Git Repository Access <pre><code>argocd repo list\nargocd repo get &lt;repo-url&gt;\n</code></pre></p> </li> <li> <p>Review Resource Conflicts <pre><code>oc get &lt;resource-type&gt; &lt;resource-name&gt; -o yaml\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#solutions_2","title":"Solutions","text":"<pre><code># Force refresh and sync\nargocd app refresh &lt;app-name&gt;\nargocd app sync &lt;app-name&gt; --force\n\n# Reset application state\nargocd app actions run &lt;app-name&gt; restart --kind Deployment\n</code></pre>"},{"location":"operations/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"operations/troubleshooting/#cluster-resource-exhaustion","title":"Cluster Resource Exhaustion","text":""},{"location":"operations/troubleshooting/#symptoms_9","title":"Symptoms","text":"<ul> <li>High CPU/memory usage</li> <li>Pod evictions</li> <li>Slow response times</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_9","title":"Troubleshooting Steps","text":"<ol> <li> <p>Identify Resource Consumers <pre><code>oc adm top pods --all-namespaces --sort-by=cpu\noc adm top pods --all-namespaces --sort-by=memory\n</code></pre></p> </li> <li> <p>Check Node Capacity <pre><code>oc describe nodes | grep -A5 \"Allocated resources\"\n</code></pre></p> </li> <li> <p>Review Resource Quotas <pre><code>oc get resourcequota --all-namespaces\noc describe resourcequota &lt;quota-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#vm-live-migration-issues","title":"VM Live Migration Issues","text":""},{"location":"operations/troubleshooting/#symptoms_10","title":"Symptoms","text":"<ul> <li>Migration fails or takes too long</li> <li>VM downtime during migration</li> <li>Network connectivity loss</li> </ul>"},{"location":"operations/troubleshooting/#troubleshooting-steps_10","title":"Troubleshooting Steps","text":"<ol> <li> <p>Check Migration Status <pre><code>oc get vmi\noc describe virtualmachinmigration &lt;migration-name&gt;\n</code></pre></p> </li> <li> <p>Verify Node Compatibility <pre><code>oc get nodes -o wide\noc describe node &lt;target-node&gt;\n</code></pre></p> </li> <li> <p>Monitor Migration Progress <pre><code>oc get events --field-selector reason=LiveMigration\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#emergency-procedures","title":"Emergency Procedures","text":""},{"location":"operations/troubleshooting/#cluster-recovery","title":"Cluster Recovery","text":""},{"location":"operations/troubleshooting/#when-multiple-nodes-are-down","title":"When Multiple Nodes Are Down","text":"<ol> <li> <p>Check etcd Health <pre><code>oc get etcd -o yaml\noc logs -n openshift-etcd &lt;etcd-pod&gt;\n</code></pre></p> </li> <li> <p>Restore from Backup <pre><code># Follow OpenShift disaster recovery procedures\noc adm restore-cluster\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#vm-emergency-access","title":"VM Emergency Access","text":""},{"location":"operations/troubleshooting/#when-vm-console-is-unresponsive","title":"When VM Console Is Unresponsive","text":"<ol> <li> <p>Use virtctl <pre><code>virtctl console &lt;vm-name&gt;\nvirtctl vnc &lt;vm-name&gt;\n</code></pre></p> </li> <li> <p>Force VM Restart <pre><code>virtctl restart &lt;vm-name&gt;\nvirtctl stop &lt;vm-name&gt; --force\n</code></pre></p> </li> </ol>"},{"location":"operations/troubleshooting/#advanced-diagnostics","title":"Advanced Diagnostics","text":""},{"location":"operations/troubleshooting/#debug-pod-creation","title":"Debug Pod Creation","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: debug-tools\nspec:\n  containers:\n  - name: debug\n    image: registry.redhat.io/ubi8/ubi:latest\n    command: ['sleep', '3600']\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - name: host\n      mountPath: /host\n  volumes:\n  - name: host\n    hostPath:\n      path: /\n  nodeSelector:\n    kubernetes.io/hostname: &lt;node-name&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#log-collection-script","title":"Log Collection Script","text":"<pre><code>#!/bin/bash\n# Comprehensive log collection script\n\nNAMESPACE=${1:-default}\nOUTPUT_DIR=\"troubleshooting-$(date +%Y%m%d-%H%M%S)\"\n\nmkdir -p $OUTPUT_DIR\n\n# Cluster information\noc cluster-info &gt; $OUTPUT_DIR/cluster-info.txt\noc get nodes -o wide &gt; $OUTPUT_DIR/nodes.txt\noc get pods --all-namespaces &gt; $OUTPUT_DIR/all-pods.txt\n\n# VM specific information\noc get vm --all-namespaces -o yaml &gt; $OUTPUT_DIR/vms.yaml\noc get vmi --all-namespaces -o yaml &gt; $OUTPUT_DIR/vmis.yaml\noc get datavolume --all-namespaces -o yaml &gt; $OUTPUT_DIR/datavolumes.yaml\n\n# Events\noc get events --all-namespaces --sort-by='.lastTimestamp' &gt; $OUTPUT_DIR/events.txt\n\n# Logs from key components\noc logs -n openshift-cnv deployment/virt-controller &gt; $OUTPUT_DIR/virt-controller.log\noc logs -n openshift-cnv deployment/virt-api &gt; $OUTPUT_DIR/virt-api.log\noc logs -n cdi deployment/cdi-controller &gt; $OUTPUT_DIR/cdi-controller.log\n\necho \"Logs collected in $OUTPUT_DIR\"\ntar -czf $OUTPUT_DIR.tar.gz $OUTPUT_DIR\n</code></pre>"},{"location":"operations/troubleshooting/#support-and-escalation","title":"Support and Escalation","text":""},{"location":"operations/troubleshooting/#when-to-escalate","title":"When to Escalate","text":"<ul> <li>Hardware failures</li> <li>Data corruption issues</li> <li>Security breaches</li> <li>Performance degradation &gt; 50%</li> <li>Multiple component failures</li> </ul>"},{"location":"operations/troubleshooting/#information-to-gather","title":"Information to Gather","text":"<ol> <li>Environment Details</li> <li>OpenShift version</li> <li>KubeVirt version</li> <li> <p>Cluster size and configuration</p> </li> <li> <p>Problem Description</p> </li> <li>Timeline of events</li> <li>Error messages</li> <li> <p>Impact assessment</p> </li> <li> <p>Diagnostic Data</p> </li> <li>Logs (sanitized)</li> <li>Configuration files</li> <li>Resource utilization data</li> </ol>"},{"location":"operations/troubleshooting/#support-contacts","title":"Support Contacts","text":"<ul> <li>Red Hat Support: https://access.redhat.com/support/</li> <li>Community Forums: https://commons.openshift.org/</li> <li>KubeVirt Community: https://kubevirt.io/community/</li> </ul> <p>This troubleshooting guide provides systematic approaches to resolve common issues in the RH OVE ecosystem. Regular review and updates of this guide ensure it remains current with evolving technologies and operational experiences.</p>"},{"location":"project-plan/budget-estimate/","title":"RH OVE Ecosystem Budget Estimate","text":""},{"location":"project-plan/budget-estimate/#executive-summary","title":"Executive Summary","text":"<p>This budget estimate outlines the financial requirements for the RH OVE Ecosystem implementation, considering infrastructure, use-cases, and migration workload projects.</p>"},{"location":"project-plan/budget-estimate/#budget-categories","title":"Budget Categories","text":"<ol> <li>Infrastructure Costs</li> <li>Migration Costs</li> <li>Use-Cases Implementation Costs</li> <li>Operational Costs</li> <li>Contingency Fund</li> </ol>"},{"location":"project-plan/budget-estimate/#detailed-budget-estimate","title":"Detailed Budget Estimate","text":""},{"location":"project-plan/budget-estimate/#1-infrastructure-costs","title":"1. Infrastructure Costs","text":"Item Estimated Cost (USD) Compute Resources $1,000,000 Storage Solutions $500,000 Networking Equipment $300,000 Security Implementations $150,000 Software Licenses $200,000 Total Infrastructure Cost $2,150,000"},{"location":"project-plan/budget-estimate/#2-migration-costs","title":"2. Migration Costs","text":"Item Estimated Cost (USD) Migration Tools $200,000 VMware Licenses $100,000 Personnel Training $150,000 Consulting Services $250,000 Total Migration Cost $700,000"},{"location":"project-plan/budget-estimate/#3-use-cases-implementation-costs","title":"3. Use-Cases Implementation Costs","text":"Item Estimated Cost (USD) Application Development Tools $300,000 Testing Frameworks $100,000 Security Compliance $150,000 Monitoring Solutions $100,000 Total Use-Cases Cost $650,000"},{"location":"project-plan/budget-estimate/#4-operational-costs","title":"4. Operational Costs","text":"Item Estimated Cost (USD) Personnel Salaries $1,200,000 Maintenance Contracts $200,000 Utilities and Overhead $150,000 Total Operational Cost $1,550,000"},{"location":"project-plan/budget-estimate/#5-contingency-fund","title":"5. Contingency Fund","text":"<ul> <li>10% of Total Estimated Cost: Approx. $505,000</li> </ul>"},{"location":"project-plan/budget-estimate/#total-estimated-project-cost","title":"Total Estimated Project Cost","text":"<ul> <li>Overall Total: $5,555,000</li> </ul>"},{"location":"project-plan/budget-estimate/#notes","title":"Notes:","text":"<ul> <li>The costs provided are approximations based on industry standards and may vary based on final project requirements and vendor agreements.</li> <li>Contingency fund is reserved for unexpected expenses and scope changes.</li> <li>Regular financial reviews will be conducted to ensure budget compliance and adjust estimates as needed.</li> </ul>"},{"location":"project-plan/detailed-project-timeline/","title":"RH OVE Ecosystem - Detailed Project Timeline","text":""},{"location":"project-plan/detailed-project-timeline/#project-overview","title":"Project Overview","text":"<p>This document provides detailed timelines, milestones, and dependencies for the three sub-projects within the RH OVE Ecosystem implementation.</p> <p>Project Duration: 12-18 months Start Date: TBD Target Completion: TBD</p>"},{"location":"project-plan/detailed-project-timeline/#timeline-summary","title":"Timeline Summary","text":"Sub-Project Duration Dependencies Key Deliverables RH OVE Infrastructure 6-8 months None (Foundation) Multi-cluster platform, ADRs, Operations runbooks Use-Cases Implementation 8-10 months Infrastructure 70% complete Working use-cases, Integration patterns Migration from VMware 10-12 months Infrastructure 80% complete Migrated workloads, Decommissioned legacy"},{"location":"project-plan/detailed-project-timeline/#project-timeline-visualization","title":"Project Timeline Visualization","text":"<pre><code>gantt\n    title RH OVE Ecosystem Project Timeline\n    dateFormat YYYY-MM-DD\n    axisFormat %m/%d\n\n    section Infrastructure Project\n    Study Phase           :inf-study, 2024-01-01, 28d\n    Design Phase          :inf-design, after inf-study, 56d\n    Implementation Phase  :inf-impl, after inf-design, 56d\n    Testing Phase         :inf-test, after inf-impl, 28d\n    Day-2 Operations      :inf-ops, after inf-test, 28d\n\n    section Use-Cases Project  \n    Study Phase           :uc-study, after inf-design, 28d\n    Design Phase          :uc-design, after uc-study, 56d\n    Implementation Phase  :uc-impl, after inf-impl, 84d\n    Testing Phase         :uc-test, after uc-impl, 28d\n    Day-2 Operations      :uc-ops, after uc-test, 28d\n\n    section Migration Project\n    Study Phase           :mig-study, after inf-impl, 56d\n    Design Phase          :mig-design, after mig-study, 56d\n    Implementation Phase  :mig-impl, after inf-test, 98d\n    Testing Phase         :mig-test, after mig-impl, 28d\n    Day-2 Operations      :mig-ops, after mig-test, 42d</code></pre>"},{"location":"project-plan/detailed-project-timeline/#timeline-dependencies-diagram","title":"Timeline Dependencies Diagram","text":"<pre><code>flowchart TD\n    A[Project Start] --&gt; B[Infrastructure Study]\n    B --&gt; C[Infrastructure Design]\n    C --&gt; D[Infrastructure Implementation]\n    D --&gt; E[Infrastructure Testing]\n    E --&gt; F[Infrastructure Day-2 Ops]\n\n    C --&gt; G[Use-Cases Study]\n    G --&gt; H[Use-Cases Design]\n    D --&gt; I[Use-Cases Implementation]\n    I --&gt; J[Use-Cases Testing]\n    J --&gt; K[Use-Cases Day-2 Ops]\n\n    D --&gt; L[Migration Study]\n    L --&gt; M[Migration Design]\n    E --&gt; N[Migration Implementation]\n    N --&gt; O[Migration Testing]\n    O --&gt; P[Migration Day-2 Ops]\n\n    F --&gt; Q[Project Complete]\n    K --&gt; Q\n    P --&gt; Q\n\n    classDef infrastructure fill:#e1f5fe\n    classDef usecases fill:#f3e5f5\n    classDef migration fill:#e8f5e8\n    classDef milestone fill:#fff3e0\n\n    class B,C,D,E,F infrastructure\n    class G,H,I,J,K usecases\n    class L,M,N,O,P migration\n    class A,Q milestone</code></pre>"},{"location":"project-plan/detailed-project-timeline/#resource-allocation-timeline","title":"Resource Allocation Timeline","text":"<pre><code>gantt\n    title Detailed Resource Allocation by Phase\n    dateFormat YYYY-MM-DD\n    axisFormat %m/%d\n\n    section Infrastructure Team\n    Infrastructure Architect       :inf-arch, 2024-01-01, 84d\n    Infrastructure Architect PT    :inf-arch-pt, 2024-03-25, 28d\n    DevOps Engineers (2)           :inf-devops, 2024-01-29, 140d\n    System Administrators (2)      :inf-sysadmin, 2024-03-25, 112d\n    Security Engineer              :inf-security, 2024-01-29, 84d\n    Network Engineer               :inf-network, 2024-01-01, 140d\n\n    section Use-Cases Team\n    Solution Architect             :uc-arch, 2024-03-25, 84d\n    Application Developers (3)     :uc-dev, 2024-04-22, 140d\n    Testing Specialist             :uc-test, 2024-08-12, 56d\n    DevOps Engineer                :uc-devops, 2024-05-20, 168d\n    Security Specialist            :uc-security, 2024-04-22, 112d\n    Business Analyst               :uc-ba, 2024-03-25, 56d\n\n    section Migration Team\n    Migration Specialist           :mig-specialist, 2024-04-22, 308d\n    VMware Administrators (2)      :mig-vmware, 2024-04-22, 196d\n    RH OVE Engineers (2)           :mig-rhove, 2024-08-12, 196d\n    Application Owners (2)         :mig-appowner, 2024-07-15, 182d\n    Performance Engineer           :mig-perf, 2024-06-17, 210d\n    Backup Administrator           :mig-backup, 2024-05-20, 280d\n\n    section Critical Milestones\n    Infrastructure 70% Complete   :milestone, 2024-06-17, 1d\n    Infrastructure 80% Complete   :milestone, 2024-07-08, 1d\n    All Projects Complete         :milestone, 2025-02-24, 1d</code></pre>"},{"location":"project-plan/detailed-project-timeline/#resource-allocation-notes","title":"Resource Allocation Notes","text":"<p>Infrastructure Team Variable Allocation: - Infrastructure Architect: Full-time during Study/Design phases, part-time for consulting during implementation - DevOps Engineers: Start during Design phase, continue through implementation and operations - System Administrators: Join during Implementation phase, continue for Day-2 operations - Security Engineer: Active during Design through Testing phases - Network Engineer: Active from Study through Implementation phases</p> <p>Use-Cases Team Variable Allocation: - Solution Architect: Active during Study and Design phases primarily - Application Developers: Core implementation team, active longest duration - Testing Specialist: Primarily active during Testing phase with some overlap - DevOps Engineer: Active during Implementation and operations phases - Security Specialist: Active during Design, Implementation, and Testing phases - Business Analyst: Active during Study and early Design phases</p> <p>Migration Team Variable Allocation: - Migration Specialist: Project-long involvement for strategy and coordination - VMware Administrators: Active during Study through early Implementation - RH OVE Engineers: Active during Implementation through Day-2 operations - Application Owners: Active during Design, Implementation, and Testing phases - Performance Engineer: Active during Testing and early Implementation phases - Backup Administrator: Active during Implementation through Day-2 operations</p>"},{"location":"project-plan/detailed-project-timeline/#sub-project-1-rh-ove-infrastructure-project","title":"Sub-Project 1: RH OVE Infrastructure Project","text":""},{"location":"project-plan/detailed-project-timeline/#phase-1-study-phase-weeks-1-4","title":"Phase 1: Study Phase (Weeks 1-4)","text":"<p>Duration: 4 weeks Effort: 3-4 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones","title":"Milestones","text":"<ul> <li> Week 2: Current state assessment completed</li> <li> Week 3: Gap analysis finalized</li> <li> Week 4: Requirements gathering complete</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables","title":"Deliverables","text":"<ul> <li>Current infrastructure assessment report</li> <li>Gap analysis document</li> <li>Technical requirements specification</li> <li>Resource capacity planning</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#key-activities","title":"Key Activities","text":"<ul> <li>Analyze existing infrastructure components</li> <li>Document current network, storage, and compute resources</li> <li>Identify skill gaps and training needs</li> <li>Define non-functional requirements (performance, security, scalability)</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-2-design-phase-weeks-5-12","title":"Phase 2: Design Phase (Weeks 5-12)","text":"<p>Duration: 8 weeks Effort: 4-5 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_1","title":"Milestones","text":"<ul> <li> Week 7: High-Level Design (HLD) approved</li> <li> Week 10: Low-Level Design (LLD) completed</li> <li> Week 12: ADRs finalized and approved</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_1","title":"Deliverables","text":"<ul> <li>High-Level Design document</li> <li>Low-Level Design document</li> <li>8 Architecture Decision Records (ADRs)</li> <li>Security architecture design</li> <li>Network topology design</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#key-activities_1","title":"Key Activities","text":"<ul> <li>Design multi-cluster architecture</li> <li>Define GitOps workflows with ArgoCD</li> <li>Plan Cilium CNI implementation</li> <li>Design backup and monitoring strategies</li> <li>Create security and IAM frameworks</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-3-implementation-phase-weeks-13-20","title":"Phase 3: Implementation Phase (Weeks 13-20)","text":"<p>Duration: 8 weeks Effort: 5-6 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_2","title":"Milestones","text":"<ul> <li> Week 15: Management cluster deployed</li> <li> Week 17: First workload cluster operational</li> <li> Week 19: GitOps pipeline functional</li> <li> Week 20: Monitoring and backup systems active</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_2","title":"Deliverables","text":"<ul> <li>Deployed multi-cluster RH OVE environment</li> <li>GitOps configuration and workflows</li> <li>Monitoring stack (Prometheus, Grafana, Dynatrace)</li> <li>Backup solution (Rubrik integration)</li> <li>Admission controllers configuration</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#key-activities_2","title":"Key Activities","text":"<ul> <li>Deploy management and workload clusters</li> <li>Configure Cilium CNI and network policies</li> <li>Implement GitOps with ArgoCD</li> <li>Set up monitoring and alerting</li> <li>Configure backup and disaster recovery</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-4-testing-phase-weeks-21-24","title":"Phase 4: Testing Phase (Weeks 21-24)","text":"<p>Duration: 4 weeks Effort: 4-5 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_3","title":"Milestones","text":"<ul> <li> Week 22: Security testing completed</li> <li> Week 23: Performance benchmarks established</li> <li> Week 24: Infrastructure acceptance testing passed</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_3","title":"Deliverables","text":"<ul> <li>Security assessment report</li> <li>Performance benchmark results</li> <li>Test execution reports</li> <li>Infrastructure acceptance criteria validation</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#key-activities_3","title":"Key Activities","text":"<ul> <li>Conduct security penetration testing</li> <li>Perform load and stress testing</li> <li>Validate disaster recovery procedures</li> <li>Execute acceptance test scenarios</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-5-day-2-operations-weeks-25-28","title":"Phase 5: Day-2 Operations (Weeks 25-28)","text":"<p>Duration: 4 weeks Effort: 3-4 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_4","title":"Milestones","text":"<ul> <li> Week 26: Operations runbooks completed</li> <li> Week 27: Team training finished</li> <li> Week 28: Operations handover complete</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_4","title":"Deliverables","text":"<ul> <li>Day-2 operations runbooks</li> <li>Troubleshooting guides</li> <li>Performance tuning procedures</li> <li>Team training materials</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#sub-project-2-use-cases-implementation-project","title":"Sub-Project 2: Use-Cases Implementation Project","text":""},{"location":"project-plan/detailed-project-timeline/#phase-1-study-phase-weeks-15-18","title":"Phase 1: Study Phase (Weeks 15-18)","text":"<p>Duration: 4 weeks Effort: 2-3 FTEs Dependency: Infrastructure Design Phase 75% complete</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_5","title":"Milestones","text":"<ul> <li> Week 16: Use-case requirements gathered</li> <li> Week 17: Business value assessment completed</li> <li> Week 18: Use-case prioritization finalized</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_5","title":"Deliverables","text":"<ul> <li>Use-case requirements specification</li> <li>Business value assessment</li> <li>Implementation priority matrix</li> <li>Resource allocation plan</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-2-design-phase-weeks-19-26","title":"Phase 2: Design Phase (Weeks 19-26)","text":"<p>Duration: 8 weeks Effort: 3-4 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_6","title":"Milestones","text":"<ul> <li> Week 21: VM lifecycle use-cases designed</li> <li> Week 23: Application deployment patterns defined</li> <li> Week 25: Enterprise integration patterns completed</li> <li> Week 26: All use-case designs approved</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_6","title":"Deliverables","text":"<ul> <li>Use-case HLD and LLD documents</li> <li>Integration architecture designs</li> <li>Security and compliance frameworks</li> <li>Performance and scalability specifications</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-3-implementation-phase-weeks-27-38","title":"Phase 3: Implementation Phase (Weeks 27-38)","text":"<p>Duration: 12 weeks Effort: 4-6 FTEs Dependency: Infrastructure Implementation 80% complete</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_7","title":"Milestones","text":"<ul> <li> Week 29: VM lifecycle use-cases implemented</li> <li> Week 32: Hybrid applications deployed</li> <li> Week 35: Database services operational</li> <li> Week 37: Security and observability complete</li> <li> Week 38: All use-cases integrated</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_7","title":"Deliverables","text":"<ul> <li>Implemented use-cases with working demonstrations</li> <li>Configuration manifests and scripts</li> <li>Integration patterns and templates</li> <li>Security controls and policies</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-4-testing-phase-weeks-39-42","title":"Phase 4: Testing Phase (Weeks 39-42)","text":"<p>Duration: 4 weeks Effort: 3-4 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_8","title":"Milestones","text":"<ul> <li> Week 40: Functional testing completed</li> <li> Week 41: Integration testing finished</li> <li> Week 42: Business validation approved</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_8","title":"Deliverables","text":"<ul> <li>Test execution reports</li> <li>Performance validation results</li> <li>Business outcome measurements</li> <li>User acceptance documentation</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-5-day-2-operations-weeks-43-46","title":"Phase 5: Day-2 Operations (Weeks 43-46)","text":"<p>Duration: 4 weeks Effort: 2-3 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_9","title":"Milestones","text":"<ul> <li> Week 44: Use-case runbooks completed</li> <li> Week 45: Monitoring dashboards operational</li> <li> Week 46: Knowledge transfer complete</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_9","title":"Deliverables","text":"<ul> <li>Use-case operational runbooks</li> <li>Monitoring and alerting configurations</li> <li>Best practices documentation</li> <li>Training materials</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#sub-project-3-migration-workload-from-vmware-project","title":"Sub-Project 3: Migration Workload from VMware Project","text":""},{"location":"project-plan/detailed-project-timeline/#phase-1-study-phase-weeks-21-28","title":"Phase 1: Study Phase (Weeks 21-28)","text":"<p>Duration: 8 weeks Effort: 3-4 FTEs Dependency: Infrastructure Implementation 60% complete</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_10","title":"Milestones","text":"<ul> <li> Week 23: VMware inventory completed</li> <li> Week 25: Workload assessment finished</li> <li> Week 27: Migration strategy approved</li> <li> Week 28: Migration waves defined</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_10","title":"Deliverables","text":"<ul> <li>Complete VMware workload inventory</li> <li>Workload assessment and compatibility analysis</li> <li>Migration strategy document</li> <li>Migration wave planning</li> <li>Risk assessment and mitigation plan</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-2-design-phase-weeks-29-36","title":"Phase 2: Design Phase (Weeks 29-36)","text":"<p>Duration: 8 weeks Effort: 4-5 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_11","title":"Milestones","text":"<ul> <li> Week 31: Migration architecture designed</li> <li> Week 33: VM templates standardized</li> <li> Week 35: Migration procedures documented</li> <li> Week 36: Pilot migration plan approved</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_11","title":"Deliverables","text":"<ul> <li>Migration architecture design</li> <li>VM template standards and configurations</li> <li>Migration procedure documentation</li> <li>Rollback and recovery procedures</li> <li>Pilot migration plan</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-3-implementation-phase-weeks-37-50","title":"Phase 3: Implementation Phase (Weeks 37-50)","text":"<p>Duration: 14 weeks Effort: 5-7 FTEs Dependency: Infrastructure Testing 100% complete</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_12","title":"Milestones","text":"<ul> <li> Week 39: Pilot migration completed</li> <li> Week 42: Wave 1 migrations finished</li> <li> Week 46: Wave 2 migrations completed</li> <li> Week 49: Wave 3 migrations finished</li> <li> Week 50: All critical workloads migrated</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_12","title":"Deliverables","text":"<ul> <li>Migrated virtual machines and applications</li> <li>Updated VM templates and configurations</li> <li>Migration execution reports</li> <li>Performance optimization results</li> <li>Updated network and security configurations</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-4-testing-phase-weeks-51-54","title":"Phase 4: Testing Phase (Weeks 51-54)","text":"<p>Duration: 4 weeks Effort: 4-5 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_13","title":"Milestones","text":"<ul> <li> Week 52: Application functionality validated</li> <li> Week 53: Performance benchmarks achieved</li> <li> Week 54: Business continuity confirmed</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_13","title":"Deliverables","text":"<ul> <li>Application validation reports</li> <li>Performance comparison analysis</li> <li>Business continuity test results</li> <li>User acceptance confirmation</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#phase-5-day-2-operations-weeks-55-60","title":"Phase 5: Day-2 Operations (Weeks 55-60)","text":"<p>Duration: 6 weeks Effort: 3-4 FTEs</p>"},{"location":"project-plan/detailed-project-timeline/#milestones_14","title":"Milestones","text":"<ul> <li> Week 57: Migration runbooks completed</li> <li> Week 58: Legacy decommissioning planned</li> <li> Week 60: Operations transition complete</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#deliverables_14","title":"Deliverables","text":"<ul> <li>Migration operations runbooks</li> <li>Legacy infrastructure decommissioning plan</li> <li>Post-migration monitoring procedures</li> <li>Lessons learned documentation</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#resource-planning","title":"Resource Planning","text":""},{"location":"project-plan/detailed-project-timeline/#team-composition-by-sub-project","title":"Team Composition by Sub-Project","text":""},{"location":"project-plan/detailed-project-timeline/#infrastructure-project","title":"Infrastructure Project","text":"<ul> <li>Infrastructure Architect: 1 FTE</li> <li>DevOps Engineers: 2 FTEs</li> <li>System Administrators: 2 FTEs</li> <li>Security Engineer: 1 FTE</li> <li>Network Engineer: 1 FTE</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#use-cases-implementation","title":"Use-Cases Implementation","text":"<ul> <li>Solution Architect: 1 FTE</li> <li>Application Developers: 3 FTEs</li> <li>Testing Specialist: 1 FTE</li> <li>DevOps Engineer: 1 FTE</li> <li>Security Specialist: 1 FTE</li> <li>Business Analyst: 1 FTE</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#migration-project","title":"Migration Project","text":"<ul> <li>Migration Specialist: 1 FTE</li> <li>VMware Administrators: 2 FTEs</li> <li>RH OVE Engineers: 2 FTEs</li> <li>Application Owners: 2 FTEs</li> <li>Performance Engineer: 1 FTE</li> <li>Backup Administrator: 1 FTE</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#cross-project-coordination","title":"Cross-Project Coordination","text":""},{"location":"project-plan/detailed-project-timeline/#weekly-sync-meetings","title":"Weekly Sync Meetings","text":"<ul> <li>Architecture review board</li> <li>Technical coordination committee</li> <li>Risk and issue management</li> <li>Resource allocation review</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#monthly-steering-committee","title":"Monthly Steering Committee","text":"<ul> <li>Progress against milestones</li> <li>Budget and resource adjustments</li> <li>Risk escalation and mitigation</li> <li>Stakeholder communication</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#risk-management","title":"Risk Management","text":""},{"location":"project-plan/detailed-project-timeline/#high-risk-areas","title":"High-Risk Areas","text":"<ol> <li>Resource Availability: Specialized skills in RH OVE and VMware</li> <li>Technical Complexity: Multi-cluster networking and storage</li> <li>Business Continuity: Migration impact on critical workloads</li> <li>Timeline Dependencies: Sequential phases with limited parallel execution</li> </ol>"},{"location":"project-plan/detailed-project-timeline/#mitigation-strategies","title":"Mitigation Strategies","text":"<ol> <li>Cross-training: Develop skills across multiple team members</li> <li>Proof of Concepts: Validate technical approaches early</li> <li>Phased Migration: Minimize business impact through careful wave planning</li> <li>Buffer Time: Include 15-20% contingency in timeline estimates</li> </ol>"},{"location":"project-plan/detailed-project-timeline/#success-metrics","title":"Success Metrics","text":""},{"location":"project-plan/detailed-project-timeline/#infrastructure-project_1","title":"Infrastructure Project","text":"<ul> <li>99.9% infrastructure uptime SLA</li> <li>Security compliance audit pass rate: 100%</li> <li>Performance benchmarks met or exceeded</li> <li>Team satisfaction with operational procedures</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#use-cases-project","title":"Use-Cases Project","text":"<ul> <li>All documented use-cases successfully demonstrated</li> <li>Business value metrics achieved</li> <li>User adoption rates meet targets</li> <li>Documentation completeness and accuracy</li> </ul>"},{"location":"project-plan/detailed-project-timeline/#migration-project_1","title":"Migration Project","text":"<ul> <li>100% critical workload migration success</li> <li>Performance parity or improvement: 95% of workloads</li> <li>Business continuity maintained throughout migration</li> <li>Legacy infrastructure decommissioning completed on schedule</li> </ul>"},{"location":"project-plan/migration-workload-project-plan/","title":"Migration Workload from VMware Project Plan","text":""},{"location":"project-plan/migration-workload-project-plan/#executive-summary","title":"Executive Summary","text":"<p>This sub-project focuses on the migration of existing workloads from VMware environments to the RH OVE platform. The migration will ensure minimal downtime and optimal performance while maintaining functionality and business continuity.</p>"},{"location":"project-plan/migration-workload-project-plan/#scope","title":"Scope","text":"<p>Plan and execute the migration of workloads from VMware environments to RH OVE, including assessment of current workloads, migration strategy development, testing, and operation of migrated components.</p>"},{"location":"project-plan/migration-workload-project-plan/#documentation-areas","title":"Documentation Areas","text":"<ul> <li>VM Lifecycle: VM importation, template management, scaling performance, and backup recovery strategies.</li> <li>Migration-Specific ADRs: Strategy decisions, compatibility considerations, and performance optimization.</li> <li>Migration Planning: Assessment procedures, migration waves, risk mitigation, and rollback procedures.</li> </ul>"},{"location":"project-plan/migration-workload-project-plan/#work-phases","title":"Work Phases","text":"<ol> <li>Study Phase</li> <li>Inventory and analyze existing VMware workloads and their requirements.</li> <li> <p>Identify dependencies and performance baselines.</p> </li> <li> <p>Design Phase</p> </li> <li>Create migration strategies with comprehensive HLD and LLD documentation.</li> <li> <p>Define migration waves and prioritization.</p> </li> <li> <p>Implementation Phase</p> </li> <li>Execute migration workflows.</li> <li> <p>Import VMs and configure templates in the new environment.</p> </li> <li> <p>Testing Phase</p> </li> <li>Validate workload functionality and performance post-migration.</li> <li> <p>Ensure compliance with business requirements.</p> </li> <li> <p>Day-2 Operations Phase</p> </li> <li>Develop monitoring, backup, and disaster recovery procedures for migrated workloads.</li> <li>Document lessons learned and operational guidelines.</li> </ol>"},{"location":"project-plan/migration-workload-project-plan/#required-personas","title":"Required Personas","text":"<ul> <li>Migration Specialist: Lead migration strategy and execution.</li> <li>VMware Administrator: Provide expertise on source environment and legacy systems.</li> <li>RH OVE Engineer: Implement target environment configurations and optimization.</li> <li>Application Owner: Validate business functionality post-migration.</li> <li>Performance Engineer: Ensure performance requirements are met in the new environment.</li> <li>Backup Administrator: Implement backup and recovery procedures for migrated workloads.</li> </ul>"},{"location":"project-plan/migration-workload-project-plan/#success-criteria","title":"Success Criteria","text":"<ul> <li>Successful migration of all critical workloads with minimal disruption to business operations.</li> <li>Achievement of performance parity or improvement post-migration.</li> <li>Complete decommissioning of legacy VMware infrastructure where applicable.</li> <li>Comprehensive documentation of migration procedures and operational guidelines.</li> </ul>"},{"location":"project-plan/personas/","title":"Skills Matrix","text":"<p>This table outlines the key skills and expertise required for each persona involved in the RH OVE project.</p> Persona Key Skills and Expertise Infrastructure Architect Cloud architecture, multi-cluster systems, strategic planning DevOps Engineer CI/CD pipelines, automation tools, GitOps, ArgoCD System Administrator Unix/Linux systems, network configurations, troubleshooting Security Engineer Security protocols, penetration testing, compliance standards Network Engineer Network topology, load balancing, disaster recovery Solution Architect Use-case design, systems integration, architectural patterns Application Developer Programming languages (Python, Java), application frameworks, microservices Testing Specialist Test automation, QA processes, performance testing Security Specialist Application security, vulnerability assessments, security audits Business Analyst Requirements gathering, stakeholder communication, problem-solving Migration Specialist Migration strategies, VMware expertise, project coordination VMware Administrator VMware infrastructure, virtualization techniques, backup/recovery RH OVE Engineer RH OVE management, infrastructure optimization, performance tuning Application Owner Business requirements alignment, post-migration validation Performance Engineer Performance analysis, system benchmarking, tuning Backup Administrator Backup management, data integrity, recovery processes"},{"location":"project-plan/personas/#persona-perspectives","title":"Persona Perspectives","text":""},{"location":"project-plan/personas/#infrastructure-project","title":"Infrastructure Project","text":"<p>Infrastructure Architect - Defines the overarching architecture and integration patterns. - Leads the High-Level and Low-Level design phases. - Ensures strategic alignment with business objectives.</p> <p>DevOps Engineer - Implements automation and CI/CD pipelines. - Oversees the setup of GitOps with ArgoCD. - Manages ongoing system optimization and updates.</p> <p>System Administrator - Manages the day-to-day operations. - Ensures system stability and performance. - Handles system troubleshooting and maintenance.</p> <p>Security Engineer - Implements security protocols and compliance measures. - Conducts regular security audits and vulnerability assessments. - Designs and implements IAM policies.</p> <p>Network Engineer - Designs network architecture and policies. - Oversees network configuration and optimization. - Ensures robust connectivity and disaster recovery protocols.</p>"},{"location":"project-plan/personas/#use-cases-implementation","title":"Use-Cases Implementation","text":"<p>Solution Architect - Guides architectural design for use-cases. - Aligns use-case requirements with system capabilities. - Ensures solution scalability and performance efficiency.</p> <p>Application Developer - Develops application components as per designed specifications. - Implements integration logic and application workflows. - Collaborates on code reviews and deployment processes.</p> <p>Testing Specialist - Develops testing strategies for functional and integration testing. - Executes test cases and validates use-case functionality. - Provides recommendations for performance enhancements.</p> <p>Security Specialist - Designs use-case-specific security measures. - Ensures compliance with relevant legislation and policies. - Conducts security drills and audits.</p> <p>Business Analyst - Facilitates requirements gathering and prioritization. - Defines business value of implemented use-cases. - Liaises between technical teams and business stakeholders.</p>"},{"location":"project-plan/personas/#migration-workload-from-vmware","title":"Migration Workload from VMware","text":"<p>Migration Specialist - Leads the strategy and execution of migration activities. - Coordinates migration phases including assessment and validation.</p> <p>VMware Administrator - Provides expertise on source VMware environments. - Ensures correct configuration and performance tuning.</p> <p>RH OVE Engineer - Manages RH OVE platform setup and optimization post-migration. - Supports both the development and operation phases.</p> <p>Application Owner - Represents application requirements during migration. - Validates business functionality and performance post-migration.</p> <p>Performance Engineer - Assesses and optimizes performance during and after migration. - Establishes performance benchmarks and monitoring thresholds.</p> <p>Backup Administrator - Manages backup and recovery strategies during migration. - Ensures data integrity and availability throughout the process.</p>"},{"location":"project-plan/raci-matrix/","title":"RH OVE Ecosystem RACI Matrix","text":""},{"location":"project-plan/raci-matrix/#executive-summary","title":"Executive Summary","text":"<p>The RACI matrix defines the roles and responsibilities of team members across different phases and activities within each sub-project of the RH OVE Ecosystem. </p>"},{"location":"project-plan/raci-matrix/#raci-guide","title":"RACI Guide","text":"<ul> <li>R (Responsible): Person(s) who perform the work</li> <li>A (Accountable): Person who ensures task completion and has decision authority</li> <li>C (Consulted): Person(s) who provide input and feedback</li> <li>I (Informed): Person(s) who need to be informed of progress and outcomes</li> </ul>"},{"location":"project-plan/raci-matrix/#rh-ove-infrastructure-project","title":"RH OVE Infrastructure Project","text":"Task Infrastructure Architect DevOps Engineer System Administrator Security Engineer Network Engineer Requirements Gathering R I I C C HLD &amp; LLD Design A C C C C Cluster Deployment C A, R R I C Network Configuration I C I C A, R Security Implementation C C C A, R I GitOps Pipeline Setup C A, R C I I System Monitoring Setup I C A, R C C"},{"location":"project-plan/raci-matrix/#use-cases-implementation-project","title":"Use-Cases Implementation Project","text":"Task Solution Architect Application Developer Testing Specialist DevOps Engineer Security Specialist Business Analyst Use-Case Requirements Analysis R, A I I C I C HLD &amp; LLD for Use-Cases A C C C C I Application Deployment C A, R I C C I Integration Development C A, R C R C I Functional Testing I C A, R C C I Security Compliance C C C C A, R I Stakeholder Review A I C I I R"},{"location":"project-plan/raci-matrix/#migration-workload-from-vmware-project","title":"Migration Workload from VMware Project","text":"Task Migration Specialist VMware Administrator RH OVE Engineer Application Owner Performance Engineer Backup Administrator Inventory &amp; Analysis R, A R C C I I Migration Strategy Development A C C C C C Migration Execution C C A, R I C C Performance Testing C C C C A, R I Rollback &amp; Recovery Planning C C A C C R Business Continuity Verification C C C R, A C I Post-Migration Monitoring C I A, R I C C"},{"location":"project-plan/raci-matrix/#conclusion","title":"Conclusion","text":"<p>The RACI matrix provides a structured overview of responsibilities and accountabilities across the RH OVE Ecosystem implementation. It ensures clear communication and role clarity, contributing to project success.</p>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/","title":"RH OVE Ecosystem Project Plan","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#executive-summary","title":"Executive Summary","text":"<p>This document outlines a comprehensive project plan for implementing the Red Hat OpenShift Virtualization Engine (RH OVE) Multi-Cluster Ecosystem. The project is divided into three strategic sub-projects, each focusing on specific aspects of the ecosystem implementation and operation.</p>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#project-structure-overview","title":"Project Structure Overview","text":"<p>The RH OVE Ecosystem implementation is organized into three complementary sub-projects:</p> <ol> <li>RH OVE Infrastructure Project - Core platform setup and operations</li> <li>Use-Cases Implementation Project - Application scenarios and demonstrations</li> <li>Migration Workload from VMware Project - Legacy workload migration</li> </ol>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-1-rh-ove-infrastructure-project","title":"Sub-Project 1: RH OVE Infrastructure Project","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#scope","title":"Scope","text":"<p>Focus on the complete lifecycle of infrastructure setup and operations, including study, High-Level Design (HLD), Low-Level Design (LLD), implementation, testing, and Day-2 operations for the core RH OVE multi-cluster infrastructure components.</p>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#documentation-areas-covered","title":"Documentation Areas Covered","text":"<ul> <li>Architecture: </li> <li>Global Overview (<code>global-overview.md</code>)</li> <li>Single Cluster Overview (<code>overview.md</code>)</li> <li>Design Principles (<code>design-principles.md</code>)</li> <li>Network Architecture (<code>network.md</code>)</li> <li>Storage Architecture (<code>storage.md</code>)</li> <li> <p>IAM Strategy (<code>iam.md</code>)</p> </li> <li> <p>Architecture Decision Records (ADRs):</p> </li> <li>Multi-cluster patterns</li> <li>GitOps with ArgoCD</li> <li>Cluster topology</li> <li>Admission control strategies</li> <li>Network CNI implementation</li> <li>Backup strategies</li> <li>Monitoring approaches</li> <li> <p>IAM implementation</p> </li> <li> <p>Deployment:</p> </li> <li>Prerequisites (<code>prerequisites.md</code>)</li> <li>Installation procedures (<code>installation.md</code>)</li> <li> <p>Configuration management (<code>configuration.md</code>)</p> </li> <li> <p>Management:</p> </li> <li>Admission control (<code>admission-control.md</code>)</li> <li>GitOps operations (<code>gitops.md</code>)</li> <li>Monitoring systems (<code>monitoring.md</code>)</li> <li> <p>Backup and recovery (<code>backup.md</code>)</p> </li> <li> <p>Operations:</p> </li> <li>Day-2 operations (<code>day2-ops.md</code>)</li> <li>Troubleshooting procedures (<code>troubleshooting.md</code>)</li> <li>Performance tuning (<code>performance.md</code>)</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#work-phases","title":"Work Phases","text":"<ol> <li>Study Phase: Detailed understanding of RH OVE architecture and components</li> <li>Design Phase: Create and validate HLD and LLD documents</li> <li>Implementation Phase: Deploy infrastructure components following established practices</li> <li>Testing Phase: Verify infrastructure robustness, security, and performance</li> <li>Day-2 Operations Phase: Establish ongoing monitoring, troubleshooting, and tuning</li> </ol>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#required-personas","title":"Required Personas","text":"<ul> <li>Infrastructure Architect: Designs overall infrastructure architecture and integration patterns</li> <li>DevOps Engineer: Implements automation, CI/CD pipelines, and GitOps workflows</li> <li>System Administrator: Manages day-to-day operations, monitoring, and maintenance</li> <li>Security Engineer: Ensures security compliance and implements security controls</li> <li>Network Engineer: Designs and implements network architecture and policies</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-2-use-cases-implementation-project","title":"Sub-Project 2: Use-Cases Implementation Project","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#scope_1","title":"Scope","text":"<p>Study, design, implement, test, and operate comprehensive use-cases demonstrating RH OVE capabilities for applications and services. Each use-case highlights unique features and integrations within RH OVE, showcasing multi-cluster deployment benefits.</p>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#documentation-areas-covered_1","title":"Documentation Areas Covered","text":"<ul> <li>Use Cases Overview: Comprehensive introduction to all use-cases, relevance, and expected outcomes</li> <li>VM Lifecycle Management:</li> <li>VM Import &amp; Migration (<code>vm-importation.md</code>)</li> <li>VM Template Management (<code>vm-template-management.md</code>)</li> <li>VM Scaling &amp; Performance (<code>vm-scaling-performance.md</code>)</li> <li>VM Backup &amp; Recovery (<code>vm-backup-recovery.md</code>)</li> <li>Application Deployment:</li> <li>Hybrid Applications (<code>hybrid-applications.md</code>) - Integration of legacy VMs, containers, and microservices</li> <li>Multi-Environment Setup (<code>setup-multi-env-application.md</code>) - Dev, staging, and production environments</li> <li>PaaS Integration:</li> <li>Database Services (<code>database-services-paas.md</code>) - Multi-cloud deployment with automated backups</li> <li>Enterprise Integration:</li> <li>Legacy Modernization (<code>legacy-modernization.md</code>) - Containerization and orchestration strategies</li> <li>Disaster Recovery (<code>disaster-recovery.md</code>) - Comprehensive DR plans and procedures</li> <li>Summary Table:</li> <li>Use-Cases Summary (<code>use-cases-table.md</code>) - Provides an overview of all use-cases</li> <li>Observability:</li> <li>End-to-End Observability (<code>end-to-end-observability.md</code>) - Monitoring, tracing, and logging</li> <li>Security:</li> <li>WAF \\u0026 Firewalling (<code>waf-firewalling.md</code>) - Security controls and compliance</li> <li>Integration:</li> <li>Events to CMDB/SIEM (<code>publishing-events-to-cmdb-siem.md</code>) - Enterprise integration patterns</li> <li>References: Best practices and glossary</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#work-phases_1","title":"Work Phases","text":"<ol> <li>Study Phase: Gather requirements and analyze use-case scenarios specific to organizational goals</li> <li>Design Phase: Develop HLD and LLD for each use-case, ensuring alignment with strategic objectives</li> <li>Implementation Phase: Configure infrastructure and deploy components, including pipelines</li> <li>Testing Phase: Perform functional and integration testing against business requirements</li> <li>Day-2 Operations Phase: Create runbooks and SOPs for ongoing support and improvement</li> </ol>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#required-personas_1","title":"Required Personas","text":"<ul> <li>Solution Architect: Designs use-case architecture and ensures business alignment</li> <li>Application Developer: Implements code and configuration changes for each use-case</li> <li>Testing Specialist: Conducts rigorous testing to validate functionality</li> <li>DevOps Engineer: Automates deployment and integration processes</li> <li>Security Specialist: Ensures compliance and security measures</li> <li>Business Analyst: Defines requirements and validates business outcomes</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-3-migration-workload-from-vmware-project","title":"Sub-Project 3: Migration Workload from VMware Project","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#scope_2","title":"Scope","text":"<p>Plan and execute migration of workloads from VMware environments to RH OVE, including study of current workloads, design of migration strategy, implementation, testing, and operation of migrated workloads.</p>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#documentation-areas-covered_2","title":"Documentation Areas Covered","text":"<ul> <li>VM Lifecycle Specific Documentation:</li> <li>VM importation procedures and best practices</li> <li>VM template management and standardization</li> <li>VM scaling and performance optimization</li> <li>VM backup and recovery strategies</li> <li>Migration-Specific ADRs:</li> <li>Migration strategy decisions</li> <li>Compatibility and interoperability considerations</li> <li>Performance and resource optimization</li> <li>Migration Planning:</li> <li>Assessment and inventory procedures</li> <li>Migration waves and prioritization</li> <li>Risk mitigation strategies</li> <li>Rollback procedures</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#work-phases_2","title":"Work Phases","text":"<ol> <li>Study Phase: Inventory and analyze existing VMware workloads and requirements</li> <li>Design Phase: Create migration strategies with comprehensive HLD and LLD documentation</li> <li>Implementation Phase: Execute migration workflows, import VMs, configure templates</li> <li>Testing Phase: Validate workload functionality and performance post-migration</li> <li>Day-2 Operations Phase: Develop monitoring, backup, and disaster recovery procedures</li> </ol>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#required-personas_2","title":"Required Personas","text":"<ul> <li>Migration Specialist: Leads migration strategy and execution</li> <li>VMware Administrator: Provides expertise on source environment</li> <li>RH OVE Engineer: Implements target environment configurations</li> <li>Application Owner: Validates business functionality post-migration</li> <li>Performance Engineer: Ensures performance requirements are met</li> <li>Backup Administrator: Implements backup and recovery procedures</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#project-dependencies-and-integration-points","title":"Project Dependencies and Integration Points","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#inter-project-dependencies","title":"Inter-Project Dependencies","text":"<ol> <li>Infrastructure \u2192 Use-Cases: Core infrastructure must be operational before use-case implementation</li> <li>Infrastructure \u2192 Migration: Target infrastructure must be ready before migration activities</li> <li>Use-Cases \u2194 Migration: Some use-cases may serve as migration validation scenarios</li> </ol>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#shared-resources","title":"Shared Resources","text":"<ul> <li>Documentation Standards: Common templates and style guides</li> <li>Testing Framework: Shared testing methodologies and tools</li> <li>Monitoring and Observability: Common monitoring stack across all projects</li> <li>Security Policies: Unified security standards and compliance requirements</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#success-criteria","title":"Success Criteria","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-1-infrastructure","title":"Sub-Project 1: Infrastructure","text":"<ul> <li>\u2705 Multi-cluster RH OVE environment deployed and operational</li> <li>\u2705 All ADRs implemented and validated</li> <li>\u2705 Monitoring and alerting systems operational</li> <li>\u2705 Backup and disaster recovery procedures tested</li> <li>\u2705 Day-2 operations runbooks completed and validated</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-2-use-cases","title":"Sub-Project 2: Use-Cases","text":"<ul> <li>\u2705 All documented use-cases successfully implemented</li> <li>\u2705 Use-cases demonstrate business value and ROI</li> <li>\u2705 Integration patterns validated and documented</li> <li>\u2705 Performance benchmarks established</li> <li>\u2705 Operational procedures for each use-case documented</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#sub-project-3-migration","title":"Sub-Project 3: Migration","text":"<ul> <li>\u2705 VMware workload inventory completed</li> <li>\u2705 Migration strategy validated through pilot migrations</li> <li>\u2705 Production workloads successfully migrated</li> <li>\u2705 Performance parity or improvement achieved</li> <li>\u2705 Decommissioning of legacy VMware infrastructure completed</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"project-plan/rh-ove-ecosystem-project-plan/#technical-risks","title":"Technical Risks","text":"<ul> <li>Complexity Management: Break down into smaller, manageable phases</li> <li>Integration Challenges: Early validation of integration points</li> <li>Performance Issues: Establish baseline metrics and continuous monitoring</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#operational-risks","title":"Operational Risks","text":"<ul> <li>Skill Gaps: Training and knowledge transfer programs</li> <li>Resource Constraints: Phased approach with clear prioritization</li> <li>Change Management: Stakeholder alignment and communication plan</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#business-risks","title":"Business Risks","text":"<ul> <li>Timeline Delays: Buffer time and parallel execution where possible</li> <li>Budget Overruns: Regular cost monitoring and optimization</li> <li>Business Continuity: Comprehensive testing and rollback procedures</li> </ul>"},{"location":"project-plan/rh-ove-ecosystem-project-plan/#conclusion","title":"Conclusion","text":"<p>This three-pronged approach ensures comprehensive implementation of the RH OVE ecosystem while maintaining focus on specific domains of expertise. Each sub-project can be executed with dedicated teams while maintaining coordination points for integration and shared components.</p> <p>The phased approach within each sub-project allows for iterative improvement and validation, reducing overall project risk while ensuring alignment with business objectives and operational requirements.</p>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/","title":"RH OVE Infrastructure Project Plan","text":""},{"location":"project-plan/rh-ove-infrastructure-project-plan/#executive-summary","title":"Executive Summary","text":"<p>This sub-project focuses on the comprehensive setup and operation of the RH OVE infrastructure, targeting the study, design, implementation, testing, and Day-2 operations of the multi-cluster environment.</p>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/#scope","title":"Scope","text":"<p>Ensure the complete lifecycle of infrastructure operation, from study to implementation and ongoing support. This includes designing and deploying core components, establishing monitoring and security protocols, and documenting operational procedures.</p>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/#documentation-areas","title":"Documentation Areas","text":"<ul> <li>Architecture: Review and define the network, storage, and IAM strategies.</li> <li>ADRs (Architecture Decision Records): Finalize key decisions including multi-cluster patterns and GitOps.</li> <li>Deployment: Identify prerequisites, and cover installation and configuration tasks.</li> <li>Management: Focus on admission control, GitOps, monitoring, and backup strategies.</li> <li>Operations: Develop detailed guidance for day-to-day operations, troubleshooting, and performance tuning.</li> </ul>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/#work-phases","title":"Work Phases","text":"<ol> <li>Study Phase</li> <li>Gather comprehensive understanding of current and future needs.</li> <li> <p>Perform gap analysis on existing infrastructure.</p> </li> <li> <p>Design Phase</p> </li> <li>Create High-Level Design (HLD) and Low-Level Design (LLD) documents.</li> <li> <p>Validate designs against organizational goals.</p> </li> <li> <p>Implementation Phase</p> </li> <li>Deploy infrastructure components utilizing best practices.</li> <li> <p>Integrate security, monitoring, and backup systems.</p> </li> <li> <p>Testing Phase</p> </li> <li>Conduct thorough infrastructure robustness, security, and performance tests.</li> <li> <p>Validate alignment with ADRs.</p> </li> <li> <p>Day-2 Operations Phase</p> </li> <li>Establish ongoing monitoring.</li> <li>Document troubleshooting and tuning procedures.</li> </ol>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/#required-personas","title":"Required Personas","text":"<ul> <li>Infrastructure Architect: Define overarching infrastructure architecture.</li> <li>DevOps Engineer: Implement and automate delivery using CI/CD and GitOps.</li> <li>System Administrator: Oversee operations, ensuring stability and performance.</li> <li>Security Engineer: Ensure alignment with security protocols and compliance.</li> <li>Network Engineer: Design and implement network components and policies.</li> </ul>"},{"location":"project-plan/rh-ove-infrastructure-project-plan/#success-criteria","title":"Success Criteria","text":"<ul> <li>Successful deployment of a robust, secure, and scalable infrastructure that supports RH OVE multi-cluster functionality.</li> <li>Full documentation of operational guidelines and procedures.</li> <li>Stakeholder satisfaction with alignment to strategic goals.</li> </ul>"},{"location":"project-plan/risk-register/","title":"RH OVE Ecosystem Risk Register","text":""},{"location":"project-plan/risk-register/#document-information","title":"Document Information","text":"<ul> <li>Document Version: 1.0</li> <li>Last Updated: TBD</li> <li>Risk Assessment Period: Project Duration (12-18 months)</li> </ul>"},{"location":"project-plan/risk-register/#risk-assessment-scale","title":"Risk Assessment Scale","text":""},{"location":"project-plan/risk-register/#probability-scale","title":"Probability Scale","text":"<ul> <li>1 - Very Low (0-10%): Risk unlikely to occur</li> <li>2 - Low (11-30%): Risk may occur in exceptional circumstances</li> <li>3 - Medium (31-50%): Risk may occur under certain conditions</li> <li>4 - High (51-80%): Risk likely to occur in most circumstances</li> <li>5 - Very High (81-100%): Risk almost certain to occur</li> </ul>"},{"location":"project-plan/risk-register/#impact-scale","title":"Impact Scale","text":"<ul> <li>1 - Very Low: Minimal impact on project objectives</li> <li>2 - Low: Minor impact with easy workarounds</li> <li>3 - Medium: Moderate impact requiring management attention</li> <li>4 - High: Major impact requiring significant resources</li> <li>5 - Very High: Critical impact threatening project success</li> </ul>"},{"location":"project-plan/risk-register/#risk-score-probability-impact","title":"Risk Score = Probability \u00d7 Impact","text":""},{"location":"project-plan/risk-register/#infrastructure-project-risks","title":"Infrastructure Project Risks","text":"Risk ID Risk Description Category Probability Impact Risk Score Owner Mitigation Strategy Status INF-001 Resource contention with 2,000 VMs and 200 applications exceeding cluster capacity Technical 3 4 12 Infrastructure Architect Implement auto-scaling, capacity monitoring, and multi-cluster load balancing Open INF-002 Network bottlenecks during peak usage with 40 Gbps requirement Technical 3 4 12 Network Engineer Design redundant network paths, implement QoS policies, traffic shaping Open INF-003 Storage IOPS degradation with 200,000 IOPS requirement Technical 3 3 9 System Administrator Implement tiered storage, SSD caching, performance monitoring Open INF-004 Cilium CNI configuration complexity in multi-cluster setup Technical 4 3 12 DevOps Engineer PoC validation, expert consultation, phased rollout Open INF-005 Security vulnerabilities in multi-tenant environment Security 2 5 10 Security Engineer Regular security audits, penetration testing, network segmentation Open INF-006 GitOps pipeline failures affecting deployment automation Operational 3 3 9 DevOps Engineer Implement pipeline monitoring, rollback procedures, backup deployment methods Open INF-007 Backup system (Rubrik) integration issues with large dataset (1 PB) Technical 3 4 12 System Administrator Extensive testing, vendor support engagement, backup strategy validation Open INF-008 Skills gap in RH OVE administration and troubleshooting Resource 4 3 12 Project Manager Training programs, knowledge transfer sessions, external consultant support Open INF-009 Hardware procurement delays affecting project timeline External 3 4 12 Infrastructure Architect Early procurement planning, multiple vendor options, buffer time Open INF-010 Monitoring system (Prometheus/Grafana) overload with 2,200 workloads Technical 3 3 9 System Administrator Distributed monitoring architecture, metric sampling, alert optimization Open"},{"location":"project-plan/risk-register/#use-cases-implementation-risks","title":"Use-Cases Implementation Risks","text":"Risk ID Risk Description Category Probability Impact Risk Score Owner Mitigation Strategy Status UC-001 Application compatibility issues with RH OVE platform (200 hybrid apps) Technical 3 4 12 Solution Architect Compatibility assessment, application inventory, testing framework Open UC-002 Performance degradation in hybrid workload scenarios Technical 3 3 9 Performance Engineer Performance benchmarking, optimization guidelines, monitoring dashboards Open UC-003 Integration complexity between containers, PaaS, and VMs Technical 4 3 12 Solution Architect Integration patterns, service mesh implementation, API gateway setup Open UC-004 Business stakeholder availability for use-case validation Business 3 3 9 Business Analyst Early stakeholder engagement, flexible scheduling, clear communication plan Open UC-005 Use-case dependencies causing implementation delays Operational 3 3 9 Project Manager Dependency mapping, parallel development tracks, modular implementation Open UC-006 Security compliance requirements for PaaS services Security 2 4 8 Security Specialist Compliance framework development, security controls validation, audit preparation Open UC-007 Data consistency issues in hybrid application scenarios Technical 2 4 8 Application Developer Data architecture design, consistency protocols, transaction management Open UC-008 Observability gaps in complex multi-tier applications Operational 3 3 9 DevOps Engineer Distributed tracing implementation, log aggregation, custom metrics Open"},{"location":"project-plan/risk-register/#migration-project-risks","title":"Migration Project Risks","text":"Risk ID Risk Description Category Probability Impact Risk Score Owner Mitigation Strategy Status MIG-001 Data loss during migration of 500 TB across 1,000 VMs Technical 2 5 10 Migration Specialist Multiple backup strategies, checksum validation, pilot testing Open MIG-002 Extended downtime exceeding 2-hour window per application Business 3 4 12 Migration Specialist Rehearsal migrations, optimized procedures, rollback planning Open MIG-003 VMware licensing and dependency issues during transition Legal/Technical 3 3 9 VMware Administrator License audit, dependency mapping, phased decommissioning Open MIG-004 Application performance degradation post-migration Technical 3 4 12 Performance Engineer Performance baselines, optimization procedures, monitoring setup Open MIG-005 Network connectivity issues during migration waves Technical 3 3 9 Network Engineer Network planning, connectivity testing, backup communication paths Open MIG-006 Resource contention during concurrent migrations (200 VMs per wave) Technical 4 3 12 RH OVE Engineer Resource planning, migration scheduling, capacity monitoring Open MIG-007 Legacy application compatibility with new infrastructure Technical 4 4 16 Application Owner Compatibility testing, application modernization planning, fallback options Open MIG-008 Staff resistance to new platform and procedures Change Management 3 3 9 Project Manager Change management program, training, communication strategy Open MIG-009 Rollback complexity if migration fails Operational 2 4 8 Migration Specialist Detailed rollback procedures, automated rollback tools, testing protocols Open MIG-010 Compliance and audit trail maintenance during migration Regulatory 2 4 8 Backup Administrator Audit procedures, compliance documentation, regulatory liaison Open"},{"location":"project-plan/risk-register/#cross-project-risks","title":"Cross-Project Risks","text":"Risk ID Risk Description Category Probability Impact Risk Score Owner Mitigation Strategy Status CP-001 Resource conflicts between concurrent sub-projects Resource 4 3 12 Project Manager Resource allocation matrix, coordination meetings, escalation procedures Open CP-002 Timeline dependencies causing cascading delays Schedule 3 4 12 Project Manager Buffer time allocation, parallel execution planning, milestone monitoring Open CP-003 Technical debt accumulation due to rapid deployment Technical 3 3 9 Technical Architect Code reviews, refactoring cycles, technical debt tracking Open CP-004 Budget overruns due to scope creep and complexity Financial 3 4 12 Project Manager Budget monitoring, change control process, stakeholder communication Open CP-005 Knowledge silos limiting cross-team collaboration Organizational 3 3 9 Project Manager Knowledge sharing sessions, documentation standards, cross-training Open CP-006 Vendor support availability for critical issues External 2 4 8 Project Manager SLA agreements, escalation procedures, alternative support channels Open"},{"location":"project-plan/risk-register/#risk-monitoring-and-review","title":"Risk Monitoring and Review","text":""},{"location":"project-plan/risk-register/#weekly-risk-review","title":"Weekly Risk Review","text":"<ul> <li>Frequency: Every Tuesday during project execution</li> <li>Attendees: Risk owners, project managers, technical leads</li> <li>Agenda: New risks, status updates, mitigation progress</li> </ul>"},{"location":"project-plan/risk-register/#monthly-risk-assessment","title":"Monthly Risk Assessment","text":"<ul> <li>Frequency: First Friday of each month</li> <li>Attendees: Steering committee, project sponsors</li> <li>Agenda: Risk trend analysis, escalation decisions, resource adjustments</li> </ul>"},{"location":"project-plan/risk-register/#risk-escalation-criteria","title":"Risk Escalation Criteria","text":"<ul> <li>Immediate Escalation: Risk score \u2265 15 (High probability + High impact)</li> <li>Weekly Escalation: Risk score \u2265 12</li> <li>Monthly Review: Risk score \u2265 9</li> </ul>"},{"location":"project-plan/risk-register/#risk-response-strategies","title":"Risk Response Strategies","text":""},{"location":"project-plan/risk-register/#high-priority-risks-score-12","title":"High-Priority Risks (Score \u2265 12)","text":"<ol> <li>MIG-007: Legacy application compatibility - Requires immediate compatibility assessment</li> <li>CP-002: Timeline dependencies - Needs buffer time implementation</li> <li>INF-001: Resource contention - Auto-scaling implementation priority</li> <li>MIG-002: Extended downtime - Rehearsal migration mandatory</li> </ol>"},{"location":"project-plan/risk-register/#medium-priority-risks-score-9-11","title":"Medium-Priority Risks (Score 9-11)","text":"<ul> <li>Regular monitoring and mitigation progress tracking</li> <li>Monthly review of mitigation effectiveness</li> <li>Proactive communication with stakeholders</li> </ul>"},{"location":"project-plan/risk-register/#low-priority-risks-score-8","title":"Low-Priority Risks (Score \u2264 8)","text":"<ul> <li>Quarterly review and assessment</li> <li>Documentation of lessons learned</li> <li>Contingency planning development</li> </ul>"},{"location":"project-plan/risk-register/#success-metrics-for-risk-management","title":"Success Metrics for Risk Management","text":""},{"location":"project-plan/risk-register/#target-kpis","title":"Target KPIs","text":"<ul> <li>Risk Closure Rate: &gt;80% of identified risks mitigated by project completion</li> <li>Schedule Impact: &lt;5% schedule variance due to risk materialization</li> <li>Budget Impact: &lt;10% budget variance due to risk management costs</li> <li>Quality Impact: Zero critical system failures due to unmanaged risks</li> </ul>"},{"location":"project-plan/risk-register/#risk-management-effectiveness","title":"Risk Management Effectiveness","text":"<ul> <li>Early identification of 90% of project risks</li> <li>Successful mitigation of all high-priority risks</li> <li>No project-threatening risks materialized</li> <li>Stakeholder satisfaction with risk communication and management</li> </ul>"},{"location":"project-plan/sizing-plan/","title":"RH OVE Ecosystem Sizing Plan","text":""},{"location":"project-plan/sizing-plan/#executive-summary","title":"Executive Summary","text":"<p>This document outlines the sizing estimates for the RH OVE Ecosystem implementation. It addresses the expected capacity and complexity for each sub-project. The plan ensures that proper resource allocation and infrastructure setups are achieved to meet performance and scalability demands.</p>"},{"location":"project-plan/sizing-plan/#to-be-review-for-node-sizing","title":"** TO BE REVIEW FOR NODE SIZING **","text":""},{"location":"project-plan/sizing-plan/#infrastructure-sizing","title":"Infrastructure Sizing","text":"<p>Scope: Support up to 200 hybrid applications (containerized, PaaS, and VMs) and 2,000 virtual machines.</p>"},{"location":"project-plan/sizing-plan/#key-metrics","title":"Key Metrics","text":"<ul> <li>Application Types: Hybrid, combining containers, PaaS services, and virtual machines.</li> <li>Maximum Applications: 200</li> <li>Maximum Virtual Machines: 2,000</li> <li>Expected Compute Resources: </li> <li>CPUs: Approximately 10,000 vCPUs</li> <li>Memory: Approximately 20 TB RAM</li> <li>Storage: Approximately 1 PB (Petabyte)</li> <li>Network Capacity: High-throughput connectivity with redundant failover capabilities.</li> <li>Security and Compliance: Adherence to regulatory standards with IAM components and network policies.</li> </ul>"},{"location":"project-plan/sizing-plan/#resource-allocation","title":"Resource Allocation","text":""},{"location":"project-plan/sizing-plan/#node-size-options","title":"Node Size Options","text":"<p>To optimize resource allocation and cost efficiency, three node size configurations are proposed:</p>"},{"location":"project-plan/sizing-plan/#option-1-small-nodes-recommended-for-developmenttesting","title":"Option 1: Small Nodes (Recommended for Development/Testing)","text":"<ul> <li>Node Configuration: 16 vCPUs, 64 GB RAM, 1 TB NVMe SSD</li> <li>Number of Nodes: 625 nodes</li> <li>Use Case: Development environments, testing workloads, small applications</li> <li>Cost Efficiency: Lower initial investment, flexible scaling</li> <li>Pros: </li> <li>Lower hardware costs per node</li> <li>Better granular scaling</li> <li>Reduced blast radius for failures</li> <li>Cons: </li> <li>Higher management overhead</li> <li>More network complexity</li> </ul>"},{"location":"project-plan/sizing-plan/#option-2-medium-nodes-recommended-for-production","title":"Option 2: Medium Nodes (Recommended for Production)","text":"<ul> <li>Node Configuration: 32 vCPUs, 128 GB RAM, 2 TB NVMe SSD</li> <li>Number of Nodes: 313 nodes</li> <li>Use Case: Production workloads, hybrid applications, medium-scale VMs</li> <li>Cost Efficiency: Balanced performance and cost</li> <li>Pros: </li> <li>Optimal resource density</li> <li>Balanced management overhead</li> <li>Good performance isolation</li> <li>Cons: </li> <li>Higher individual node cost</li> <li>Less flexible for small workloads</li> </ul>"},{"location":"project-plan/sizing-plan/#option-3-large-nodes-recommended-for-high-performance-workloads","title":"Option 3: Large Nodes (Recommended for High-Performance Workloads)","text":"<ul> <li>Node Configuration: 64 vCPUs, 256 GB RAM, 4 TB NVMe SSD</li> <li>Number of Nodes: 157 nodes</li> <li>Use Case: High-performance applications, large VMs, compute-intensive workloads</li> <li>Cost Efficiency: Best performance per dollar for large workloads</li> <li>Pros: </li> <li>Maximum resource density</li> <li>Lower management overhead</li> <li>Best for large workloads</li> <li>Cons: </li> <li>Higher blast radius</li> <li>Less flexibility for smaller workloads</li> <li>Higher individual node investment</li> </ul>"},{"location":"project-plan/sizing-plan/#recommended-hybrid-approach","title":"Recommended Hybrid Approach","text":"<p>Distribution across clusters: - Management Cluster: 6 Medium nodes (dedicated for cluster management) - Production Clusters:    - 60% Medium nodes (188 nodes) - Primary production workloads   - 30% Large nodes (47 nodes) - High-performance applications   - 10% Small nodes (62 nodes) - Development and testing</p> <p>Total Node Count: 303 nodes Total Resources: ~10,000 vCPUs, ~20 TB RAM, ~600 TB Storage</p>"},{"location":"project-plan/sizing-plan/#network-and-storage-specifications","title":"Network and Storage Specifications","text":"<ul> <li>Network Bandwidth: Up to 40 Gbps per cluster</li> <li>Storage IOPS: Minimum 200,000 IOPS aggregate</li> <li>Network Architecture: </li> <li>25 Gbps per node connectivity</li> <li>Redundant spine-leaf topology</li> <li>Dedicated storage network (10 Gbps)</li> </ul>"},{"location":"project-plan/sizing-plan/#application-gabari-descriptions","title":"Application Gabari Descriptions","text":"<p>To ensure compatibility and optimal performance, applications are categorized based on typical resource demands and architectural patterns:</p>"},{"location":"project-plan/sizing-plan/#1-microservices-applications","title":"1. Microservices Applications","text":"<ul> <li>Configuration: Typically small, scalable units with minimal resource needs per instance (1-2 vCPUs, 2-4 GB RAM)</li> <li>Key Considerations: Designed for high scalability, containerized deployments, and stateless architecture</li> <li>Use Cases: Web services, REST APIs, lightweight backend services</li> </ul>"},{"location":"project-plan/sizing-plan/#2-monolithic-applications","title":"2. Monolithic Applications","text":"<ul> <li>Configuration: Larger resource footprint with robust processing needs (4-8 vCPUs, 16-32 GB RAM)</li> <li>Key Considerations: May not scale horizontally; benefits from vertical scaling</li> <li>Use Cases: Legacy applications, computational intensive tasks, single-platform systems</li> </ul>"},{"location":"project-plan/sizing-plan/#3-distributed-applications","title":"3. Distributed Applications","text":"<ul> <li>Configuration: Moderate resources per service, optimized for distributed workload (2-4 vCPUs, 8-16 GB RAM per node)</li> <li>Key Considerations: Requires synchronization across nodes, often benefits from microservices/design separation</li> <li>Use Cases: Databases, clustered applications, interconnected services</li> </ul>"},{"location":"project-plan/sizing-plan/#4-resource-intensive-applications","title":"4. Resource-Intensive Applications","text":"<ul> <li>Configuration: High-performance requirements, large scale of resources (8-16 vCPUs, 32-64 GB RAM)</li> <li>Key Considerations: Compute-intensive, may need specific hardware accelerators (e.g., GPUs)</li> <li>Use Cases: Data analytics, machine learning workloads, scientific computing </li> </ul>"},{"location":"project-plan/sizing-plan/#migration-sizing","title":"Migration Sizing","text":"<p>Scope: Plan for the migration of 1,000 virtual machines and 100 applications.</p>"},{"location":"project-plan/sizing-plan/#key-metrics_1","title":"Key Metrics","text":"<ul> <li>Virtual Machines: 1,000</li> <li>VM Types: Includes various OS types and legacy configurations</li> <li>Average VM size: 4 vCPUs, 16 GB RAM per VM</li> <li>Storage per VM: 500 GB</li> <li>Applications: 100</li> <li>Application Types: Legacy, modern monoliths, and distributed services</li> <li>Data Migration Volume: 500 TB</li> </ul>"},{"location":"project-plan/sizing-plan/#migration-planning","title":"Migration Planning","text":"<ul> <li>Migration Waves: 5 waves, 200 VMs + 20 Applications per wave</li> <li>Expected Downtime: Max 2 hours per application</li> <li>Risk Mitigation:</li> <li>Pilot migrations for high-risk workloads</li> <li>Rollback strategies for failed migrations</li> </ul>"},{"location":"project-plan/sizing-plan/#strategic-considerations","title":"Strategic Considerations","text":""},{"location":"project-plan/sizing-plan/#infrastructure","title":"Infrastructure","text":"<ol> <li>Scalability: Design to accommodate future growth up to 300 applications and 3,000 VMs.</li> <li>Redundancy: Implement failover and disaster recovery protocols.</li> <li>Monitoring and Logging: Comprehensive observability with real-time analytics.</li> </ol>"},{"location":"project-plan/sizing-plan/#migration","title":"Migration","text":"<ol> <li>Compatibility: Analyze application dependencies and compatibility early.</li> <li>Data Integrity: Ensure lossless data transfer methods.</li> <li>Operational Support: Equip teams with runbooks for migration phases.</li> </ol>"},{"location":"project-plan/sizing-plan/#appendices","title":"Appendices","text":""},{"location":"project-plan/sizing-plan/#sizing-assumptions","title":"Sizing Assumptions","text":"<ul> <li>Based on existing organizational usage patterns and vendor best practices.</li> </ul>"},{"location":"project-plan/sizing-plan/#dependencies","title":"Dependencies","text":"<ul> <li>Align sizing with strategic initiatives.</li> <li>Regular reviews to anticipate scaling needs and compliance demands.</li> </ul>"},{"location":"project-plan/sizing-plan/#risk-factors","title":"Risk Factors","text":"<ul> <li>Sizing models subject to change with evolving requirements and emerging technologies.</li> </ul>"},{"location":"project-plan/use-cases-implementation-project-plan/","title":"Use-Cases Implementation Project Plan","text":""},{"location":"project-plan/use-cases-implementation-project-plan/#executive-summary","title":"Executive Summary","text":"<p>This sub-project focuses on implementing and showcasing various use-cases within the RH OVE ecosystem. The use-cases demonstrate the system's capabilities in multi-cluster environments, covering diverse application scenarios.</p>"},{"location":"project-plan/use-cases-implementation-project-plan/#scope","title":"Scope","text":"<p>Study, design, implement, test, and operate comprehensive use-cases showcasing RH OVE capabilities, including hybrid applications, database services, legacy modernization, and more.</p>"},{"location":"project-plan/use-cases-implementation-project-plan/#documentation-areas","title":"Documentation Areas","text":"<ul> <li>Use Cases: Overview, multi-env setup, hybrid applications, database services, legacy modernization, disaster recovery, observability, security, integration.</li> <li>References: Best practices and glossary.</li> </ul>"},{"location":"project-plan/use-cases-implementation-project-plan/#work-phases","title":"Work Phases","text":"<ol> <li>Study Phase</li> <li> <p>Analyze use case requirements and their alignment with organizational goals.</p> </li> <li> <p>Design Phase</p> </li> <li>Develop High-Level and Low-Level Designs for each use case.</li> <li> <p>Ensure designs support strategic objectives.</p> </li> <li> <p>Implementation Phase</p> </li> <li> <p>Build and deploy configurations, manifests, and scripts for each use case.</p> </li> <li> <p>Testing Phase</p> </li> <li>Conduct functional and integration testing </li> <li> <p>Validate business outcomes and performance benchmarks.</p> </li> <li> <p>Day-2 Operations Phase</p> </li> <li>Develop runbooks and SOPs for operational support.</li> <li>Ensure ongoing integration and optimization.</li> </ol>"},{"location":"project-plan/use-cases-implementation-project-plan/#required-personas","title":"Required Personas","text":"<ul> <li>Solution Architect: Guide the architectural design of use-cases.</li> <li>Application Developer: Implement required code and configurations.</li> <li>Testing Specialist: Validate use-case functionality and integration.</li> <li>DevOps Engineer: Automate deployment processes.</li> <li>Security Specialist: Ensure security compliance and measures.</li> <li>Business Analyst: Define requirements and assess business value.</li> </ul>"},{"location":"project-plan/use-cases-implementation-project-plan/#success-criteria","title":"Success Criteria","text":"<ul> <li>Implementation of documented use-cases that align to strategic goals and demonstrate RH OVE's capabilities to stakeholders.</li> <li>Comprehensive documentation of each use-case, including lessons learned and best practices for replication.</li> </ul>"},{"location":"project-plan/weekly-charge-breakdown/","title":"Weekly Workload Breakdown by Persona and Project","text":""},{"location":"project-plan/weekly-charge-breakdown/#overview","title":"Overview","text":"<p>This document provides a detailed weekly workload breakdown in person-days for all personas across the three sub-projects of the RH OVE Ecosystem implementation.</p> <p>Project Duration: 60 weeks (15 months) Total Estimated Workload: 2,694 person-days</p>"},{"location":"project-plan/weekly-charge-breakdown/#persona-skill-levels-and-availability","title":"Persona Skill Levels and Availability","text":"Persona Type Skill Level Standard Days/Week Notes Infrastructure Architect Senior 5 Part-time (2.5 days) during Implementation Solution Architect Senior 5 Full-time during active phases Migration Specialist Senior 5 Full-time throughout migration project DevOps Engineer Mid-Senior 5 Multiple resources (2x) Security Engineer Senior 5 Full-time during active phases Network Engineer Senior 5 Full-time during active phases Security Specialist Senior 5 Full-time during active phases Application Owner Mid-Senior 5 Multiple resources (2x) Performance Engineer Mid-Senior 5 Full-time during active phases Business Analyst Mid-Senior 5 Full-time during active phases RH OVE Engineer Mid-Senior 5 Multiple resources (2x) Application Developer Mid-Level 5 Multiple resources (3x) System Administrator Mid-Level 5 Multiple resources (2x) Testing Specialist Mid-Level 5 Full-time during active phases VMware Administrator Mid-Level 5 Multiple resources (2x) Backup Administrator Mid-Level 5 Full-time during active phases"},{"location":"project-plan/weekly-charge-breakdown/#project-1-rh-ove-infrastructure","title":"Project 1: RH OVE Infrastructure","text":""},{"location":"project-plan/weekly-charge-breakdown/#weekly-allocation-by-phase-person-days","title":"Weekly Allocation by Phase (Person-Days)","text":"Week Phase Infrastructure Architect DevOps Engineer (2x) System Administrator (2x) Security Engineer Network Engineer Weekly Total (Days) 1-4 Study 5 0 0 0 5 10 5-12 Design 5 10 0 5 5 25 13-20 Implementation 2.5 (PT) 10 10 5 5 32.5 21-24 Testing 0 10 10 5 0 25 25-28 Day-2 Ops 0 10 10 0 0 20 <p>Infrastructure Project Total: 582.5 person-days</p>"},{"location":"project-plan/weekly-charge-breakdown/#phase-wise-workload-summary","title":"Phase-wise Workload Summary","text":"<ul> <li>Study Phase (Weeks 1-4): 40 person-days</li> <li>Design Phase (Weeks 5-12): 200 person-days  </li> <li>Implementation Phase (Weeks 13-20): 260 person-days</li> <li>Testing Phase (Weeks 21-24): 100 person-days</li> <li>Day-2 Operations (Weeks 25-28): 80 person-days</li> </ul>"},{"location":"project-plan/weekly-charge-breakdown/#project-2-use-cases-implementation","title":"Project 2: Use-Cases Implementation","text":""},{"location":"project-plan/weekly-charge-breakdown/#weekly-allocation-by-phase-person-days_1","title":"Weekly Allocation by Phase (Person-Days)","text":"Week Phase Solution Architect Application Developer (3x) Testing Specialist DevOps Engineer Security Specialist Business Analyst Weekly Total (Days) 15-18 Study 5 0 0 0 0 5 10 19-26 Design 5 0 0 0 5 5 15 27-38 Implementation 0 15 0 5 5 0 25 39-42 Testing 0 15 5 5 5 0 30 43-46 Day-2 Ops 0 0 0 5 0 0 5 <p>Use-Cases Project Total: 560 person-days</p>"},{"location":"project-plan/weekly-charge-breakdown/#phase-wise-workload-summary_1","title":"Phase-wise Workload Summary","text":"<ul> <li>Study Phase (Weeks 15-18): 40 person-days</li> <li>Design Phase (Weeks 19-26): 120 person-days</li> <li>Implementation Phase (Weeks 27-38): 300 person-days</li> <li>Testing Phase (Weeks 39-42): 120 person-days</li> <li>Day-2 Operations (Weeks 43-46): 20 person-days</li> </ul>"},{"location":"project-plan/weekly-charge-breakdown/#project-3-migration-from-vmware","title":"Project 3: Migration from VMware","text":""},{"location":"project-plan/weekly-charge-breakdown/#weekly-allocation-by-phase-person-days_2","title":"Weekly Allocation by Phase (Person-Days)","text":"Week Phase Migration Specialist VMware Admin (2x) RH OVE Engineer (2x) Application Owner (2x) Performance Engineer Backup Administrator Weekly Total (Days) 21-28 Study 5 10 0 0 0 0 15 29-36 Design 5 10 0 10 0 0 25 37-50 Implementation 5 10 10 10 5 5 45 51-54 Testing 5 0 10 10 5 5 35 55-60 Day-2 Ops 5 0 10 0 5 5 25 <p>Migration Project Total: 869 person-days</p>"},{"location":"project-plan/weekly-charge-breakdown/#phase-wise-workload-summary_2","title":"Phase-wise Workload Summary","text":"<ul> <li>Study Phase (Weeks 21-28): 120 person-days</li> <li>Design Phase (Weeks 29-36): 200 person-days</li> <li>Implementation Phase (Weeks 37-50): 630 person-days</li> <li>Testing Phase (Weeks 51-54): 140 person-days</li> <li>Day-2 Operations (Weeks 55-60): 150 person-days</li> </ul>"},{"location":"project-plan/weekly-charge-breakdown/#summary-by-sub-project","title":"Summary by Sub-Project","text":"Sub-Project Duration (Weeks) Total Workload (Person-Days) Average Weekly Workload (Days) RH OVE Infrastructure 28 582.5 20.8 Use-Cases Implementation 32 560 17.5 Migration from VMware 40 869 21.7 Total 60 2,011.5 33.5"},{"location":"project-plan/weekly-charge-breakdown/#peak-resource-utilization","title":"Peak Resource Utilization","text":""},{"location":"project-plan/weekly-charge-breakdown/#highest-workload-weeks","title":"Highest Workload Weeks","text":"Week Range Projects Active Weekly Workload (Days) Key Activities 37-42 Infrastructure + Use-Cases + Migration 102.5 Implementation overlap 43-46 Use-Cases + Migration 30 Testing phases 47-50 Migration Implementation 45 Critical migration waves"},{"location":"project-plan/weekly-charge-breakdown/#resource-overlap-analysis","title":"Resource Overlap Analysis","text":"<p>Weeks 15-28: Infrastructure and Use-Cases overlap Weeks 21-46: All three projects active simultaneously Weeks 47-60: Migration project completion phase</p>"},{"location":"project-plan/weekly-charge-breakdown/#workload-distribution-by-persona-type","title":"Workload Distribution by Persona Type","text":"Persona Type Total Weeks Active Total Workload (Days) Percentage Migration Specialist 40 200 9.9% Application Developer (3x) 16 240 11.9% RH OVE Engineer (2x) 24 240 11.9% VMware Administrator (2x) 28 280 13.9% DevOps Engineer (2x) 36 360 17.9% Infrastructure Architect 18.5 92.5 4.6% System Administrator (2x) 16 160 8.0% Security Engineer 16 80 4.0% Network Engineer 20 100 5.0% Solution Architect 12 60 3.0% Testing Specialist 4 20 1.0% Security Specialist 12 60 3.0% Business Analyst 4 20 1.0% Application Owner (2x) 26 260 12.9% Performance Engineer 18 90 4.5% Backup Administrator 30 150 7.5%"},{"location":"project-plan/weekly-charge-breakdown/#notes","title":"Notes","text":"<ol> <li>Part-time allocation: Infrastructure Architect works part-time (2.5 days/week) during Implementation phase</li> <li>Resource multiplication: Numbers in parentheses (e.g., 2x, 3x) indicate multiple resources of the same type</li> <li>Phase overlap: Some personas work across multiple phases with varying intensity</li> <li>Workload estimates: Based on standard 5-day work weeks for full-time resources</li> <li>Flexibility: Resource allocation can be adjusted based on project needs and availability</li> </ol>"},{"location":"project-plan/weekly-charge-breakdown/#risk-factors-affecting-workload","title":"Risk Factors Affecting Workload","text":"<ol> <li>Skill scarcity: Specialized RH OVE and migration expertise may require extended timelines</li> <li>Timeline compression: Accelerated delivery may require additional parallel resources</li> <li>Scope changes: Additional use-cases or migration complexity could increase workload</li> <li>Resource availability: Team member availability and scheduling conflicts may impact timelines</li> <li>Learning curve: New team members may require additional time for technology familiarization</li> </ol>"},{"location":"references/best-practices/","title":"Best Practices","text":""},{"location":"references/best-practices/#overview","title":"Overview","text":"<p>This document outlines best practices for designing, deploying, and managing the multi-cluster RH OVE ecosystem, ensuring performance, security, and operational efficiency. This includes guidance on managing centralized services within the management cluster and distributing workloads across application clusters.</p>"},{"location":"references/best-practices/#multi-cluster-architecture-best-practices","title":"Multi-Cluster Architecture Best Practices","text":""},{"location":"references/best-practices/#cluster-design","title":"Cluster Design","text":"<ul> <li>Separation of Concerns: Maintain clear separation between management and application clusters</li> <li>Environment Isolation: Use dedicated clusters for production, staging, and development</li> <li>Resource Planning: Size clusters appropriately for their intended workloads</li> <li>Network Segmentation: Implement proper network isolation between cluster environments</li> </ul>"},{"location":"references/best-practices/#management-cluster","title":"Management Cluster","text":"<ul> <li>High Availability: Deploy management services with HA configuration</li> <li>Resource Allocation: Dedicate sufficient resources for centralized services</li> <li>Backup Strategy: Implement comprehensive backup for management cluster state</li> <li>Security Hardening: Apply strict security controls as this cluster manages the entire fleet</li> </ul>"},{"location":"references/best-practices/#application-clusters","title":"Application Clusters","text":"<ul> <li>Standardization: Use consistent cluster configurations across environments</li> <li>Agent Deployment: Ensure proper deployment of management agents (ArgoCD, RHACS, monitoring)</li> <li>Local Resources: Optimize local resource allocation for workload requirements</li> <li>Compliance: Maintain consistent security and compliance postures</li> </ul>"},{"location":"references/best-practices/#architecture-best-practices","title":"Architecture Best Practices","text":""},{"location":"references/best-practices/#namespace-design","title":"Namespace Design","text":"<ul> <li>Use Application Namespaces: Segregate workloads by application or team-based namespaces for enhanced security and resource management</li> <li>Environment Prefixes: Use consistent naming conventions (e.g., prod-, staging-, dev-)</li> <li>Label and Annotate: Use consistent labeling and annotations for automation and policy application</li> <li>Cross-Cluster Consistency: Maintain similar namespace structures across clusters</li> </ul>"},{"location":"references/best-practices/#multi-tenancy","title":"Multi-Tenancy","text":"<ul> <li>RBAC Implementation: Apply Role-Based Access Control to enforce access restrictions</li> <li>Network Policies: Utilize Cilium to enforce strict network policies between tenants</li> <li>Resource Quotas: Implement appropriate resource quotas per tenant/namespace</li> <li>Policy Distribution: Use centralized policy management with cluster-specific enforcement</li> </ul>"},{"location":"references/best-practices/#multi-cluster-gitops-best-practices","title":"Multi-Cluster GitOps Best Practices","text":""},{"location":"references/best-practices/#repository-structure","title":"Repository Structure","text":"<ul> <li>Centralized Repositories: Use centralized Git repositories for all cluster configurations</li> <li>Environment Branching: Implement proper branching strategies for different environments</li> <li>Application Separation: Separate application definitions from infrastructure configurations</li> <li>Policy as Code: Store all policies and governance rules in version control</li> </ul>"},{"location":"references/best-practices/#deployment-strategies","title":"Deployment Strategies","text":"<ul> <li>Progressive Deployment: Deploy to development, then staging, then production clusters</li> <li>Automated Validation: Implement automated testing and validation in CI/CD pipelines</li> <li>Rollback Procedures: Maintain clear rollback procedures for failed deployments</li> <li>Change Management: Implement proper change management processes for critical updates</li> </ul>"},{"location":"references/best-practices/#deployment-best-practices","title":"Deployment Best Practices","text":""},{"location":"references/best-practices/#infrastructure-planning","title":"Infrastructure Planning","text":"<ul> <li>Capacity Planning: Assess resource needs well in advance and plan infrastructure accordingly</li> <li>High Availability (HA): Configure HA for critical components and services</li> <li>Cluster Sizing: Right-size clusters based on workload requirements and growth projections</li> <li>Geographic Distribution: Consider geographic distribution for disaster recovery</li> </ul>"},{"location":"references/best-practices/#configuration-management","title":"Configuration Management","text":"<ul> <li>Infrastructure as Code (IaC): Use GitOps and Argo CD for configuration management and deployment consistency</li> <li>Version Control: Ensure all configurations and manifests are version controlled</li> <li>Template Management: Use Helm charts or Kustomize for template management</li> <li>Secret Management: Implement proper secret management across clusters</li> </ul>"},{"location":"references/best-practices/#security-best-practices","title":"Security Best Practices","text":""},{"location":"references/best-practices/#network-security","title":"Network Security","text":"<ul> <li>Zero Trust Network: Implement zero trust principles using Cilium's microsegmentation and network policies.</li> <li>Encryption: Enforce encryption of data in transit and at rest.</li> </ul>"},{"location":"references/best-practices/#container-and-vm-security","title":"Container and VM Security","text":"<ul> <li>Security Contexts: Apply security contexts to restrict container capabilities and privileges.</li> <li>Image Scanning: Regularly scan container and VM images for vulnerabilities.</li> </ul>"},{"location":"references/best-practices/#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"references/best-practices/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<ul> <li>Comprehensive Monitoring: Utilize tools like Dynatrace and Prometheus for end-to-end monitoring.</li> <li>Alerting Systems: Set up robust alerting and notification systems for proactive issue resolution.</li> </ul>"},{"location":"references/best-practices/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Regular Backups: Schedule regular backups and test recovery procedures periodically.</li> <li>Data Retention Policies: Define and implement data retention and cleanup policies.</li> </ul>"},{"location":"references/best-practices/#continuous-improvement","title":"Continuous Improvement","text":""},{"location":"references/best-practices/#reviews-and-audits","title":"Reviews and Audits","text":"<ul> <li>Performance Reviews: Conduct regular performance reviews and optimizations.</li> <li>Security Audits: Perform periodic security audits and policy compliance checks.</li> </ul>"},{"location":"references/best-practices/#community-engagement","title":"Community Engagement","text":"<ul> <li>Stay Updated: Engage with the community via forums and contribute to open-source projects.</li> <li>Professional Development: Encourage ongoing learning and certification for team members.</li> </ul>"},{"location":"references/best-practices/#documentation-and-knowledge-sharing","title":"Documentation and Knowledge Sharing","text":"<ul> <li>Maintain Documentation: Keep operational runbooks and architecture diagrams updated.</li> <li>Knowledge Transfer: Conduct regular training sessions and share lessons learned.</li> </ul>"},{"location":"references/best-practices/#conclusion","title":"Conclusion","text":"<p>Adhering to these best practices ensures a well-architected, secure, and efficient RH OVE ecosystem that can adapt and scale with changing business needs.</p>"},{"location":"references/downloads/","title":"Downloads and Exports","text":"<p>This page provides access to downloadable files and exports generated from the RH OVE Multi-Cluster Ecosystem documentation.</p>"},{"location":"references/downloads/#documentation-exports","title":"Documentation Exports","text":""},{"location":"references/downloads/#complete-documentation","title":"Complete Documentation","text":"<ul> <li>RH OVE Complete Documentation (DOCX) - Comprehensive documentation in Microsoft Word format including all sections, diagrams, and technical specifications</li> </ul>"},{"location":"references/downloads/#project-management-files","title":"Project Management Files","text":"<ul> <li>Weekly Workload Breakdown (XLSX) - Detailed project timeline and resource allocation spreadsheet</li> <li>Project Charges (CSV) - Budget breakdown and cost analysis in CSV format</li> </ul>"},{"location":"references/downloads/#architecture-diagrams-drawio-format","title":"Architecture Diagrams (Draw.io Format)","text":""},{"location":"references/downloads/#core-architecture","title":"Core Architecture","text":"<ul> <li>Global Overview - Multi-cluster architecture overview</li> <li>Single Cluster Overview - Individual cluster architecture</li> <li>Design Principles - Architecture design principles and patterns</li> <li>Network Architecture - Network topology and connectivity</li> <li>Storage Architecture - Storage solutions and data management</li> <li>IAM Strategy - Identity and access management</li> </ul>"},{"location":"references/downloads/#adr-diagrams","title":"ADR Diagrams","text":"<ul> <li>ADR Table - Architecture decision records overview</li> <li>ADR-008 IAM Strategy - IAM implementation decisions</li> </ul>"},{"location":"references/downloads/#deployment-management","title":"Deployment &amp; Management","text":"<ul> <li>Prerequisites - Deployment prerequisites flowchart</li> <li>Installation Guide - Installation process workflow</li> <li>Admission Control - Security and admission control</li> <li>GitOps Operations - GitOps workflow and processes</li> <li>Monitoring - Monitoring and observability architecture</li> </ul>"},{"location":"references/downloads/#operations","title":"Operations","text":"<ul> <li>Performance Tuning - Performance optimization strategies</li> <li>Troubleshooting - Troubleshooting workflows and decision trees</li> </ul>"},{"location":"references/downloads/#use-cases","title":"Use Cases","text":"<ul> <li>Use Cases Table - Use cases overview and mapping</li> <li>VM Import &amp; Migration - Virtual machine migration processes</li> <li>VM Template Management - VM template lifecycle</li> <li>VM Scaling &amp; Performance - VM performance optimization</li> <li>VM Backup &amp; Recovery - Backup and disaster recovery</li> <li>Hybrid Applications - Hybrid application deployment</li> <li>Multi-Environment Setup - Multi-environment application deployment</li> <li>Database Services - PaaS database services</li> <li>Legacy Modernization - Legacy system modernization</li> <li>Disaster Recovery - Enterprise disaster recovery</li> <li>End-to-End Observability - Comprehensive monitoring solution</li> <li>WAF &amp; Firewalling - Web application firewall and security</li> <li>Events to CMDB/SIEM - Event integration workflows</li> </ul>"},{"location":"references/downloads/#project-planning","title":"Project Planning","text":"<ul> <li>Detailed Timeline - Project timeline and milestones</li> <li>Home Page Diagram - Main documentation overview</li> </ul>"},{"location":"references/downloads/#file-formats-and-usage","title":"File Formats and Usage","text":""},{"location":"references/downloads/#docx-files","title":"DOCX Files","text":"<ul> <li>Purpose: Complete documentation for offline reading, sharing, and printing</li> <li>Software: Microsoft Word, LibreOffice Writer, Google Docs</li> <li>Best For: Executive summaries, client presentations, offline documentation</li> </ul>"},{"location":"references/downloads/#xlsx-files","title":"XLSX Files","text":"<ul> <li>Purpose: Project management data, timelines, and resource planning</li> <li>Software: Microsoft Excel, LibreOffice Calc, Google Sheets</li> <li>Best For: Project tracking, budget analysis, resource allocation</li> </ul>"},{"location":"references/downloads/#csv-files","title":"CSV Files","text":"<ul> <li>Purpose: Data interchange and analysis</li> <li>Software: Any spreadsheet application, data analysis tools</li> <li>Best For: Data import, cost analysis, reporting</li> </ul>"},{"location":"references/downloads/#drawio-files-drawio","title":"Draw.io Files (.drawio)","text":"<ul> <li>Purpose: Editable architecture diagrams and flowcharts</li> <li>Software: Draw.io, Draw.io Desktop</li> <li>Best For: Diagram customization, architecture updates, visual documentation</li> </ul>"},{"location":"references/downloads/#how-to-use-these-files","title":"How to Use These Files","text":""},{"location":"references/downloads/#opening-drawio-files","title":"Opening Draw.io Files","text":"<ol> <li>Visit app.diagrams.net</li> <li>Click \"Open Existing Diagram\"</li> <li>Upload the <code>.drawio</code> file from your downloads</li> <li>Edit, customize, and export as needed</li> </ol>"},{"location":"references/downloads/#viewing-documentation","title":"Viewing Documentation","text":"<ul> <li>DOCX files can be opened directly in most word processors</li> <li>Use the table of contents for easy navigation</li> <li>All diagrams are embedded as high-resolution images</li> </ul>"},{"location":"references/downloads/#project-management","title":"Project Management","text":"<ul> <li>XLSX files contain detailed project timelines and resource allocations</li> <li>CSV files can be imported into project management tools</li> <li>Use filters and pivot tables for custom analysis</li> </ul>"},{"location":"references/downloads/#version-information","title":"Version Information","text":"<ul> <li>Generated: 2025-08-04</li> <li>Documentation Version: 1.0</li> <li>Last Updated: Based on latest documentation changes</li> </ul>"},{"location":"references/downloads/#support","title":"Support","text":"<p>For questions about these exports or to request additional formats, please: - Create an issue in the project repository - Contact the documentation team - Refer to the main documentation for detailed technical information</p> <p>Note: All files are generated automatically from the source documentation. For the most up-to-date information, always refer to the online documentation.</p>"},{"location":"references/glossary/","title":"Glossary","text":""},{"location":"references/glossary/#overview","title":"Overview","text":"<p>This glossary provides definitions for key terms and concepts used throughout the RH OVE ecosystem documentation.</p>"},{"location":"references/glossary/#a","title":"A","text":"<p>Admission Control : A Kubernetes mechanism that validates and mutates API requests before they are persisted to etcd. In RH OVE, this includes OpenShift built-in controllers, KubeVirt webhooks, and Kyverno policies.</p> <p>Ansible : An open-source, agentless IT automation tool used for configuration management, application deployment, orchestration, and task automation across multiple systems.</p> <p>Argo CD : A declarative GitOps continuous delivery tool for Kubernetes that automatically synchronizes applications with their desired state defined in Git repositories.</p>"},{"location":"references/glossary/#b","title":"B","text":"<p>Backup Policy : A set of rules and schedules that define how, when, and what data should be backed up in the RH OVE environment, typically managed by Rubrik.</p>"},{"location":"references/glossary/#c","title":"C","text":"<p>CDI (Containerized Data Importer) : A Kubernetes extension that provides facilities for enabling Persistent Volume Claims (PVCs) to be used as disks for KubeVirt VMs by importing, uploading, and cloning disk images.</p> <p>Cilium : An open-source software for providing and transparently securing network connectivity and load balancing between application workloads using eBPF technology.</p> <p>CMDB (Configuration Management Database) : A repository that acts as a data warehouse for IT installations, containing information about configuration items and their relationships, often integrated with ServiceNow.</p> <p>CNI (Container Network Interface) : A specification and libraries for writing plugins to configure network interfaces in Linux containers, with Cilium being the recommended CNI for RH OVE.</p> <p>CRD (Custom Resource Definition) : A Kubernetes extension mechanism that allows users to define custom resources that extend the Kubernetes API, extensively used in KubeVirt for VM management.</p> <p>CSI (Container Storage Interface) : A standard for exposing arbitrary block and file storage systems to containerized workloads on Kubernetes, enabling storage vendors to develop plugins that work across different container orchestration systems.</p>"},{"location":"references/glossary/#d","title":"D","text":"<p>DataVolume : A KubeVirt CRD that provides a declarative way to import, upload, and clone data into PVCs, serving as the primary storage mechanism for VM disks.</p> <p>Day-2 Operations : Post-deployment operational activities including maintenance, monitoring, updates, scaling, and optimization of the RH OVE environment.</p> <p>Dynatrace : An application performance monitoring and observability platform that provides full-stack monitoring for RH OVE environments.</p>"},{"location":"references/glossary/#e","title":"E","text":"<p>eBPF (extended Berkeley Packet Filter) : A kernel technology that allows programs to run in kernel space without changing kernel source code or loading kernel modules, used by Cilium for high-performance networking.</p> <p>etcd : A distributed, reliable key-value store used by Kubernetes to store all cluster data, providing a consistent and highly-available data store for cluster state.</p>"},{"location":"references/glossary/#g","title":"G","text":"<p>GitOps : An operational framework that takes DevOps best practices used for application development and applies them to infrastructure automation, using Git as the single source of truth.</p> <p>Grafana : An open-source platform for monitoring and observability that enables visualization, alerting, and exploration of metrics from multiple data sources including Prometheus, Elasticsearch, and others.</p>"},{"location":"references/glossary/#h","title":"H","text":"<p>Hugepages : Large memory pages that can improve performance for memory-intensive applications by reducing memory management overhead in virtual machines.</p> <p>Hubble : The network observability layer for Cilium that provides deep visibility into network flows, security policies, and performance metrics.</p> <p>Helm : A Kubernetes package manager that helps you manage Kubernetes applications through charts, which are packages of pre-configured Kubernetes resources.</p> <p>HyperConverged : A top-level CRD in OpenShift Virtualization that manages the deployment and configuration of all virtualization components.</p>"},{"location":"references/glossary/#i","title":"I","text":"<p>Ingress : A Kubernetes API object that manages external access to services in a cluster, typically HTTP, providing load balancing, SSL termination, and name-based virtual hosting.</p> <p>Istio : An open-source service mesh that provides a uniform way to secure, connect, and monitor microservices, offering traffic management, security, and observability features.</p>"},{"location":"references/glossary/#k","title":"K","text":"<p>KubeVirt : An open-source Kubernetes add-on that enables running virtual machines alongside containers in a Kubernetes cluster, forming the foundation of OpenShift Virtualization.</p> <p>Karmada : A Kubernetes management system that enables multi-cluster application management and provides centralized control plane for managing workloads across multiple Kubernetes clusters.</p> <p>Kyverno : A policy engine designed for Kubernetes that validates, mutates, and generates configurations using admission controller webhooks and background scans.</p>"},{"location":"references/glossary/#m","title":"M","text":"<p>MacVLAN : A Linux networking driver that allows creating multiple virtual network interfaces with different MAC addresses on a single physical network interface, commonly used with Multus for VM networking.</p> <p>Multus CNI : A Container Network Interface (CNI) plugin that enables attachment of multiple network interfaces to pods and VMs in Kubernetes, allowing complex networking scenarios beyond single-network configurations.</p>"},{"location":"references/glossary/#n","title":"N","text":"<p>NAD (Network Attachment Definition) : See NetworkAttachmentDefinition.</p> <p>NetworkAttachmentDefinition : A CRD used by Multus that defines additional network interfaces for pods and VMs, enabling multi-network configurations beyond the default cluster network.</p> <p>Network Plumbing Working Group : A Kubernetes community working group focused on developing networking enhancements, including Multus CNI and related multi-networking technologies.</p> <p>NUMA (Non-Uniform Memory Access) : A computer memory design used in multiprocessing where memory access time depends on the memory location relative to the processor, important for VM performance tuning.</p>"},{"location":"references/glossary/#o","title":"O","text":"<p>OLM (Operator Lifecycle Manager) : A component of the Operator Framework that helps users install, update, and manage the lifecycle of Kubernetes operators and their associated services.</p> <p>OpenShift Virtualization : Red Hat's enterprise virtualization solution that allows running virtual machines alongside containers on the same OpenShift platform.</p>"},{"location":"references/glossary/#p","title":"P","text":"<p>Prometheus : An open-source systems monitoring and alerting toolkit with a dimensional data model, flexible query language (PromQL), efficient time series database, and modern alerting approach.</p> <p>PVC (Persistent Volume Claim) : A request for storage by a user or application in Kubernetes, used extensively in RH OVE for VM disk storage.</p>"},{"location":"references/glossary/#q","title":"Q","text":"<p>QEMU Guest Agent : A daemon that runs inside virtual machines to provide enhanced integration between the VM and the hypervisor, enabling better monitoring and management.</p>"},{"location":"references/glossary/#r","title":"R","text":"<p>RBAC (Role-Based Access Control) : A method of restricting system access to authorized users based on their roles within an organization, fundamental to multi-tenant security in RH OVE.</p> <p>RH OVE (Red Hat OpenShift Virtualization Engine) : Red Hat's solution for running virtual machines on OpenShift, based on the upstream KubeVirt project.</p> <p>Rubrik : An enterprise data management platform that provides backup, recovery, and data protection services, certified for integration with RH OVE.</p>"},{"location":"references/glossary/#s","title":"S","text":"<p>ServiceNow : An IT service management platform that provides CMDB functionality and can be integrated with RH OVE for automated configuration tracking.</p> <p>SR-IOV (Single Root I/O Virtualization) : A specification that allows efficient sharing of PCIe devices between virtual machines, enabling high-performance networking for VMs.</p>"},{"location":"references/glossary/#t","title":"T","text":"<p>Tekton : A Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems, allowing developers to build, test, and deploy applications.</p> <p>Terraform : An open-source infrastructure as code tool that allows users to define and provision data center infrastructure using a declarative configuration language.</p>"},{"location":"references/glossary/#v","title":"V","text":"<p>VirtualMachine (VM) : A KubeVirt CRD that represents a virtual machine definition, including CPU, memory, storage, and network configurations.</p> <p>VirtualMachineInstance (VMI) : A KubeVirt CRD that represents a running virtual machine instance, showing the actual runtime state of a VM.</p> <p>VirtualMachineInstanceReplicaSet : A KubeVirt CRD that ensures a specified number of VMI replicas are running, similar to Kubernetes ReplicaSets for pods.</p> <p>virtctl : A command-line tool for managing KubeVirt virtual machines, providing functionality to start, stop, console access, and manage VMs.</p> <p>VLAN (Virtual Local Area Network) : A network configuration that enables the logical partitioning of a physical network into multiple broadcast domains, improving security and network management.</p> <p>VPC (Virtual Private Cloud) : A logically isolated section of a cloud provider's infrastructure where users can launch resources in a virtual network that they define.</p>"},{"location":"references/glossary/#w","title":"W","text":"<p>WebAssembly (WASM) : A binary instruction format for a stack-based virtual machine that enables high-performance applications on web browsers and server environments, increasingly used for cloud-native applications.</p>"},{"location":"references/glossary/#common-acronyms","title":"Common Acronyms","text":"<ul> <li>ADR: Architecture Decision Record</li> <li>API: Application Programming Interface</li> <li>CDI: Containerized Data Importer</li> <li>CI/CD: Continuous Integration/Continuous Deployment</li> <li>CMDB: Configuration Management Database</li> <li>CNI: Container Network Interface</li> <li>CPU: Central Processing Unit</li> <li>CRD: Custom Resource Definition</li> <li>CSI: Container Storage Interface</li> <li>DNS: Domain Name System</li> <li>HA: High Availability</li> <li>IAM: Identity and Access Management</li> <li>I/O: Input/Output</li> <li>IOPS: Input/Output Operations Per Second</li> <li>ITSM: IT Service Management</li> <li>JSON: JavaScript Object Notation</li> <li>LDAP: Lightweight Directory Access Protocol</li> <li>NFS: Network File System</li> <li>OAuth: Open Authorization</li> <li>OIDC: OpenID Connect</li> <li>OLM: Operator Lifecycle Manager</li> <li>RBAC: Role-Based Access Control</li> <li>REST: Representational State Transfer</li> <li>SAML: Security Assertion Markup Language</li> <li>SIEM: Security Information and Event Management</li> <li>SLA: Service Level Agreement</li> <li>SSD: Solid State Drive</li> <li>TLS: Transport Layer Security</li> <li>VLAN: Virtual Local Area Network</li> <li>VM: Virtual Machine</li> <li>VMI: Virtual Machine Instance</li> <li>VPC: Virtual Private Cloud</li> <li>WAF: Web Application Firewall</li> <li>WASM: WebAssembly</li> <li>YAML: YAML Ain't Markup Language</li> </ul> <p>This glossary provides essential terminology for understanding and working with the RH OVE ecosystem. Terms are regularly updated as the technology and documentation evolve.</p>"},{"location":"references/product-urls/","title":"Product URLs and References","text":""},{"location":"references/product-urls/#overview","title":"Overview","text":"<p>This document provides an exhaustive list of product URLs, documentation, and resources for all components used in the RH OVE ecosystem design and implementation.</p>"},{"location":"references/product-urls/#core-platform-components","title":"Core Platform Components","text":""},{"location":"references/product-urls/#red-hat-openshift","title":"Red Hat OpenShift","text":"<ul> <li>Product Homepage: https://www.redhat.com/en/technologies/cloud-computing/openshift</li> <li>Documentation: https://docs.openshift.com/</li> <li>Container Platform Docs: https://docs.openshift.com/container-platform/latest/</li> <li>Installation Guide: https://docs.openshift.com/container-platform/latest/installing/</li> <li>API Reference: https://docs.openshift.com/container-platform/latest/rest_api/</li> <li>Red Hat Customer Portal: https://access.redhat.com/</li> <li>OpenShift Blog: https://www.redhat.com/en/blog/channel/red-hat-openshift</li> </ul>"},{"location":"references/product-urls/#red-hat-openshift-virtualization-engine-rh-ove","title":"Red Hat OpenShift Virtualization Engine (RH OVE)","text":"<ul> <li>Product Page: https://www.redhat.com/en/technologies/cloud-computing/openshift/virtualization-engine</li> <li>Datasheet: https://www.redhat.com/en/resources/red-hat-openshift-virtualization-engine-datasheet</li> <li>Documentation: https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/virtualization/</li> <li>Getting Started: https://docs.redhat.com/en/documentation/openshift_container_platform/4.15/html/virtualization/about</li> <li>Reference Implementation: https://access.redhat.com/sites/default/files/attachments/openshift_virtualization_reference_implementation_guide.pdf</li> </ul>"},{"location":"references/product-urls/#kubevirt-upstream","title":"KubeVirt (Upstream)","text":"<ul> <li>Project Homepage: https://kubevirt.io/</li> <li>GitHub Repository: https://github.com/kubevirt/kubevirt</li> <li>Documentation: https://kubevirt.io/user-guide/</li> <li>Architecture: https://kubevirt.io/user-guide/architecture/</li> <li>Installation: https://kubevirt.io/user-guide/cluster_admin/installation/</li> <li>API Validation: https://kubevirt.io/user-guide/cluster_admin/api_validation/</li> <li>GitOps Guide: https://kubevirt.io/user-guide/cluster_admin/gitops/</li> </ul>"},{"location":"references/product-urls/#networking-components","title":"Networking Components","text":""},{"location":"references/product-urls/#cilium","title":"Cilium","text":"<ul> <li>Project Homepage: https://cilium.io/</li> <li>Documentation: https://docs.cilium.io/</li> <li>GitHub Repository: https://github.com/cilium/cilium</li> <li>OpenShift Installation: https://docs.cilium.io/en/stable/installation/k8s-install-openshift-okd.html</li> <li>Blog: https://cilium.io/blog/</li> <li>OpenShift Certification: https://cilium.io/blog/2021/04/19/openshift-certification/</li> <li>Learning Hub: https://www.tigera.io/learn/guides/cilium-vs-calico/cilium/</li> </ul>"},{"location":"references/product-urls/#hubble-network-observability","title":"Hubble (Network Observability)","text":"<ul> <li>Documentation: https://docs.cilium.io/en/stable/gettingstarted/hubble/</li> <li>GitHub Repository: https://github.com/cilium/hubble</li> <li>UI Repository: https://github.com/cilium/hubble-ui</li> </ul>"},{"location":"references/product-urls/#multus-cni-multi-network","title":"Multus CNI (Multi-Network)","text":"<ul> <li>Project Homepage: https://github.com/k8snetworkplumbingwg/multus-cni</li> <li>Documentation: https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md</li> <li>OpenShift Documentation: https://docs.openshift.com/container-platform/latest/networking/multiple_networks/understanding-multiple-networks.html</li> <li>Network Attachment Definitions: https://docs.openshift.com/container-platform/latest/networking/multiple_networks/configuring-additional-network-types.html</li> <li>SR-IOV Network Operator: https://docs.openshift.com/container-platform/latest/networking/hardware_networks/about-sriov.html</li> <li>CNCF Landscape: https://landscape.cncf.io/card-mode?category=cni&amp;grouping=category</li> <li>Kubernetes Network SIG: https://github.com/kubernetes/community/tree/master/sig-network</li> </ul>"},{"location":"references/product-urls/#sr-iov-network-operator","title":"SR-IOV Network Operator","text":"<ul> <li>GitHub Repository: https://github.com/k8snetworkplumbingwg/sriov-network-operator</li> <li>OpenShift SR-IOV Documentation: https://docs.openshift.com/container-platform/latest/networking/hardware_networks/installing-sriov-operator.html</li> <li>Configuration Guide: https://docs.openshift.com/container-platform/latest/networking/hardware_networks/configuring-sriov-device.html</li> <li>Performance Tuning: https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-dpdk-and-rdma.html</li> </ul>"},{"location":"references/product-urls/#network-plumbing-working-group","title":"Network Plumbing Working Group","text":"<ul> <li>GitHub Organization: https://github.com/k8snetworkplumbingwg</li> <li>Community Meetings: https://github.com/k8snetworkplumbingwg/community</li> <li>CNI Plugins: https://github.com/containernetworking/plugins</li> </ul>"},{"location":"references/product-urls/#policy-and-security","title":"Policy and Security","text":""},{"location":"references/product-urls/#kyverno","title":"Kyverno","text":"<ul> <li>Project Homepage: https://kyverno.io/</li> <li>Documentation: https://kyverno.io/docs/</li> <li>GitHub Repository: https://github.com/kyverno/kyverno</li> <li>Installation: https://kyverno.io/docs/installation/</li> <li>Policy Examples: https://kyverno.io/policies/</li> <li>Admission Controllers Guide: https://kyverno.io/docs/introduction/admission-controllers/</li> <li>Helm Chart: https://github.com/kyverno/kyverno/tree/main/charts/kyverno</li> </ul>"},{"location":"references/product-urls/#gitops-and-cicd","title":"GitOps and CI/CD","text":""},{"location":"references/product-urls/#argo-cd","title":"Argo CD","text":"<ul> <li>Project Homepage: https://argoproj.github.io/argo-cd/</li> <li>Documentation: https://argo-cd.readthedocs.io/</li> <li>GitHub Repository: https://github.com/argoproj/argo-cd</li> <li>Getting Started: https://argo-cd.readthedocs.io/en/stable/getting_started/</li> <li>OpenShift GitOps: https://docs.openshift.com/container-platform/latest/cicd/gitops/understanding-openshift-gitops.html</li> </ul>"},{"location":"references/product-urls/#red-hat-openshift-gitops","title":"Red Hat OpenShift GitOps","text":"<ul> <li>Product Page: https://www.redhat.com/en/technologies/cloud-computing/openshift/gitops</li> <li>Documentation: https://docs.openshift.com/container-platform/latest/cicd/gitops/</li> <li>Operator Hub: https://operatorhub.io/operator/openshift-gitops-operator</li> </ul>"},{"location":"references/product-urls/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"references/product-urls/#dynatrace","title":"Dynatrace","text":"<ul> <li>Product Homepage: https://www.dynatrace.com/</li> <li>Platform Documentation: https://docs.dynatrace.com/</li> <li>OpenShift Integration: https://docs.dynatrace.com/docs/ingest-from/setup-on-k8s/deployment/other/ocp-operator-hub</li> <li>Kubernetes Monitoring: https://docs.dynatrace.com/docs/observe/infrastructure-monitoring/container-platform-monitoring/kubernetes-monitoring/</li> <li>Operator GitHub: https://github.com/Dynatrace/dynatrace-operator</li> <li>Red Hat Partner Page: https://www.dynatrace.com/hub/detail/red-hat-openshift/</li> <li>Blog: https://www.dynatrace.com/news/blog/</li> </ul>"},{"location":"references/product-urls/#prometheus","title":"Prometheus","text":"<ul> <li>Project Homepage: https://prometheus.io/</li> <li>Documentation: https://prometheus.io/docs/</li> <li>GitHub Repository: https://github.com/prometheus/prometheus</li> <li>OpenShift Monitoring: https://docs.openshift.com/container-platform/latest/monitoring/</li> </ul>"},{"location":"references/product-urls/#grafana","title":"Grafana","text":"<ul> <li>Project Homepage: https://grafana.com/</li> <li>Documentation: https://grafana.com/docs/</li> <li>GitHub Repository: https://github.com/grafana/grafana</li> </ul>"},{"location":"references/product-urls/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"references/product-urls/#rubrik","title":"Rubrik","text":"<ul> <li>Product Homepage: https://www.rubrik.com/</li> <li>OpenShift Solutions: https://www.rubrik.com/solutions/openshift</li> <li>Kubernetes Solutions: https://www.rubrik.com/solutions/kubernetes</li> <li>Documentation: https://docs.rubrik.com/</li> <li>Compatibility Matrix: https://docs.rubrik.com/en-us/compat_matrix/index.html</li> <li>Blog: https://www.rubrik.com/blog/</li> <li>Red Hat Partnership: https://www.redhat.com/en/blog/red-hat-openshift-virtualization-and-rubrik</li> </ul>"},{"location":"references/product-urls/#itsm-and-cmdb-integration","title":"ITSM and CMDB Integration","text":""},{"location":"references/product-urls/#servicenow","title":"ServiceNow","text":"<ul> <li>Product Homepage: https://www.servicenow.com/</li> <li>CMDB Documentation: https://docs.servicenow.com/bundle/xanadu-servicenow-platform/page/product/configuration-management/</li> <li>Developer Documentation: https://developer.servicenow.com/</li> <li>REST API Guide: https://docs.servicenow.com/bundle/xanadu-application-development/page/integrate/inbound-rest/concept/c_RESTAPI.html</li> <li>IntegrationHub: https://docs.servicenow.com/bundle/xanadu-servicenow-platform/page/administer/integrationhub/</li> </ul>"},{"location":"references/product-urls/#storage-solutions","title":"Storage Solutions","text":""},{"location":"references/product-urls/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"<ul> <li>CSI Specification: https://github.com/container-storage-interface/spec</li> <li>Kubernetes CSI Documentation: https://kubernetes-csi.github.io/docs/</li> <li>OpenShift Storage: https://docs.openshift.com/container-platform/latest/storage/</li> </ul>"},{"location":"references/product-urls/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<ul> <li>GitHub Repository: https://github.com/kubevirt/containerized-data-importer</li> <li>Documentation: https://kubevirt.io/user-guide/storage/disks_and_volumes/</li> </ul>"},{"location":"references/product-urls/#additional-tools-and-utilities","title":"Additional Tools and Utilities","text":""},{"location":"references/product-urls/#virtctl","title":"virtctl","text":"<ul> <li>Documentation: https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/</li> <li>GitHub Repository: https://github.com/kubevirt/kubevirt/tree/main/cmd/virtctl</li> </ul>"},{"location":"references/product-urls/#openshift-cli-oc","title":"OpenShift CLI (oc)","text":"<ul> <li>Documentation: https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html</li> <li>Download: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/</li> </ul>"},{"location":"references/product-urls/#community-and-learning-resources","title":"Community and Learning Resources","text":""},{"location":"references/product-urls/#red-hat-learning","title":"Red Hat Learning","text":"<ul> <li>Red Hat Training: https://www.redhat.com/en/services/training</li> <li>OpenShift Learning Portal: https://learn.openshift.com/</li> <li>Red Hat Developer: https://developers.redhat.com/</li> </ul>"},{"location":"references/product-urls/#community-forums","title":"Community Forums","text":"<ul> <li>Red Hat Customer Portal: https://access.redhat.com/discussions</li> <li>OpenShift Commons: https://commons.openshift.org/</li> <li>KubeVirt Community: https://kubevirt.io/community/</li> <li>CNCF Slack: https://slack.cncf.io/</li> </ul>"},{"location":"references/product-urls/#github-organizations","title":"GitHub Organizations","text":"<ul> <li>Red Hat: https://github.com/redhat</li> <li>OpenShift: https://github.com/openshift</li> <li>KubeVirt: https://github.com/kubevirt</li> <li>Cilium: https://github.com/cilium</li> <li>Kyverno: https://github.com/kyverno</li> <li>Argo Project: https://github.com/argoproj</li> </ul>"},{"location":"references/product-urls/#professional-services-and-support","title":"Professional Services and Support","text":""},{"location":"references/product-urls/#red-hat-consulting","title":"Red Hat Consulting","text":"<ul> <li>Services Overview: https://www.redhat.com/en/services/consulting</li> <li>OpenShift Consulting: https://www.redhat.com/en/services/consulting/openshift</li> </ul>"},{"location":"references/product-urls/#partner-ecosystem","title":"Partner Ecosystem","text":"<ul> <li>Red Hat Partner Directory: https://connect.redhat.com/en/partner-directory</li> <li>Certified Container Images: https://catalog.redhat.com/software/containers/explore</li> <li>Operator Hub: https://operatorhub.io/</li> </ul> <p>This comprehensive list provides direct access to all the resources, documentation, and tools needed for implementing and managing the RH OVE ecosystem.</p>"},{"location":"references/sbom/","title":"Software Bill of Materials (SBOM)","text":""},{"location":"references/sbom/#overview","title":"Overview","text":"<p>This document provides a comprehensive Software Bill of Materials (SBOM) for the RH OVE Multi-Cluster Ecosystem, consolidating all required software components, versions, and dependencies needed for successful deployment and operation.</p>"},{"location":"references/sbom/#core-platform-components","title":"Core Platform Components","text":""},{"location":"references/sbom/#red-hat-openshift-container-platform","title":"Red Hat OpenShift Container Platform","text":"Component Version License Purpose Source OpenShift Container Platform 4.12+ (recommended 4.14+) Commercial Kubernetes platform foundation Red Hat OpenShift CLI (oc) Matches cluster version Apache 2.0 Command-line interface Red Hat OpenShift Web Console Integrated with OCP Commercial Web-based management interface Red Hat"},{"location":"references/sbom/#virtualization-stack","title":"Virtualization Stack","text":"Component Version License Purpose Source OpenShift Virtualization Operator 4.14+ Commercial VM management on OpenShift Red Hat KubeVirt Latest (upstream) Apache 2.0 Kubernetes VM orchestration KubeVirt Community virtctl Matches KubeVirt version Apache 2.0 VM command-line tool KubeVirt Community Containerized Data Importer (CDI) Latest Apache 2.0 VM disk import/management KubeVirt Community libvirt 7.0+ LGPL 2.1+ Virtualization API Red Hat Enterprise Linux QEMU/KVM 6.0+ GPL v2 Hypervisor Red Hat Enterprise Linux"},{"location":"references/sbom/#networking-components","title":"Networking Components","text":""},{"location":"references/sbom/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"Component Version License Purpose Source Cilium CNI 1.12+ Apache 2.0 Primary network plugin with eBPF Cilium Cilium Operator Matches Cilium version Apache 2.0 Cilium management operator Cilium Hubble Integrated with Cilium Apache 2.0 Network observability Cilium Multus CNI 3.8+ Apache 2.0 Multi-network support Network Plumbing WG SR-IOV Network Operator 4.12+ Apache 2.0 High-performance networking Red Hat SR-IOV CNI Latest Apache 2.0 SR-IOV network plugin Network Plumbing WG"},{"location":"references/sbom/#network-tools","title":"Network Tools","text":"Component Version License Purpose Source iptables 1.8+ GPL v2 Network filtering Linux eBPF Kernel 4.14+ GPL v2 Network programming Linux Kernel OVS (Open vSwitch) 2.15+ Apache 2.0 Virtual switching Open vSwitch"},{"location":"references/sbom/#security-and-policy-management","title":"Security and Policy Management","text":""},{"location":"references/sbom/#security-platforms","title":"Security Platforms","text":"Component Version License Purpose Source Red Hat Advanced Cluster Security Latest Commercial Security and compliance platform Red Hat Kyverno 1.8+ Apache 2.0 Policy engine Kyverno Community Falco 0.32+ Apache 2.0 Runtime security monitoring CNCF"},{"location":"references/sbom/#certificate-and-identity-management","title":"Certificate and Identity Management","text":"Component Version License Purpose Source cert-manager 1.10+ Apache 2.0 Certificate lifecycle management CNCF External Secrets Operator 0.7+ Apache 2.0 Secret management External Secrets OpenShift OAuth Integrated Commercial Authentication provider Red Hat"},{"location":"references/sbom/#gitops-and-continuous-deployment","title":"GitOps and Continuous Deployment","text":""},{"location":"references/sbom/#gitops-platform","title":"GitOps Platform","text":"Component Version License Purpose Source Red Hat OpenShift GitOps Latest Commercial GitOps platform based on Argo CD Red Hat Argo CD 2.6+ Apache 2.0 GitOps continuous deployment Argo Project Argo Workflows 3.4+ Apache 2.0 Workflow orchestration Argo Project Argo Rollouts 1.4+ Apache 2.0 Progressive delivery Argo Project"},{"location":"references/sbom/#source-control-integration","title":"Source Control Integration","text":"Component Version License Purpose Source Git 2.30+ GPL v2 Version control system Git Community GitHub/GitLab Webhooks API v4+ Various Repository integration GitHub/GitLab"},{"location":"references/sbom/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"references/sbom/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"Component Version License Purpose Source Dynatrace Operator Latest Commercial Full-stack observability platform Dynatrace Prometheus 2.40+ Apache 2.0 Metrics collection and storage CNCF Grafana 9.0+ AGPL v3 Metrics visualization Grafana Labs AlertManager 0.25+ Apache 2.0 Alert management Prometheus Node Exporter 1.5+ Apache 2.0 Node metrics collection Prometheus kube-state-metrics 2.7+ Apache 2.0 Kubernetes metrics Kubernetes"},{"location":"references/sbom/#logging","title":"Logging","text":"Component Version License Purpose Source OpenShift Logging 5.6+ Commercial Log aggregation platform Red Hat Elasticsearch 7.17+ Elastic License Log storage and search Elastic Fluentd 1.15+ Apache 2.0 Log collection and forwarding CNCF Kibana 7.17+ Elastic License Log visualization Elastic"},{"location":"references/sbom/#distributed-tracing","title":"Distributed Tracing","text":"Component Version License Purpose Source Jaeger 1.40+ Apache 2.0 Distributed tracing CNCF OpenTelemetry Operator 0.70+ Apache 2.0 Telemetry collection CNCF"},{"location":"references/sbom/#storage-solutions","title":"Storage Solutions","text":""},{"location":"references/sbom/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"Component Version License Purpose Source AWS EBS CSI Driver 1.15+ Apache 2.0 Block storage for AWS Kubernetes Azure Disk CSI Driver 1.25+ Apache 2.0 Block storage for Azure Kubernetes GCE Persistent Disk CSI 1.10+ Apache 2.0 Block storage for GCP Kubernetes Ceph CSI 3.8+ Apache 2.0 Distributed storage Ceph NetApp Trident 22.10+ Apache 2.0 Enterprise storage NetApp Dell CSI Driver 2.8+ Apache 2.0 Dell enterprise storage Dell Technologies"},{"location":"references/sbom/#storage-management","title":"Storage Management","text":"Component Version License Purpose Source OpenShift Data Foundation 4.12+ Commercial Software-defined storage Red Hat Local Storage Operator 4.12+ Apache 2.0 Local storage management Red Hat"},{"location":"references/sbom/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":""},{"location":"references/sbom/#backup-solutions","title":"Backup Solutions","text":"Component Version License Purpose Source Rubrik Latest Commercial Enterprise backup platform Rubrik Velero 1.10+ Apache 2.0 Kubernetes backup VMware OADP (OpenShift API for Data Protection) 1.1+ Apache 2.0 Backup operator Red Hat"},{"location":"references/sbom/#multi-cluster-management","title":"Multi-Cluster Management","text":""},{"location":"references/sbom/#cluster-management","title":"Cluster Management","text":"Component Version License Purpose Source Red Hat Advanced Cluster Management Latest Commercial Multi-cluster management Red Hat Karmada 1.6+ Apache 2.0 Multi-cluster orchestration Karmada Community Skupper 1.2+ Apache 2.0 Application connectivity Red Hat"},{"location":"references/sbom/#development-and-tooling","title":"Development and Tooling","text":""},{"location":"references/sbom/#command-line-tools","title":"Command Line Tools","text":"Component Version License Purpose Source kubectl Matches cluster version Apache 2.0 Kubernetes CLI Kubernetes helm 3.10+ Apache 2.0 Package manager CNCF kustomize 4.5+ Apache 2.0 Configuration management Kubernetes jq 1.6+ MIT JSON processing jq yq 4.30+ MIT YAML processing yq"},{"location":"references/sbom/#container-tools","title":"Container Tools","text":"Component Version License Purpose Source Podman 4.3+ Apache 2.0 Container management Red Hat Buildah 1.28+ Apache 2.0 Container image building Red Hat Skopeo 1.10+ Apache 2.0 Container image operations Red Hat"},{"location":"references/sbom/#operating-system-requirements","title":"Operating System Requirements","text":""},{"location":"references/sbom/#base-operating-system","title":"Base Operating System","text":"Component Version License Purpose Source Red Hat Enterprise Linux CoreOS 4.12+ Commercial Container-optimized OS Red Hat Red Hat Enterprise Linux 8.6+ or 9.0+ Commercial General-purpose OS Red Hat"},{"location":"references/sbom/#system-dependencies","title":"System Dependencies","text":"Component Version License Purpose Source systemd 239+ LGPL 2.1+ System and service manager systemd Docker/Podman 4.0+ Apache 2.0 Container runtime Various CRI-O 1.25+ Apache 2.0 Container runtime CRI-O runc 1.1+ Apache 2.0 Container runtime OCI"},{"location":"references/sbom/#integration-and-itsm","title":"Integration and ITSM","text":""},{"location":"references/sbom/#itsm-integration","title":"ITSM Integration","text":"Component Version License Purpose Source ServiceNow Latest Commercial ITSM platform ServiceNow ServiceNow MID Server Latest Commercial Integration middleware ServiceNow"},{"location":"references/sbom/#event-management","title":"Event Management","text":"Component Version License Purpose Source Splunk 8.2+ Commercial SIEM platform Splunk Elastic Security 7.17+ Elastic License Security analytics Elastic"},{"location":"references/sbom/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"references/sbom/#minimum-hardware-specifications","title":"Minimum Hardware Specifications","text":"Component Requirement Purpose CPU (Master Nodes) 4+ cores per node Control plane operations Memory (Master Nodes) 16GB+ per node Control plane operations Storage (Master Nodes) 120GB+ per node etcd and system data CPU (Worker Nodes) 8+ cores per node Workload execution Memory (Worker Nodes) 32GB+ per node VM and container workloads Storage (Worker Nodes) 500GB+ per node Application data Network 10Gbps+ High-performance networking Virtualization Intel VT-x/AMD-V Hardware virtualization support"},{"location":"references/sbom/#network-requirements","title":"Network Requirements","text":""},{"location":"references/sbom/#port-requirements","title":"Port Requirements","text":"Port Range Protocol Purpose 6443 TCP Kubernetes API server 22623 TCP Machine config server 80/443 TCP HTTP/HTTPS ingress 9000-9999 TCP Host level services 10250-10259 TCP Kubernetes node ports 30000-32767 TCP NodePort services"},{"location":"references/sbom/#license-summary","title":"License Summary","text":""},{"location":"references/sbom/#commercial-licenses-required","title":"Commercial Licenses Required","text":"<ul> <li>Red Hat OpenShift Container Platform</li> <li>OpenShift Virtualization</li> <li>Red Hat Advanced Cluster Security</li> <li>Red Hat Advanced Cluster Management</li> <li>Red Hat Enterprise Linux / CoreOS</li> <li>Dynatrace (monitoring platform)</li> <li>Rubrik (backup platform)</li> <li>ServiceNow (ITSM platform)</li> </ul>"},{"location":"references/sbom/#open-source-components","title":"Open Source Components","text":"<ul> <li>KubeVirt and related components (Apache 2.0)</li> <li>Cilium networking (Apache 2.0)</li> <li>Argo CD and GitOps tools (Apache 2.0)</li> <li>Kyverno policy engine (Apache 2.0)</li> <li>Prometheus monitoring stack (Apache 2.0)</li> <li>Various Kubernetes ecosystem tools (Apache 2.0)</li> </ul>"},{"location":"references/sbom/#version-compatibility-matrix","title":"Version Compatibility Matrix","text":""},{"location":"references/sbom/#supported-openshift-versions","title":"Supported OpenShift Versions","text":"OpenShift Version KubeVirt Cilium RHACM RHACS 4.12.x 4.12+ 1.12+ 2.7+ 4.2+ 4.13.x 4.13+ 1.13+ 2.8+ 4.3+ 4.14.x 4.14+ 1.14+ 2.9+ 4.4+ 4.15.x 4.15+ 1.15+ 2.10+ 4.5+"},{"location":"references/sbom/#security-considerations","title":"Security Considerations","text":""},{"location":"references/sbom/#cve-monitoring","title":"CVE Monitoring","text":"<p>All components should be regularly updated to address security vulnerabilities. Subscribe to security advisories from:</p> <ul> <li>Red Hat Security Advisories</li> <li>CNCF Security SIG</li> <li>Individual project security lists</li> <li>National Vulnerability Database (NVD)</li> </ul>"},{"location":"references/sbom/#supply-chain-security","title":"Supply Chain Security","text":"<ul> <li>Verify image signatures for all container images</li> <li>Use Red Hat certified operators when available</li> <li>Implement image scanning in CI/CD pipelines</li> <li>Maintain software inventory and track dependencies</li> </ul>"},{"location":"references/sbom/#maintenance-and-updates","title":"Maintenance and Updates","text":""},{"location":"references/sbom/#update-frequency","title":"Update Frequency","text":"<ul> <li>Security patches: As soon as available</li> <li>Minor versions: Monthly evaluation</li> <li>Major versions: Quarterly evaluation</li> <li>OpenShift: Follow Red Hat support lifecycle</li> </ul>"},{"location":"references/sbom/#end-of-life-planning","title":"End-of-Life Planning","text":"<p>Track EOL dates for all components and plan migrations:</p> <ul> <li>OpenShift: 18-month support lifecycle per version</li> <li>Kubernetes: 12-month support window</li> <li>Third-party components: Vendor-specific lifecycles</li> </ul> <p>Document Version: 1.0 Last Updated: 2025-08-04 Next Review: 2025-11-04</p> <p>This SBOM should be reviewed and updated quarterly or whenever significant changes are made to the RH OVE ecosystem architecture.</p>"},{"location":"use-cases/database-services-paas/","title":"Use Case: Database Services as Platform-as-a-Service (PaaS)","text":""},{"location":"use-cases/database-services-paas/#business-context","title":"Business Context","text":"<p>Organizations require flexible, scalable database solutions that can support various application needs while minimizing operational overhead. This use case demonstrates how to implement database services as Platform-as-a-Service (PaaS) offerings within the RH OVE ecosystem, providing self-service database provisioning, automated management, and seamless integration with application workloads.</p>"},{"location":"use-cases/database-services-paas/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/database-services-paas/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>OpenShift Data Foundation for persistent storage</li> <li>Database operators (PostgreSQL, MySQL, MongoDB, Redis operators)</li> <li>Service mesh for secure communication (OpenShift Service Mesh)</li> <li>Monitoring and observability stack (Prometheus, Grafana)</li> <li>Backup and disaster recovery solutions (OADP/Velero/Rubrik)</li> </ul>"},{"location":"use-cases/database-services-paas/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: Variable based on database workload (2-16 cores per instance)</li> <li>Memory: 4GB-64GB RAM per database instance</li> <li>Storage: High-performance persistent storage with snapshot capabilities</li> <li>Network: Low-latency networking for database replication and client connections</li> </ul>"},{"location":"use-cases/database-services-paas/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Database PaaS Control Plane\"\n        DB_OPERATOR[\"Database Operators\"]\n        SERVICE_CATALOG[\"Service Catalog\"]\n        PROVISIONING[\"Auto-Provisioning\"]\n        BACKUP_CONTROLLER[\"Backup Controller\"]\n    end\n\n    subgraph \"Database Instances\"\n        POSTGRES_CLUSTER[\"PostgreSQL Cluster\"]\n        MYSQL_CLUSTER[\"MySQL Cluster\"]\n        MONGO_CLUSTER[\"MongoDB Cluster\"]\n        REDIS_CLUSTER[\"Redis Cluster\"]\n    end\n\n    subgraph \"Application Layer\"\n        APP1[\"Application 1\"]\n        APP2[\"Application 2\"]\n        MICROSERVICES[\"Microservices\"]\n    end\n\n    subgraph \"Storage Layer\"\n        ODF[\"OpenShift Data Foundation\"]\n        SNAPSHOTS[\"Volume Snapshots\"]\n        BACKUPS[\"Backup Storage\"]\n    end\n\n    SERVICE_CATALOG --&gt; DB_OPERATOR\n    DB_OPERATOR --&gt; POSTGRES_CLUSTER\n    DB_OPERATOR --&gt; MYSQL_CLUSTER\n    DB_OPERATOR --&gt; MONGO_CLUSTER\n    DB_OPERATOR --&gt; REDIS_CLUSTER\n\n    APP1 --&gt; POSTGRES_CLUSTER\n    APP2 --&gt; MYSQL_CLUSTER\n    MICROSERVICES --&gt; REDIS_CLUSTER\n\n    POSTGRES_CLUSTER --&gt; ODF\n    MYSQL_CLUSTER --&gt; ODF\n    MONGO_CLUSTER --&gt; ODF\n    REDIS_CLUSTER --&gt; ODF\n\n    BACKUP_CONTROLLER --&gt; SNAPSHOTS\n    SNAPSHOTS --&gt; BACKUPS\n\n    style DB_OPERATOR fill:#f9f,stroke:#333\n    style SERVICE_CATALOG fill:#ff9,stroke:#333\n    style ODF fill:#9ff,stroke:#333</code></pre>"},{"location":"use-cases/database-services-paas/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/database-services-paas/#step-1-install-database-operators","title":"Step 1: Install Database Operators","text":""},{"location":"use-cases/database-services-paas/#postgresql-operator-deployment","title":"PostgreSQL Operator Deployment","text":"<pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: postgresql-operator\n  namespace: database-operators\nspec:\n  channel: stable\n  name: postgresql-operator\n  source: certified-operators\n  sourceNamespace: openshift-marketplace\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: database-operators\n  labels:\n    openshift.io/cluster-monitoring: \"true\"\n</code></pre>"},{"location":"use-cases/database-services-paas/#mysql-operator-deployment","title":"MySQL Operator Deployment","text":"<pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: mysql-operator\n  namespace: database-operators\nspec:\n  channel: stable\n  name: mysql-operator\n  source: certified-operators\n  sourceNamespace: openshift-marketplace\n</code></pre>"},{"location":"use-cases/database-services-paas/#step-2-create-database-service-templates","title":"Step 2: Create Database Service Templates","text":""},{"location":"use-cases/database-services-paas/#postgresql-service-template","title":"PostgreSQL Service Template","text":"<pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-template\n  namespace: database-services\nspec:\n  instances: 3\n\n  postgresql:\n    parameters:\n      max_connections: \"200\"\n      shared_buffers: \"256MB\"\n      effective_cache_size: \"1GB\"\n      work_mem: \"4MB\"\n      maintenance_work_mem: \"64MB\"\n\n  bootstrap:\n    initdb:\n      database: app_db\n      owner: app_user\n      secret:\n        name: postgres-credentials\n\n  storage:\n    size: 100Gi\n    storageClass: fast-ssd\n\n  resources:\n    requests:\n      memory: \"2Gi\"\n      cpu: \"1\"\n    limits:\n      memory: \"4Gi\"\n      cpu: \"2\"\n\n  monitoring:\n    enabled: true\n    customQueriesConfigMap:\n    - name: postgres-monitoring\n      key: custom-queries.yaml\n\n  backup:\n    barmanObjectStore:\n      destinationPath: s3://db-backups/postgres\n      s3Credentials:\n        accessKeyId:\n          name: backup-credentials\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: backup-credentials\n          key: SECRET_ACCESS_KEY\n      wal:\n        retention: \"7d\"\n      data:\n        retention: \"30d\"\n</code></pre>"},{"location":"use-cases/database-services-paas/#mysql-service-template","title":"MySQL Service Template","text":"<pre><code>apiVersion: mysql.oracle.com/v2\nkind: InnoDBCluster\nmetadata:\n  name: mysql-template\n  namespace: database-services\nspec:\n  secretName: mysql-credentials\n  tlsUseSelfSigned: true\n  instances: 3\n  router:\n    instances: 2\n\n  datadirVolumeClaimTemplate:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 100Gi\n    storageClassName: fast-ssd\n\n  mycnf: |\n    [mysqld]\n    max_connections = 500\n    innodb_buffer_pool_size = 2G\n    innodb_log_file_size = 256M\n    binlog_expire_logs_seconds = 604800\n\n  resources:\n    requests:\n      memory: \"2Gi\"\n      cpu: \"1\"\n    limits:\n      memory: \"4Gi\"\n      cpu: \"2\"\n\n  backupProfiles:\n  - name: daily-backup\n    dumpInstance:\n      dumpOptions:\n        users: true\n        excludeSchemas: [\"information_schema\", \"performance_schema\"]\n    schedule: \"0 2 * * *\"\n    backupRetentionDays: 30\n</code></pre>"},{"location":"use-cases/database-services-paas/#step-3-implement-self-service-provisioning","title":"Step 3: Implement Self-Service Provisioning","text":""},{"location":"use-cases/database-services-paas/#service-catalog-integration","title":"Service Catalog Integration","text":"<pre><code>apiVersion: servicecatalog.k8s.io/v1beta1\nkind: ServiceClass\nmetadata:\n  name: postgresql-service\nspec:\n  clusterServiceBrokerName: database-service-broker\n  externalName: postgresql\n  description: \"Managed PostgreSQL Database Service\"\n  externalMetadata:\n    displayName: \"PostgreSQL Database\"\n    imageUrl: \"https://example.com/postgresql-icon.png\"\n    longDescription: \"High-availability PostgreSQL database with automated backups and monitoring\"\n    providerDisplayName: \"Database Team\"\n    supportUrl: \"https://example.com/support\"\n  plans:\n  - name: small\n    externalID: postgres-small\n    description: \"Small PostgreSQL instance (2 CPU, 4GB RAM, 100GB storage)\"\n    free: false\n    externalMetadata:\n      displayName: \"Small\"\n      bullets:\n      - \"2 CPU cores\"\n      - \"4GB RAM\"\n      - \"100GB storage\"\n      - \"Daily backups\"\n  - name: medium\n    externalID: postgres-medium\n    description: \"Medium PostgreSQL instance (4 CPU, 8GB RAM, 500GB storage)\"\n    free: false\n    externalMetadata:\n      displayName: \"Medium\"\n      bullets:\n      - \"4 CPU cores\"\n      - \"8GB RAM\"\n      - \"500GB storage\"\n      - \"Daily backups\"\n      - \"Read replicas\"\n</code></pre>"},{"location":"use-cases/database-services-paas/#database-provisioning-operator","title":"Database Provisioning Operator","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: database-provisioning-operator\n  namespace: database-operators\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: database-provisioning-operator\n  template:\n    metadata:\n      labels:\n        app: database-provisioning-operator\n    spec:\n      containers:\n      - name: operator\n        image: quay.io/example/database-provisioning-operator:latest\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"\"\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: \"database-provisioning-operator\"\n        ports:\n        - containerPort: 8080\n          name: metrics\n        - containerPort: 8081\n          name: health\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n</code></pre>"},{"location":"use-cases/database-services-paas/#step-4-configure-automated-backup-and-recovery","title":"Step 4: Configure Automated Backup and Recovery","text":""},{"location":"use-cases/database-services-paas/#backup-schedule-configuration","title":"Backup Schedule Configuration","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: database-backup-scheduler\n  namespace: database-services\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup-job\n            image: quay.io/example/database-backup:latest\n            env:\n            - name: BACKUP_TYPE\n              value: \"full\"\n            - name: RETENTION_DAYS\n              value: \"30\"\n            - name: S3_BUCKET\n              value: \"database-backups\"\n            - name: AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name: backup-credentials\n                  key: access-key-id\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: backup-credentials\n                  key: secret-access-key\n            command:\n            - /bin/bash\n            - -c\n            - |\n              #!/bin/bash\n              set -e\n\n              # Discover all database instances\n              for db in $(kubectl get postgresql,mysql,mongodb -o name --all-namespaces); do\n                echo \"Backing up $db\"\n                backup-database.sh \"$db\"\n              done\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"use-cases/database-services-paas/#step-5-implement-monitoring-and-alerting","title":"Step 5: Implement Monitoring and Alerting","text":""},{"location":"use-cases/database-services-paas/#database-monitoring-configuration","title":"Database Monitoring Configuration","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: database-services-monitor\n  namespace: database-services\n  labels:\n    monitoring: database-services\nspec:\n  selector:\n    matchLabels:\n      monitoring: enabled\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n    honorLabels: true\n---\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: database-services-alerts\n  namespace: database-services\nspec:\n  groups:\n  - name: database.alerts\n    rules:\n    - alert: DatabaseInstanceDown\n      expr: up{job=\"database-services\"} == 0\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Database instance {{ $labels.instance }} is down\"\n        description: \"Database instance {{ $labels.instance }} has been down for more than 5 minutes.\"\n\n    - alert: DatabaseHighConnections\n      expr: database_connections_active / database_connections_max &gt; 0.8\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Database {{ $labels.instance }} has high connection usage\"\n        description: \"Database {{ $labels.instance }} is using {{ $value | humanizePercentage }} of available connections.\"\n\n    - alert: DatabaseSlowQueries\n      expr: rate(database_slow_queries_total[5m]) &gt; 10\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Database {{ $labels.instance }} has high slow query rate\"\n        description: \"Database {{ $labels.instance }} has {{ $value }} slow queries per second.\"\n\n    - alert: DatabaseBackupFailed\n      expr: database_backup_last_success_timestamp &lt; (time() - 86400)\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Database backup failed for {{ $labels.instance }}\"\n        description: \"Database backup for {{ $labels.instance }} has not succeeded in the last 24 hours.\"\n</code></pre>"},{"location":"use-cases/database-services-paas/#step-6-implement-database-security","title":"Step 6: Implement Database Security","text":""},{"location":"use-cases/database-services-paas/#network-policies-for-database-isolation","title":"Network Policies for Database Isolation","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-isolation\n  namespace: database-services\nspec:\n  podSelector:\n    matchLabels:\n      tier: database\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: application-services\n    - podSelector:\n        matchLabels:\n          tier: application\n    ports:\n    - protocol: TCP\n      port: 5432  # PostgreSQL\n    - protocol: TCP\n      port: 3306  # MySQL\n    - protocol: TCP\n      port: 27017 # MongoDB\n    - protocol: TCP\n      port: 6379  # Redis\n  egress:\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 53   # DNS\n    - protocol: UDP\n      port: 53   # DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: backup-services\n    ports:\n    - protocol: TCP\n      port: 443  # HTTPS for backup storage\n</code></pre>"},{"location":"use-cases/database-services-paas/#database-secret-management","title":"Database Secret Management","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: database-secret-store\n  namespace: database-services\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"database-secrets\"\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: postgres-credentials\n  namespace: database-services\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: database-secret-store\n    kind: SecretStore\n  target:\n    name: postgres-credentials\n    creationPolicy: Owner\n    template:\n      type: Opaque\n      data:\n        username: \"{{ .username }}\"\n        password: \"{{ .password }}\"\n        database: \"{{ .database }}\"\n  data:\n  - secretKey: username\n    remoteRef:\n      key: database/postgres\n      property: username\n  - secretKey: password\n    remoteRef:\n      key: database/postgres\n      property: password\n  - secretKey: database\n    remoteRef:\n      key: database/postgres\n      property: database\n</code></pre>"},{"location":"use-cases/database-services-paas/#application-integration-examples","title":"Application Integration Examples","text":""},{"location":"use-cases/database-services-paas/#spring-boot-application-configuration","title":"Spring Boot Application Configuration","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spring-app\n  namespace: application-services\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: spring-app\n  template:\n    metadata:\n      labels:\n        app: spring-app\n        tier: application\n    spec:\n      containers:\n      - name: app\n        image: quay.io/example/spring-app:latest\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: \"jdbc:postgresql://postgres-service.database-services.svc.cluster.local:5432/app_db\"\n        - name: SPRING_DATASOURCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: app-db-credentials\n              key: username\n        - name: SPRING_DATASOURCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: app-db-credentials\n              key: password\n        - name: SPRING_JPA_HIBERNATE_DDL_AUTO\n          value: \"validate\"\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n</code></pre>"},{"location":"use-cases/database-services-paas/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/database-services-paas/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/database-services-paas/#database-connection-issues","title":"Database Connection Issues","text":"<ul> <li>Issue: Applications cannot connect to database services</li> <li>Solution:</li> <li>Verify network policies allow traffic between application and database namespaces</li> <li>Check service discovery and DNS resolution</li> <li>Validate database credentials and authentication</li> </ul>"},{"location":"use-cases/database-services-paas/#performance-issues","title":"Performance Issues","text":"<ul> <li>Issue: Slow database queries and high latency</li> <li>Solution:</li> <li>Review database configuration parameters</li> <li>Analyze slow query logs and optimize indexes</li> <li>Scale database resources (CPU, memory, storage IOPS)</li> <li>Implement connection pooling</li> </ul>"},{"location":"use-cases/database-services-paas/#backup-and-recovery-failures","title":"Backup and Recovery Failures","text":"<ul> <li>Issue: Database backups failing or restoration issues</li> <li>Solution:</li> <li>Verify backup storage credentials and permissions</li> <li>Check backup retention policies and storage quotas</li> <li>Test backup restoration procedures regularly</li> <li>Monitor backup job logs for errors</li> </ul>"},{"location":"use-cases/database-services-paas/#resource-constraints","title":"Resource Constraints","text":"<ul> <li>Issue: Database instances running out of resources</li> <li>Solution:</li> <li>Implement resource monitoring and alerting</li> <li>Configure horizontal and vertical scaling policies</li> <li>Optimize database configuration for workload patterns</li> <li>Consider database sharding for large datasets</li> </ul>"},{"location":"use-cases/database-services-paas/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/database-services-paas/#database-design-and-configuration","title":"Database Design and Configuration","text":"<ul> <li>Resource Planning: Size database instances based on expected workload patterns</li> <li>Configuration Tuning: Optimize database parameters for specific use cases</li> <li>Connection Management: Implement connection pooling and limit concurrent connections</li> <li>Index Strategy: Create appropriate indexes for query performance</li> </ul>"},{"location":"use-cases/database-services-paas/#security-and-compliance","title":"Security and Compliance","text":"<ul> <li>Encryption: Enable encryption at rest and in transit for all database communications</li> <li>Access Control: Implement least-privilege access using RBAC and network policies</li> <li>Audit Logging: Enable database audit logging for compliance requirements</li> <li>Secret Rotation: Implement automated secret rotation for database credentials</li> </ul>"},{"location":"use-cases/database-services-paas/#operations-and-maintenance","title":"Operations and Maintenance","text":"<ul> <li>Monitoring: Implement comprehensive monitoring for performance, availability, and capacity</li> <li>Backup Strategy: Implement automated backups with tested restoration procedures</li> <li>Disaster Recovery: Plan and test disaster recovery procedures regularly</li> <li>Maintenance Windows: Schedule regular maintenance windows for updates and optimizations</li> </ul>"},{"location":"use-cases/database-services-paas/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/database-services-paas/#gitops-integration","title":"GitOps Integration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: database-services\n  namespace: argocd\nspec:\n  project: platform-services\n  source:\n    repoURL: https://git.example.com/database-services-config\n    targetRevision: HEAD\n    path: overlays/production\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: database-services\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"use-cases/database-services-paas/#multi-cluster-database-federation","title":"Multi-Cluster Database Federation","text":"<ul> <li>Cross-Cluster Replication: Implement database replication across multiple clusters</li> <li>Global Load Balancing: Use global load balancers for database read replicas</li> <li>Disaster Recovery: Maintain database replicas in different geographical regions</li> </ul> <p>This comprehensive guide provides everything needed to implement Database Services as PaaS within the RH OVE ecosystem, enabling self-service database provisioning, automated management, and seamless integration with application workloads while maintaining enterprise-grade security, performance, and reliability.</p>"},{"location":"use-cases/disaster-recovery/","title":"Use Case: Disaster Recovery","text":""},{"location":"use-cases/disaster-recovery/#business-context","title":"Business Context","text":"<p>Disaster recovery is a crucial aspect of business continuity, ensuring that workloads can be swiftly restored following catastrophic events. This use case outlines strategies and tools for implementing effective disaster recovery plans within the RH OVE ecosystem.</p>"},{"location":"use-cases/disaster-recovery/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/disaster-recovery/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ clusters with multi-cluster management enabled</li> <li>Cross-cluster networking with VPN or direct connectivity</li> <li>Data replication and backup solutions</li> <li>Disaster recovery orchestration tools (Red Hat Advanced Cluster Management - RHACM)</li> </ul>"},{"location":"use-cases/disaster-recovery/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Compute: Sufficient capacity on recovery clusters</li> <li>Storage: Redundant storage solutions with replication</li> <li>Network: Reliable, high-speed connections between primary and secondary sites</li> </ul>"},{"location":"use-cases/disaster-recovery/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Primary Cluster\"\n        VM1[\"VM 1\"]\n        VM2[\"VM 2\"]\n        PRIMARY_STORAGE[\"Primary Storage\"]\n    end\n\n    subgraph \"Disaster Recovery Cluster\"\n        DR_VM1[\"DR VM 1\"]\n        DR_VM2[\"DR VM 2\"]\n        DR_STORAGE[\"DR Storage\"]\n    end\n\n    PRIMARY_STORAGE -- Replication --&gt; DR_STORAGE\n    VM1 -- Synchronization --&gt; DR_VM1\n    VM2 -- Synchronization --&gt; DR_VM2\n\n    style PRIMARY_STORAGE fill:#f99,stroke:#333\n    style DR_STORAGE fill:#99f,stroke:#333</code></pre>"},{"location":"use-cases/disaster-recovery/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/disaster-recovery/#step-1-plan-and-prepare","title":"Step 1: Plan and Prepare","text":""},{"location":"use-cases/disaster-recovery/#define-disaster-recovery-objectives","title":"Define Disaster Recovery Objectives","text":"<ul> <li>Identify RTO (Recovery Time Objective) and RPO (Recovery Point Objective)</li> </ul>"},{"location":"use-cases/disaster-recovery/#inventory-assessment","title":"Inventory Assessment","text":"<ul> <li>Document existing resources and dependencies</li> </ul>"},{"location":"use-cases/disaster-recovery/#step-2-configure-data-replication","title":"Step 2: Configure Data Replication","text":""},{"location":"use-cases/disaster-recovery/#persistent-storage-replication","title":"Persistent Storage Replication","text":"<ul> <li>Configure synchronous or asynchronous replication between primary and DR sites.</li> </ul> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: dr-replication-demo\n  namespace: storage-replication\nspec:\n  selector:\n    matchLabels:\n      app: replication\n  serviceName: \"replication\"\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: replication\n    spec:\n      containers:\n      - name: replication-agent\n        image: replication-agent:latest\n        args:\n        - --source-pvc\n        - source-storage-pvc\n        - --target-pvc\n        - target-storage-pvc\n</code></pre>"},{"location":"use-cases/disaster-recovery/#step-3-implement-cross-cluster-networking","title":"Step 3: Implement Cross-Cluster Networking","text":""},{"location":"use-cases/disaster-recovery/#vpn-configuration-for-cluster-connectivity","title":"VPN Configuration for Cluster Connectivity","text":"<ul> <li>Set up VPN tunnels or configure direct connectivity between cluster sites.</li> </ul>"},{"location":"use-cases/disaster-recovery/#step-4-deploy-dr-orchestration-tools","title":"Step 4: Deploy DR Orchestration Tools","text":""},{"location":"use-cases/disaster-recovery/#rhacm-configuration","title":"RHACM Configuration","text":"<ul> <li>Deploy Red Hat Advanced Cluster Management for cluster failover management.</li> </ul> <pre><code>apiVersion: cluster.open-cluster-management.io/v1\nkind: ManagedCluster\nmetadata:\n  name: disaster-recovery-cluster\nspec:\n  hubAcceptsClient: true\n  managedClusterClientConfigs:\n  - url: https://api.dr-cluster.example.com:6443\n</code></pre>"},{"location":"use-cases/disaster-recovery/#step-5-automate-failover-and-recovery","title":"Step 5: Automate Failover and Recovery","text":""},{"location":"use-cases/disaster-recovery/#failover-scripts-and-automation","title":"Failover Scripts and Automation","text":"<ul> <li>Develop scripts to automate the failover process based on RHACM policies.</li> </ul> <pre><code>#!/bin/bash\n# Failover script for disaster recovery activation\n\n# Scale down primary workloads\nkubectl scale deployment --all --replicas=0 -n primary-workloads\n\n# Scale up DR workloads\nkubectl scale deployment --all --replicas=1 -n disaster-recovery-workloads\n\n# Update DNS settings\nupdate-dns --zone=example.com --record=*.example.com --new-ip=dr-cluster-ip\n</code></pre>"},{"location":"use-cases/disaster-recovery/#step-6-testing-and-validation","title":"Step 6: Testing and Validation","text":""},{"location":"use-cases/disaster-recovery/#disaster-recovery-drills","title":"Disaster Recovery Drills","text":"<ul> <li>Conduct regular DR drills to test and validate recovery procedures.</li> </ul> <pre><code># Trigger disaster recovery drill\nrun-drill --cluster=disaster-recovery-cluster --scenario=full-cluster-failure\n</code></pre>"},{"location":"use-cases/disaster-recovery/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/disaster-recovery/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/disaster-recovery/#replication-lag","title":"Replication Lag","text":"<ul> <li>Issue: Data replication falls behind</li> <li>Solution: </li> <li>Increase network bandwidth</li> <li>Optimize replication frequencies</li> <li>Monitor replication service for bottlenecks</li> </ul>"},{"location":"use-cases/disaster-recovery/#failover-errors","title":"Failover Errors","text":"<ul> <li>Issue: Failover task errors or delays</li> <li>Solution:</li> <li>Verify failover scripts and automation procedures</li> <li>Test DNS updates and propagation</li> <li>Check cluster configuration consistency</li> </ul>"},{"location":"use-cases/disaster-recovery/#network-connectivity-issues","title":"Network Connectivity Issues","text":"<ul> <li>Issue: VPN or network interruptions</li> <li>Solution:</li> <li>Test alternate routes and consider multi-path routing</li> <li>Verify firewall and security group configurations</li> <li>Implement continuous network monitoring</li> </ul>"},{"location":"use-cases/disaster-recovery/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/disaster-recovery/#strategy-and-planning","title":"Strategy and Planning","text":"<ul> <li>Comprehensive Planning: Develop detailed DR plans aligned with business priorities</li> <li>Periodic Reviews: Regularly review DR strategies and update based on changes in infrastructure</li> <li>Stakeholder Engagement: Involve all relevant stakeholders in DR planning and testing</li> </ul>"},{"location":"use-cases/disaster-recovery/#technology-and-tools","title":"Technology and Tools","text":"<ul> <li>Automation: Leverage automation for failover processes to minimize human error</li> <li>Monitoring and Alerts: Implement monitoring and alerting for quick detection of failures</li> <li>Compliance and Auditing: Ensure DR plans meet compliance and regulatory requirements</li> </ul>"},{"location":"use-cases/disaster-recovery/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/disaster-recovery/#multi-cluster-management","title":"Multi-Cluster Management","text":"<ul> <li>Use RHACM for managing multiple clusters, facilitating disaster recovery coordination</li> </ul>"},{"location":"use-cases/disaster-recovery/#environmental-parity","title":"Environmental Parity","text":"<ul> <li>Ensure consistency in configurations between primary and secondary environments</li> </ul> <p>This guide provides the steps and best practices necessary to establish robust disaster recovery systems within the RH OVE ecosystem, ensuring business continuity and data availability even in the event of a major failure or disaster.</p>"},{"location":"use-cases/end-to-end-observability/","title":"Use Case: End-to-End Application Observability in RH OVE","text":""},{"location":"use-cases/end-to-end-observability/#business-context","title":"Business Context","text":"<p>In the Red Hat OpenShift Virtualization Engine (RH OVE) ecosystem, comprehensive observability is essential for monitoring both containerized applications and virtual machines, understanding performance bottlenecks, troubleshooting issues, and ensuring optimal resource utilization across hybrid workloads. This use case demonstrates two complementary approaches: native OpenShift observability tools and integration with Dynatrace for enterprise-grade observability.</p>"},{"location":"use-cases/end-to-end-observability/#what-developers-need-to-expose","title":"What Developers Need to Expose","text":"<p>For effective end-to-end observability, developers must instrument their applications to expose:</p>"},{"location":"use-cases/end-to-end-observability/#required-metrics","title":"Required Metrics","text":"<ul> <li>Business Metrics: Transaction counts, success rates, revenue metrics</li> <li>Application Metrics: Response times, error rates, throughput</li> <li>Resource Metrics: CPU, memory, disk I/O, network usage</li> <li>Custom Metrics: Domain-specific KPIs and performance indicators</li> </ul>"},{"location":"use-cases/end-to-end-observability/#required-traces","title":"Required Traces","text":"<ul> <li>Request Traces: End-to-end request flow across microservices</li> <li>Database Traces: SQL queries and database connection metrics</li> <li>External Service Traces: API calls to third-party services</li> <li>Async Operations: Message queue operations, background jobs</li> </ul>"},{"location":"use-cases/end-to-end-observability/#required-logs","title":"Required Logs","text":"<ul> <li>Structured Logs: JSON formatted with consistent fields</li> <li>Error Logs: Exception details with stack traces</li> <li>Audit Logs: Security and compliance events</li> <li>Performance Logs: Slow queries, long-running operations</li> </ul>"},{"location":"use-cases/end-to-end-observability/#health-endpoints","title":"Health Endpoints","text":"<ul> <li>Liveness Probes: <code>/health/live</code> - Application is running</li> <li>Readiness Probes: <code>/health/ready</code> - Application ready to serve traffic</li> <li>Metrics Endpoint: <code>/metrics</code> - Prometheus-formatted metrics</li> <li>Info Endpoint: <code>/info</code> - Application version and build information</li> </ul>"},{"location":"use-cases/end-to-end-observability/#1-native-openshift-observability","title":"1. Native OpenShift Observability","text":""},{"location":"use-cases/end-to-end-observability/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with built-in monitoring stack</li> <li>OpenShift Data Foundation for persistent storage</li> <li>Red Hat OpenShift Logging (based on Loki)</li> <li>Red Hat OpenShift distributed tracing (Jaeger)</li> <li>Cilium Hubble for network observability</li> <li>KubeVirt monitoring for VM workloads</li> </ul>"},{"location":"use-cases/end-to-end-observability/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"RH OVE Application Layer\"\n        CONTAINER_APPS[\"Container Applications\"]\n        VM_WORKLOADS[\"VM Workloads\"]\n        HYBRID_APPS[\"Hybrid Applications\"]\n    end\n\n    subgraph \"OpenShift Native Observability Stack\"\n        OCP_PROMETHEUS[\"OpenShift Prometheus\"]\n        OCP_GRAFANA[\"OpenShift Console &amp; Grafana\"]\n        OCP_JAEGER[\"Red Hat OpenShift distributed tracing\"]\n        OCP_LOKI[\"Red Hat OpenShift Logging\"]\n        CILIUM_HUBBLE[\"Cilium Hubble\"]\n        KUBEVIRT_METRICS[\"KubeVirt Metrics\"]\n    end\n\n    subgraph \"Storage &amp; Processing\"\n        ODF_STORAGE[\"OpenShift Data Foundation\"]\n        ALERTMANAGER[\"AlertManager\"]\n    end\n\n    CONTAINER_APPS --&gt; OCP_PROMETHEUS\n    VM_WORKLOADS --&gt; KUBEVIRT_METRICS\n    HYBRID_APPS --&gt; OCP_PROMETHEUS\n\n    CONTAINER_APPS --&gt; OCP_JAEGER\n    HYBRID_APPS --&gt; OCP_JAEGER\n\n    CONTAINER_APPS --&gt; OCP_LOKI\n    VM_WORKLOADS --&gt; OCP_LOKI\n    HYBRID_APPS --&gt; OCP_LOKI\n\n    CILIUM_HUBBLE --&gt; OCP_PROMETHEUS\n    KUBEVIRT_METRICS --&gt; OCP_PROMETHEUS\n\n    OCP_PROMETHEUS --&gt; OCP_GRAFANA\n    OCP_JAEGER --&gt; OCP_GRAFANA\n    OCP_LOKI --&gt; OCP_GRAFANA\n\n    OCP_PROMETHEUS --&gt; ALERTMANAGER\n    OCP_PROMETHEUS --&gt; ODF_STORAGE\n    OCP_LOKI --&gt; ODF_STORAGE\n\n    style OCP_PROMETHEUS fill:#f9f,stroke:#333\n    style OCP_GRAFANA fill:#99f,stroke:#333\n    style OCP_JAEGER fill:#9ff,stroke:#333\n    style OCP_LOKI fill:#ff9,stroke:#333\n    style CILIUM_HUBBLE fill:#f99,stroke:#333</code></pre>"},{"location":"use-cases/end-to-end-observability/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/end-to-end-observability/#step-1-enable-openshift-built-in-monitoring","title":"Step 1: Enable OpenShift Built-in Monitoring","text":""},{"location":"use-cases/end-to-end-observability/#configure-user-workload-monitoring","title":"Configure User Workload Monitoring","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkloadMonitoring: true\n    prometheusK8s:\n      retention: 30d\n      volumeClaimTemplate:\n        spec:\n          storageClassName: ocs-storagecluster-ceph-rbd\n          resources:\n            requests:\n              storage: 100Gi\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-2-application-instrumentation-for-container-applications","title":"Step 2: Application Instrumentation for Container Applications","text":""},{"location":"use-cases/end-to-end-observability/#comprehensive-metrics-configuration-go-example","title":"Comprehensive Metrics Configuration (Go Example)","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"net/http\"\n    \"time\"\n\n    \"github.com/prometheus/client_golang/prometheus\"\n    \"github.com/prometheus/client_golang/prometheus/promauto\"\n    \"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\nvar (\n    // Business metrics\n    httpRequestsTotal = promauto.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"myapp_http_requests_total\",\n            Help: \"Total number of HTTP requests by status code and method\",\n        },\n        []string{\"method\", \"status_code\", \"endpoint\"},\n    )\n\n    // Performance metrics\n    httpRequestDuration = promauto.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name: \"myapp_http_request_duration_seconds\",\n            Help: \"HTTP request duration in seconds\",\n            Buckets: prometheus.DefBuckets,\n        },\n        []string{\"method\", \"endpoint\"},\n    )\n\n    // Resource metrics\n    activeConnections = promauto.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"myapp_active_connections\",\n            Help: \"Number of active connections\",\n        },\n    )\n\n    // Custom business metrics\n    ordersProcessed = promauto.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"myapp_orders_processed_total\",\n            Help: \"Total number of orders processed\",\n        },\n        []string{\"status\"},\n    )\n)\n\nfunc instrumentHandler(next http.HandlerFunc, endpoint string) http.HandlerFunc {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n\n        // Process request\n        next.ServeHTTP(w, r)\n\n        // Record metrics\n        duration := time.Since(start).Seconds()\n        httpRequestDuration.WithLabelValues(r.Method, endpoint).Observe(duration)\n        httpRequestsTotal.WithLabelValues(r.Method, \"200\", endpoint).Inc()\n    })\n}\n\nfunc main() {\n    // Health endpoints\n    http.HandleFunc(\"/health/live\", func(w http.ResponseWriter, r *http.Request) {\n        w.WriteHeader(http.StatusOK)\n        w.Write([]byte(\"alive\"))\n    })\n\n    http.HandleFunc(\"/health/ready\", func(w http.ResponseWriter, r *http.Request) {\n        // Check dependencies (DB, external services)\n        w.WriteHeader(http.StatusOK)\n        w.Write([]byte(\"ready\"))\n    })\n\n    // Metrics endpoint\n    http.Handle(\"/metrics\", promhttp.Handler())\n\n    // Business endpoints with instrumentation\n    http.HandleFunc(\"/api/orders\", instrumentHandler(ordersHandler, \"/api/orders\"))\n\n    http.ListenAndServe(\":8080\", nil)\n}\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#distributed-tracing-configuration-nodejs-example","title":"Distributed Tracing Configuration (Node.js Example)","text":"<pre><code>const { NodeSDK } = require('@opentelemetry/auto-instrumentations-node');\nconst { JaegerExporter } = require('@opentelemetry/exporter-jaeger');\nconst { Resource } = require('@opentelemetry/resources');\nconst { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\n\n// Configure Jaeger exporter for OpenShift distributed tracing\nconst jaegerExporter = new JaegerExporter({\n  endpoint: 'http://jaeger-collector.openshift-distributed-tracing-system.svc.cluster.local:14268/api/traces',\n});\n\n// Initialize OpenTelemetry SDK\nconst sdk = new NodeSDK({\n  resource: new Resource({\n    [SemanticResourceAttributes.SERVICE_NAME]: 'myapp-service',\n    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.APP_VERSION || '1.0.0',\n    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',\n  }),\n  traceExporter: jaegerExporter,\n});\n\nsdk.start();\n\nconst express = require('express');\nconst { trace, context } = require('@opentelemetry/api');\nconst app = express();\n\n// Custom tracing for business operations\napp.get('/api/orders/:id', async (req, res) =&gt; {\n  const tracer = trace.getTracer('myapp');\n\n  await tracer.startActiveSpan('process_order', async (span) =&gt; {\n    try {\n      // Add custom attributes\n      span.setAttributes({\n        'order.id': req.params.id,\n        'user.id': req.headers['user-id'],\n        'operation.type': 'order_processing'\n      });\n\n      // Simulate database call with tracing\n      await tracer.startActiveSpan('database_query', async (dbSpan) =&gt; {\n        // Database operation\n        dbSpan.setAttributes({\n          'db.operation': 'SELECT',\n          'db.table': 'orders'\n        });\n        dbSpan.end();\n      });\n\n      // Simulate external API call\n      await tracer.startActiveSpan('external_api_call', async (apiSpan) =&gt; {\n        apiSpan.setAttributes({\n          'http.method': 'POST',\n          'http.url': 'https://payment-service/process'\n        });\n        apiSpan.end();\n      });\n\n      res.json({ orderId: req.params.id, status: 'processed' });\n    } catch (error) {\n      span.recordException(error);\n      span.setStatus({ code: trace.SpanStatusCode.ERROR, message: error.message });\n      res.status(500).json({ error: 'Processing failed' });\n    } finally {\n      span.end();\n    }\n  });\n});\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#structured-logging-configuration","title":"Structured Logging Configuration","text":"<pre><code># Python example with structured logging for OpenShift Logging\nimport logging\nimport json\nimport sys\nfrom datetime import datetime\n\nclass StructuredLogger:\n    def __init__(self, service_name):\n        self.service_name = service_name\n        self.logger = logging.getLogger(service_name)\n        self.logger.setLevel(logging.INFO)\n\n        # Configure JSON formatter for OpenShift Logging\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(self.JsonFormatter())\n        self.logger.addHandler(handler)\n\n    class JsonFormatter(logging.Formatter):\n        def format(self, record):\n            log_entry = {\n                'timestamp': datetime.utcnow().isoformat() + 'Z',\n                'level': record.levelname,\n                'service': record.name,\n                'message': record.getMessage(),\n            }\n\n            # Add custom fields if present\n            if hasattr(record, 'user_id'):\n                log_entry['user_id'] = record.user_id\n            if hasattr(record, 'trace_id'):\n                log_entry['trace_id'] = record.trace_id\n            if hasattr(record, 'span_id'):\n                log_entry['span_id'] = record.span_id\n\n            return json.dumps(log_entry)\n\n    def info(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.info(message, extra=extra)\n\n    def error(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.error(message, extra=extra)\n\n# Usage in application\nlogger = StructuredLogger('myapp-service')\n\ndef process_order(order_id, user_id):\n    logger.info(\n        \"Processing order\",\n        user_id=user_id,\n        order_id=order_id,\n        operation='order_processing'\n    )\n\n    try:\n        # Business logic\n        result = do_business_logic()\n        logger.info(\n            \"Order processed successfully\",\n            user_id=user_id,\n            order_id=order_id,\n            result=result\n        )\n    except Exception as e:\n        logger.error(\n            \"Order processing failed\",\n            user_id=user_id,\n            order_id=order_id,\n            error=str(e),\n            stack_trace=traceback.format_exc()\n        )\n        raise\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-3-configure-openshift-native-observability-components","title":"Step 3: Configure OpenShift Native Observability Components","text":""},{"location":"use-cases/end-to-end-observability/#enable-red-hat-openshift-logging","title":"Enable Red Hat OpenShift Logging","text":"<pre><code>apiVersion: logging.coreos.com/v1\nkind: ClusterLogging\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  managementState: Managed\n  logStore:\n    type: lokistack\n    lokistack:\n      name: logging-loki\n  collection:\n    type: vector\n    vector:\n      resources:\n        limits:\n          memory: 1Gi\n        requests:\n          memory: 512Mi\n  visualization:\n    type: ocp-console\n---\napiVersion: loki.grafana.com/v1\nkind: LokiStack\nmetadata:\n  name: logging-loki\n  namespace: openshift-logging\nspec:\n  size: 1x.small\n  storage:\n    schemas:\n    - version: v12\n      effectiveDate: '2022-06-01'\n    secret:\n      name: logging-loki-s3\n      type: s3\n  storageClassName: ocs-storagecluster-ceph-rbd\n  tenants:\n    mode: openshift-logging\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#deploy-red-hat-openshift-distributed-tracing","title":"Deploy Red Hat OpenShift distributed tracing","text":"<pre><code>apiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: jaeger-production\n  namespace: openshift-distributed-tracing-system\nspec:\n  strategy: production\n  storage:\n    type: elasticsearch\n    elasticsearch:\n      nodeCount: 3\n      storage:\n        storageClassName: ocs-storagecluster-ceph-rbd\n        size: 100Gi\n      resources:\n        requests:\n          memory: 4Gi\n          cpu: 1\n        limits:\n          memory: 4Gi\n          cpu: 1\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-4-configure-application-monitoring","title":"Step 4: Configure Application Monitoring","text":""},{"location":"use-cases/end-to-end-observability/#servicemonitor-for-application-metrics","title":"ServiceMonitor for Application Metrics","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myapp-metrics\n  namespace: myapp-namespace\n  labels:\n    app: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n    honorLabels: true\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-metrics\n  namespace: myapp-namespace\n  labels:\n    app: myapp\nspec:\n  ports:\n  - name: metrics\n    port: 8080\n    targetPort: 8080\n  selector:\n    app: myapp\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#prometheusrule-for-custom-alerts","title":"PrometheusRule for Custom Alerts","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: myapp-alerts\n  namespace: myapp-namespace\nspec:\n  groups:\n  - name: myapp.rules\n    rules:\n    - alert: MyAppHighErrorRate\n      expr: |\n        (\n          sum(rate(myapp_http_requests_total{status_code=~\"5..\"}[5m]))\n          /\n          sum(rate(myapp_http_requests_total[5m]))\n        ) &gt; 0.05\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High error rate detected in MyApp\"\n        description: \"Error rate is {{ $value | humanizePercentage }} for the last 5 minutes\"\n\n    - alert: MyAppHighLatency\n      expr: |\n        histogram_quantile(0.95, \n          sum(rate(myapp_http_request_duration_seconds_bucket[5m])) by (le)\n        ) &gt; 1.0\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High latency detected in MyApp\"\n        description: \"95th percentile latency is {{ $value }}s\"\n\n    - alert: MyAppPodCrashLooping\n      expr: rate(kube_pod_container_status_restarts_total{namespace=\"myapp-namespace\"}[15m]) &gt; 0\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"MyApp pod is crash looping\"\n        description: \"Pod {{ $labels.pod }} is restarting frequently\"\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-5-vm-workload-monitoring","title":"Step 5: VM Workload Monitoring","text":""},{"location":"use-cases/end-to-end-observability/#kubevirt-vm-monitoring","title":"KubeVirt VM Monitoring","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: kubevirt-vm-metrics\n  namespace: kubevirt-system\nspec:\n  selector:\n    matchLabels:\n      prometheus.kubevirt.io: \"true\"\n  endpoints:\n  - port: metrics\n    interval: 30s\n    honorLabels: true\n---\n# VM-specific PrometheusRule\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: vm-alerts\n  namespace: vm-workloads\nspec:\n  groups:\n  - name: vm.rules\n    rules:\n    - alert: VMHighCPUUsage\n      expr: kubevirt_vmi_vcpu_seconds_total &gt; 0.8\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} has high CPU usage\"\n\n    - alert: VMHighMemoryUsage\n      expr: |\n        (\n          kubevirt_vmi_memory_resident_bytes\n          /\n          kubevirt_vmi_memory_maximum_bytes\n        ) &gt; 0.9\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"VM {{ $labels.name }} has high memory usage\"\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-6-network-observability-with-cilium-hubble","title":"Step 6: Network Observability with Cilium Hubble","text":""},{"location":"use-cases/end-to-end-observability/#enable-cilium-hubble","title":"Enable Cilium Hubble","text":"<pre><code>apiVersion: cilium.io/v2alpha1\nkind: CiliumConfig\nmetadata:\n  name: cilium-config\n  namespace: cilium-system\nspec:\n  hubble:\n    enabled: true\n    metrics:\n      enabled:\n      - dns:query;ignoreAAAA\n      - drop\n      - tcp\n      - flow\n      - icmp\n      - http\n    relay:\n      enabled: true\n    ui:\n      enabled: true\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#best-practices-for-native-observability","title":"Best Practices for Native Observability","text":"<ul> <li>Consistent Labeling: Use standardized labels across all metrics (service, version, environment)</li> <li>Cardinality Management: Avoid high-cardinality labels that can overwhelm Prometheus</li> <li>Sampling Strategy: Implement trace sampling for high-traffic applications (1-10% sample rate)</li> <li>Log Levels: Use appropriate log levels and structured logging with consistent fields</li> <li>Resource Limits: Set appropriate resource limits for observability components</li> <li>Retention Policies: Configure appropriate retention for metrics (30d) and logs (7d for debug, 30d for info/error)</li> <li>Alert Fatigue: Create meaningful alerts with proper thresholds and runbooks</li> </ul>"},{"location":"use-cases/end-to-end-observability/#2-observability-with-dynatrace","title":"2. Observability with Dynatrace","text":""},{"location":"use-cases/end-to-end-observability/#infrastructure-requirements_1","title":"Infrastructure Requirements","text":"<ul> <li>Dynatrace OneAgent deployed on OpenShift nodes</li> <li>Dynatrace SaaS or Managed account</li> <li>Network access to Dynatrace monitoring endpoints</li> </ul>"},{"location":"use-cases/end-to-end-observability/#architecture-overview_1","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Application Layer\"\n        APP1[\"Microservice Application\"]\n    end\n\n    subgraph \"Dynatrace Observability\"\n        ONEAGENT[\"Dynatrace OneAgent\"]\n        DT_SAAS[\"Dynatrace SaaS\"]\n    end\n\n    APP1 -- data --&gt; ONEAGENT\n    ONEAGENT --&gt; DT_SAAS\n\n    style ONEAGENT fill:#f9f,stroke:#333\n    style DT_SAAS fill:#99f,stroke:#333</code></pre>"},{"location":"use-cases/end-to-end-observability/#implementation-steps_1","title":"Implementation Steps","text":""},{"location":"use-cases/end-to-end-observability/#step-1-deploy-dynatrace-oneagent","title":"Step 1: Deploy Dynatrace OneAgent","text":"<ul> <li>Use Dynatrace Operator for OpenShift to deploy OneAgent.</li> </ul> <pre><code>apiVersion: dynatrace.com/v1alpha1\nkind: Dynakube\nmetadata:\n  name: dynakube\n  namespace: dynatrace\nspec:\n  oneAgent:\n    classicFullStack: true\n  apiUrl: \"https://&lt;environment-id&gt;.live.dynatrace.com/api\"\n  tokens: \"api-token\"\n</code></pre>"},{"location":"use-cases/end-to-end-observability/#step-2-application-configuration","title":"Step 2: Application Configuration","text":"<ul> <li>No changes required for application code, as OneAgent will automatically instrument all services.</li> </ul>"},{"location":"use-cases/end-to-end-observability/#step-3-monitor-and-analyze","title":"Step 3: Monitor and Analyze","text":"<ul> <li>Use Dynatrace dashboards for comprehensive observability and performance analysis.</li> <li>Implement AI-driven alerts for proactive issue detection.</li> </ul>"},{"location":"use-cases/end-to-end-observability/#best-practices","title":"Best Practices","text":"<ul> <li>Ensure Network Connectivity: Verify network connectivity to Dynatrace endpoints.</li> <li>Optimize Resource Allocation: Ensure sufficient resources for OneAgent processing.</li> <li>Leverage Dynatrace AI: Utilize Dynatrace's AI capabilities for automated root cause analysis.</li> </ul> <p>This comprehensive guide provides both native and third-party observability solutions, enabling holistic insights into application performance and behavior within the RH OVE ecosystem.</p>"},{"location":"use-cases/hybrid-applications/","title":"Use Case: Hybrid Application Deployment (VMs, Containers, and PaaS)","text":""},{"location":"use-cases/hybrid-applications/#business-context","title":"Business Context","text":"<p>Organizations increasingly adopt hybrid application models involving VMs, containers, and Platform-as-a-Service (PaaS) solutions to streamline operations and leverage the cloud. This use case demonstrates how to integrate these diverse technologies within the RH OVE multi-cluster ecosystem, maximizing flexibility and efficiency.</p>"},{"location":"use-cases/hybrid-applications/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/hybrid-applications/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with OpenShift Virtualization enabled</li> <li>KubeVirt for VM support</li> <li>Cilium CNI with service mesh capabilities</li> <li>Persistent storage solution (e.g., OpenShift Data Foundation)</li> <li>Multus for multi-network support</li> </ul>"},{"location":"use-cases/hybrid-applications/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: 2+ cores per microservice or VM</li> <li>Memory: 4GB+ RAM per microservice or VM</li> <li>Storage: 20GB+ per microservice, scalable as needed</li> <li>Network: High throughput, low latency network configuration</li> </ul>"},{"location":"use-cases/hybrid-applications/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"OpenShift Cluster\"\n        VM[\"Virtual Machines\"]\n        POD[\"Containers\"]\n        PAS[\"PaaS Services\"]\n    end\n\n    subgraph \"Service Mesh\"\n        CILIUM_GATEWAY[\"Cilium Gateway\"]\n        CILIUM_PROXY[\"Cilium L7 Proxy\"]\n    end\n\n    subgraph \"Persistent Storage\"\n        PVS[\"PersistentVolume\"]\n        PVCs[\"PersistentVolumeClaim\"]\n        ODF[\"OpenShift Data Foundation\"]\n    end\n\n    VM --&gt;|run| POD\n    POD --- PAS\n    POD --&gt;|L7 Policy| CILIUM_PROXY\n    CILIUM_PROXY --&gt;|Monitor| POD\n    CILIUM_GATEWAY --&gt;|Ingress| POD\n    VM --&gt;|Storage| ODF\n    POD --&gt;|Storage| PVCs\n    PVCs --&gt; PVS\n\n    style ODF fill:#f9f,stroke:#333\n    style CILIUM_GATEWAY fill:#ff9,stroke:#333</code></pre>"},{"location":"use-cases/hybrid-applications/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/hybrid-applications/#step-1-vm-integration","title":"Step 1: VM Integration","text":""},{"location":"use-cases/hybrid-applications/#provision-virtual-machines","title":"Provision Virtual Machines","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: app-vm\n  namespace: hybrid-app\nspec:\n  running: true\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n      networks:\n      - networkName: default\n        pod: {}\n      volumes:\n      - dataVolume:\n          name: app-vm-dv\n        name: rootdisk\n---\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: app-vm-dv\n  namespace: hybrid-app\nspec:\n  source:\n    http:\n      url: \"https://vm-images.example.com/app-vm.img\"\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 50Gi\n</code></pre>"},{"location":"use-cases/hybrid-applications/#step-2-container-integration","title":"Step 2: Container Integration","text":""},{"location":"use-cases/hybrid-applications/#deploy-containerized-services","title":"Deploy Containerized Services","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-container\n  namespace: hybrid-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: app-container\n  template:\n    metadata:\n      labels:\n        app: app-container\n    spec:\n      containers:\n      - name: app\n        image: quay.io/example/app-image:latest\n        ports:\n        - containerPort: 8080\n      volumes:\n      - name: app-storage\n        persistentVolumeClaim:\n          claimName: app-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: app-pvc\n  namespace: hybrid-app\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre>"},{"location":"use-cases/hybrid-applications/#step-3-service-mesh-enablement","title":"Step 3: Service Mesh Enablement","text":""},{"location":"use-cases/hybrid-applications/#configure-cilium-gateway-api-and-l7-policies","title":"Configure Cilium Gateway API and L7 Policies","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: Gateway\nmetadata:\n  name: app-gateway\n  namespace: hybrid-app\nspec:\n  gatewayClassName: cilium\n  listeners:\n  - name: http\n    port: 80\n    protocol: HTTP\n    hostname: \"app.example.com\"\n---\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: HTTPRoute\nmetadata:\n  name: app-routing\n  namespace: hybrid-app\nspec:\n  parentRefs:\n  - name: app-gateway\n  hostnames:\n  - \"app.example.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: \"/\"\n    backendRefs:\n    - name: app-container\n      port: 8080\n---\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: app-l7-policy\n  namespace: hybrid-app\nspec:\n  endpointSelector:\n    matchLabels:\n      app: app-container\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        \"k8s:io.cilium.k8s.policy.cluster\": \"default\"\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n        - method: \"POST\"\n        - method: \"PUT\"\n        - method: \"DELETE\"\n\n### Step 4: PaaS Integration\n\n#### Deploy Middleware and Database Services\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: middleware\n  namespace: hybrid-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: middleware\n  template:\n    metadata:\n      labels:\n        app: middleware\n    spec:\n      containers:\n      - name: middleware\n        image: quay.io/example/middleware-image:latest\n        env:\n        - name: DB_HOST\n          value: db.example.com\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secrets\n              key: username\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: middleware-service\n  namespace: hybrid-app\nspec:\n  selector:\n    app: middleware\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  type: ClusterIP\n</code></pre>"},{"location":"use-cases/hybrid-applications/#step-5-continuous-integrationcontinuous-deployment-cicd","title":"Step 5: Continuous Integration/Continuous Deployment (CI/CD)","text":""},{"location":"use-cases/hybrid-applications/#integrate-with-argocd","title":"Integrate with ArgoCD","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: hybrid-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://git.example.com/hybrid-app-config\n    targetRevision: HEAD\n    path: hybrid\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: hybrid-app\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n</code></pre>"},{"location":"use-cases/hybrid-applications/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/hybrid-applications/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/hybrid-applications/#vm-performance-degradation","title":"VM Performance Degradation","text":"<ul> <li>Check resource allocation and adjust CPU/RAM as needed.</li> <li>Ensure the underlying storage is not a bottleneck.</li> </ul>"},{"location":"use-cases/hybrid-applications/#service-mesh-connectivity-issues","title":"Service Mesh Connectivity Issues","text":"<ul> <li>Validate Cilium Gateway API configurations and ensure L7 policies are applied.</li> <li>Check Cilium network policies and ensure proper endpoint selection.</li> </ul>"},{"location":"use-cases/hybrid-applications/#storage-access-issues","title":"Storage Access Issues","text":"<ul> <li>Verify PVC binding and ensure correct storage class is applied.</li> <li>Check application logs for connectivity and permission errors.</li> </ul>"},{"location":"use-cases/hybrid-applications/#best-practices","title":"Best Practices","text":"<ul> <li>Resource Management: Dynamic scaling policies for both VMs and containers.</li> <li>Network Policies: Zero-trust approach with Cilium network policies for clear segmentation.</li> <li>Security Hardening: Apply security best practices for VM and container images with Cilium L7 policies.</li> <li>Monitoring and Logging: Unified logging solutions using Fluentd or Logstash with direct integration to Prometheus and Cilium Hubble for network observability.</li> </ul>"},{"location":"use-cases/hybrid-applications/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/hybrid-applications/#monitoring-integration","title":"Monitoring Integration","text":"<pre><code># Monitoring and alerting configuration\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: hybrid-app-monitor\n  namespace: hybrid-app\nspec:\n  selector:\n    matchLabels:\n      app: app-container\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n</code></pre> <p>This comprehensive guide provides everything needed to deploy and manage hybrid applications within the RH OVE multi-cluster ecosystem while adhering to enterprise-grade security, monitoring, and operational standards.</p>"},{"location":"use-cases/legacy-modernization/","title":"Use Case: Legacy Application Modernization","text":""},{"location":"use-cases/legacy-modernization/#business-context","title":"Business Context","text":"<p>Legacy application modernization is critical for organizations looking to leverage modern cloud-native technologies while preserving existing business logic and data. This use case demonstrates a phased approach to modernizing legacy applications using the RH OVE ecosystem, enabling gradual transformation with minimal business disruption.</p>"},{"location":"use-cases/legacy-modernization/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/legacy-modernization/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>Application migration tools (Migration Toolkit for Applications - MTA)</li> <li>Service mesh for hybrid connectivity (OpenShift Service Mesh/Istio)</li> <li>CI/CD pipelines (OpenShift Pipelines/Tekton)</li> <li>Container registry (Quay.io or OpenShift integrated registry)</li> </ul>"},{"location":"use-cases/legacy-modernization/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Compute: Sufficient resources for both legacy and modernized components during transition</li> <li>Storage: Persistent storage for data migration and synchronization</li> <li>Network: High-bandwidth connectivity for data replication and service communication</li> </ul>"},{"location":"use-cases/legacy-modernization/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Legacy Environment\"\n        LEGACY_APP[\"Legacy Application\"]\n        LEGACY_DB[\"Legacy Database\"]\n        LEGACY_FILES[\"File System\"]\n    end\n\n    subgraph \"Modernization Platform\"\n        VM_LAYER[\"VM Layer (Lift &amp; Shift)\"]\n        CONTAINER_LAYER[\"Container Layer\"]\n        MICROSERVICES[\"Microservices\"]\n        MODERN_DB[\"Modern Database\"]\n    end\n\n    subgraph \"Integration Layer\"\n        SERVICE_MESH[\"Service Mesh\"]\n        API_GATEWAY[\"API Gateway\"]\n        MESSAGE_QUEUE[\"Message Queue\"]\n    end\n\n    LEGACY_APP --&gt; VM_LAYER\n    VM_LAYER --&gt; CONTAINER_LAYER\n    CONTAINER_LAYER --&gt; MICROSERVICES\n    LEGACY_DB --&gt; MODERN_DB\n\n    SERVICE_MESH --&gt; VM_LAYER\n    SERVICE_MESH --&gt; CONTAINER_LAYER\n    SERVICE_MESH --&gt; MICROSERVICES\n\n    API_GATEWAY --&gt; SERVICE_MESH\n    MESSAGE_QUEUE --&gt; SERVICE_MESH\n\n    style LEGACY_APP fill:#faa,stroke:#333\n    style MICROSERVICES fill:#afa,stroke:#333\n    style SERVICE_MESH fill:#aaf,stroke:#333</code></pre>"},{"location":"use-cases/legacy-modernization/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/legacy-modernization/#phase-1-assessment-and-planning","title":"Phase 1: Assessment and Planning","text":""},{"location":"use-cases/legacy-modernization/#application-discovery","title":"Application Discovery","text":"<pre><code># MTA Configuration for Application Analysis\napiVersion: tackle.konveyor.io/v1alpha1\nkind: Application\nmetadata:\n  name: legacy-app-analysis\n  namespace: konveyor-tackle\nspec:\n  name: \"Legacy ERP System\"\n  description: \"Monolithic ERP application requiring modernization\"\n  repository:\n    kind: git\n    url: \"https://git.example.com/legacy-erp\"\n  binary: \"erp-application.war\"\n</code></pre>"},{"location":"use-cases/legacy-modernization/#migration-assessment","title":"Migration Assessment","text":"<pre><code># Run application analysis using MTA CLI\nkonveyor-cli analyze \\\n  --input /path/to/legacy-app \\\n  --output /path/to/analysis-results \\\n  --target cloud-readiness \\\n  --target containers\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-2-lift-and-shift-vm-migration","title":"Phase 2: Lift and Shift (VM Migration)","text":""},{"location":"use-cases/legacy-modernization/#vm-based-legacy-application-deployment","title":"VM-based Legacy Application Deployment","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: legacy-erp-vm\n  namespace: modernization\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        app: legacy-erp\n        tier: application\n        phase: lift-shift\n    spec:\n      domain:\n        cpu:\n          cores: 4\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n          - disk:\n              bus: virtio\n            name: datadisk\n          interfaces:\n          - name: default\n            bridge: {}\n        memory:\n          guest: 8Gi\n        resources:\n          requests:\n            memory: 8Gi\n            cpu: 4\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - dataVolume:\n          name: legacy-erp-root\n        name: rootdisk\n      - dataVolume:\n          name: legacy-erp-data\n        name: datadisk\n---\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: legacy-erp-root\n  namespace: modernization\nspec:\n  source:\n    http:\n      url: \"https://vm-images.example.com/legacy-erp-root.img\"\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 100Gi\n---\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: legacy-erp-data\n  namespace: modernization\nspec:\n  source:\n    http:\n      url: \"https://vm-images.example.com/legacy-erp-data.img\"\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 500Gi\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-3-containerization","title":"Phase 3: Containerization","text":""},{"location":"use-cases/legacy-modernization/#legacy-application-container-deployment","title":"Legacy Application Container Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: legacy-erp-container\n  namespace: modernization\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: legacy-erp\n      tier: application\n      phase: containerized\n  template:\n    metadata:\n      labels:\n        app: legacy-erp\n        tier: application\n        phase: containerized\n    spec:\n      containers:\n      - name: erp-app\n        image: quay.io/example/legacy-erp:containerized\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 8443\n          name: https\n        env:\n        - name: DB_HOST\n          value: \"legacy-database-service\"\n        - name: DB_PORT\n          value: \"5432\"\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: database\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: username\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: password\n        volumeMounts:\n        - name: app-data\n          mountPath: /opt/erp/data\n        - name: app-config\n          mountPath: /opt/erp/config\n      volumes:\n      - name: app-data\n        persistentVolumeClaim:\n          claimName: erp-data-pvc\n      - name: app-config\n        configMap:\n          name: erp-config\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-4-service-decomposition","title":"Phase 4: Service Decomposition","text":""},{"location":"use-cases/legacy-modernization/#extract-user-management-service","title":"Extract User Management Service","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-management-service\n  namespace: modernization\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-management\n      tier: microservice\n  template:\n    metadata:\n      labels:\n        app: user-management\n        tier: microservice\n        version: v1\n    spec:\n      containers:\n      - name: user-management\n        image: quay.io/example/user-management:v1.0.0\n        ports:\n        - containerPort: 8080\n          name: http\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: user-db-credentials\n              key: url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis-credentials\n              key: url\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-management-service\n  namespace: modernization\nspec:\n  selector:\n    app: user-management\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n  type: ClusterIP\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-5-service-mesh-integration","title":"Phase 5: Service Mesh Integration","text":""},{"location":"use-cases/legacy-modernization/#istio-service-mesh-configuration","title":"Istio Service Mesh Configuration","text":"<pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: legacy-erp-routing\n  namespace: modernization\nspec:\n  hosts:\n  - erp.example.com\n  http:\n  - match:\n    - uri:\n        prefix: /api/users\n    route:\n    - destination:\n        host: user-management-service\n        port:\n          number: 80\n      weight: 100\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: legacy-erp-container\n        port:\n          number: 8080\n      weight: 90\n    - destination:\n        host: legacy-erp-vm\n        port:\n          number: 8080\n      weight: 10\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: legacy-erp-destination\n  namespace: modernization\nspec:\n  host: legacy-erp-container\n  trafficPolicy:\n    circuitBreaker:\n      consecutive5xxErrors: 3\n      interval: 30s\n      baseEjectionTime: 30s\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-6-data-migration-strategy","title":"Phase 6: Data Migration Strategy","text":""},{"location":"use-cases/legacy-modernization/#database-migration-pipeline","title":"Database Migration Pipeline","text":"<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: data-migration-pipeline\n  namespace: modernization\nspec:\n  params:\n  - name: source-db-url\n    description: Source database connection URL\n  - name: target-db-url\n    description: Target database connection URL\n  - name: migration-batch-size\n    default: \"1000\"\n    description: Number of records to migrate per batch\n\n  tasks:\n  - name: validate-source\n    taskRef:\n      name: database-validation\n    params:\n    - name: db-url\n      value: $(params.source-db-url)\n\n  - name: create-target-schema\n    taskRef:\n      name: schema-creation\n    params:\n    - name: db-url\n      value: $(params.target-db-url)\n    runAfter:\n    - validate-source\n\n  - name: migrate-data\n    taskRef:\n      name: data-migration\n    params:\n    - name: source-db-url\n      value: $(params.source-db-url)\n    - name: target-db-url\n      value: $(params.target-db-url)\n    - name: batch-size\n      value: $(params.migration-batch-size)\n    runAfter:\n    - create-target-schema\n\n  - name: validate-migration\n    taskRef:\n      name: migration-validation\n    params:\n    - name: source-db-url\n      value: $(params.source-db-url)\n    - name: target-db-url\n      value: $(params.target-db-url)\n    runAfter:\n    - migrate-data\n</code></pre>"},{"location":"use-cases/legacy-modernization/#phase-7-progressive-traffic-migration","title":"Phase 7: Progressive Traffic Migration","text":""},{"location":"use-cases/legacy-modernization/#canary-deployment-strategy","title":"Canary Deployment Strategy","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: legacy-erp-rollout\n  namespace: modernization\nspec:\n  replicas: 5\n  strategy:\n    canary:\n      steps:\n      - setWeight: 10\n      - pause: {duration: 300s}\n      - setWeight: 25\n      - pause: {duration: 300s}\n      - setWeight: 50\n      - pause: {duration: 300s}\n      - setWeight: 75\n      - pause: {duration: 300s}\n      canaryService: legacy-erp-canary\n      stableService: legacy-erp-stable\n      trafficRouting:\n        istio:\n          virtualService:\n            name: legacy-erp-routing\n            routes:\n            - primary\n  selector:\n    matchLabels:\n      app: legacy-erp\n  template:\n    metadata:\n      labels:\n        app: legacy-erp\n    spec:\n      containers:\n      - name: erp-app\n        image: quay.io/example/legacy-erp:modernized\n        ports:\n        - containerPort: 8080\n</code></pre>"},{"location":"use-cases/legacy-modernization/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"use-cases/legacy-modernization/#application-performance-monitoring","title":"Application Performance Monitoring","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: modernization-monitoring\n  namespace: modernization\nspec:\n  selector:\n    matchLabels:\n      monitoring: enabled\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n---\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: modernization-alerts\n  namespace: modernization\nspec:\n  groups:\n  - name: modernization.alerts\n    rules:\n    - alert: LegacyAppHighErrorRate\n      expr: rate(http_requests_total{status=~\"5..\"}[5m]) &gt; 0.1\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High error rate in legacy application\"\n\n    - alert: MigrationDataInconsistency\n      expr: migration_data_consistency_check != 1\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Data inconsistency detected during migration\"\n</code></pre>"},{"location":"use-cases/legacy-modernization/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/legacy-modernization/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/legacy-modernization/#performance-degradation-during-migration","title":"Performance Degradation During Migration","text":"<ul> <li>Issue: Application performance degrades during data migration</li> <li>Solution: </li> <li>Implement read replicas for database queries</li> <li>Use incremental migration strategies</li> <li>Schedule heavy migration tasks during off-peak hours</li> </ul>"},{"location":"use-cases/legacy-modernization/#service-communication-failures","title":"Service Communication Failures","text":"<ul> <li>Issue: Communication failures between legacy and modern components</li> <li>Solution:</li> <li>Verify service mesh configuration</li> <li>Check network policies and firewall rules</li> <li>Implement circuit breakers and retry mechanisms</li> </ul>"},{"location":"use-cases/legacy-modernization/#data-consistency-issues","title":"Data Consistency Issues","text":"<ul> <li>Issue: Data inconsistencies between old and new systems</li> <li>Solution:</li> <li>Implement two-phase commit protocols</li> <li>Use event sourcing for critical operations</li> <li>Regular data validation and reconciliation</li> </ul>"},{"location":"use-cases/legacy-modernization/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/legacy-modernization/#migration-strategy","title":"Migration Strategy","text":"<ul> <li>Incremental Approach: Migrate functionality incrementally to reduce risk</li> <li>Feature Toggles: Use feature flags to control rollout of new functionality</li> <li>Rollback Plans: Always have rollback procedures for each migration phase</li> </ul>"},{"location":"use-cases/legacy-modernization/#testing-and-validation","title":"Testing and Validation","text":"<ul> <li>Automated Testing: Implement comprehensive test suites for both legacy and modern components</li> <li>Performance Testing: Conduct load testing throughout the migration process</li> <li>Data Validation: Implement automated data consistency checks</li> </ul>"},{"location":"use-cases/legacy-modernization/#security-considerations","title":"Security Considerations","text":"<ul> <li>Zero-Trust Architecture: Implement zero-trust principles across all components</li> <li>Secrets Management: Use proper secrets management for database credentials and API keys</li> <li>Network Segmentation: Implement proper network segmentation between components</li> </ul>"},{"location":"use-cases/legacy-modernization/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/legacy-modernization/#gitops-integration","title":"GitOps Integration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: legacy-modernization\n  namespace: argocd\nspec:\n  project: modernization\n  source:\n    repoURL: https://git.example.com/legacy-modernization-config\n    targetRevision: HEAD\n    path: environments/production\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: modernization\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"use-cases/legacy-modernization/#multi-cluster-deployment","title":"Multi-Cluster Deployment","text":"<ul> <li>Staging Environment: Use separate clusters for testing and validation</li> <li>Production Rollout: Implement blue-green deployment across clusters</li> <li>Disaster Recovery: Maintain legacy systems as backup during transition</li> </ul> <p>This comprehensive guide provides a structured approach to legacy application modernization within the RH OVE ecosystem, enabling organizations to gradually transform their applications while maintaining business continuity and reducing risk.</p>"},{"location":"use-cases/overview/","title":"RH OVE Use Cases Overview","text":""},{"location":"use-cases/overview/#introduction","title":"Introduction","text":"<p>This section provides comprehensive use cases for the Red Hat OpenShift Virtualization Engine (RH OVE) multi-cluster ecosystem, demonstrating real-world scenarios and implementation patterns.</p>"},{"location":"use-cases/overview/#complete-use-cases-summary-table","title":"\ud83d\udcca Complete Use Cases Summary Table","text":"<p>For a comprehensive overview of all use cases with complexity ratings, implementation timelines, and prerequisites, see our Use Cases Summary Table.</p>"},{"location":"use-cases/overview/#use-case-categories","title":"Use Case Categories","text":""},{"location":"use-cases/overview/#1-vm-lifecycle-management","title":"1. VM Lifecycle Management","text":"<ul> <li>VM Import and Migration: Importing existing VMs from various sources</li> <li>VM Template Management: Creating and managing VM templates</li> <li>VM Scaling and Performance: Dynamic resource allocation and optimization</li> <li>VM Backup and Recovery: Data protection strategies for virtual machines</li> </ul>"},{"location":"use-cases/overview/#2-hybrid-application-deployment","title":"2. Hybrid Application Deployment","text":"<ul> <li>VM + Container Integration: Running legacy VMs alongside containerized services</li> <li>Multi-Tier Applications: Web, application, and database tiers across different deployment models</li> <li>Service Mesh Integration: Connecting VMs and containers through service mesh</li> <li>Data Sharing: Persistent storage sharing between VMs and containers</li> </ul>"},{"location":"use-cases/overview/#3-platform-as-a-service-paas-integration","title":"3. Platform-as-a-Service (PaaS) Integration","text":"<ul> <li>Database Services: Running databases as VMs with containerized management</li> <li>Middleware Platforms: Message queues, application servers, and integration platforms</li> <li>Development Environments: Self-service development platforms</li> <li>CI/CD Pipeline Integration: Automated deployment across VM and container workloads</li> </ul>"},{"location":"use-cases/overview/#4-enterprise-integration-scenarios","title":"4. Enterprise Integration Scenarios","text":"<ul> <li>Legacy Application Modernization: Gradual migration strategies</li> <li>Disaster Recovery: Cross-cluster failover and recovery procedures</li> <li>Multi-Cloud Deployment: Hybrid cloud scenarios with on-premises integration</li> <li>Compliance and Security: Meeting enterprise security and regulatory requirements</li> </ul>"},{"location":"use-cases/overview/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"use-cases/overview/#hub-and-spoke-pattern","title":"Hub and Spoke Pattern","text":"<ul> <li>Management cluster orchestrates multiple application clusters</li> <li>Centralized policy and governance with distributed execution</li> <li>GitOps-driven configuration management</li> </ul>"},{"location":"use-cases/overview/#network-integration","title":"Network Integration","text":"<ul> <li>Cilium CNI with Multus multi-network support</li> <li>VLAN integration for legacy systems</li> <li>Service mesh connectivity between VMs and containers</li> </ul>"},{"location":"use-cases/overview/#identity-and-access-management","title":"Identity and Access Management","text":"<ul> <li>OIDC-based authentication with Keycloak</li> <li>RBAC integration across VM and container environments</li> <li>Multi-factor authentication for administrative access</li> </ul>"},{"location":"use-cases/overview/#use-case-implementation-guide","title":"Use Case Implementation Guide","text":""},{"location":"use-cases/overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenShift 4.12+ clusters with KubeVirt enabled</li> <li>Sufficient compute, memory, and storage resources</li> <li>Network connectivity between clusters</li> <li>Identity provider integration (Keycloak/LDAP/AD)</li> </ul>"},{"location":"use-cases/overview/#common-patterns","title":"Common Patterns","text":"<ol> <li>Resource Provisioning: Using CDI for VM disk management</li> <li>Network Configuration: Multi-network setup with Multus</li> <li>Storage Management: Persistent volume strategies for VMs</li> <li>Monitoring Integration: Unified monitoring for VMs and containers</li> <li>Backup Strategies: Integrated backup solutions for hybrid workloads</li> </ol>"},{"location":"use-cases/overview/#getting-started","title":"Getting Started","text":"<ol> <li>Review the specific use case documentation</li> <li>Understand the architectural requirements</li> <li>Prepare the environment according to prerequisites</li> <li>Follow the step-by-step implementation guide</li> <li>Validate the deployment and test functionality</li> <li>Monitor and maintain the solution</li> </ol> <p>Each use case provides: - Business Context: Why this pattern is needed - Technical Requirements: Infrastructure and software requirements - Implementation Steps: Detailed configuration procedures - Validation Procedures: Testing and verification steps - Troubleshooting Guide: Common issues and solutions - Best Practices: Recommendations for production deployment</p>"},{"location":"use-cases/publishing-events-to-cmdb-siem/","title":"Use Case: Publishing Kubernetes Events to CMDB and SIEM Solutions","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#business-context","title":"Business Context","text":"<p>Enterprise IT environments require robust integration between Kubernetes events and existing IT service management and security incident management platforms. Integrating Kubernetes events with CMDB (such as ServiceNow or GLPI) and SIEM solutions enhances visibility, compliance, and proactive issue resolution. This use case focuses on using an event-bus to publish infrastructure changes, security issues, and admission control issues from Kubernetes to these platforms.</p>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ cluster with event capture capabilities</li> <li>Event-bus solution such as NATS, Kafka, or RabbitMQ</li> <li>Connectivity to CMDB (e.g., ServiceNow, GLPI) and SIEM solutions</li> <li>Persistent storage for event retention and replay</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: Adequate resources for handling and processing events</li> <li>Memory: Sufficient memory for event processing and transformation</li> <li>Storage: Persistent storage for event logs and audit trails</li> <li>Network: Reliable connectivity to external platforms</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#event-categories-and-sources","title":"Event Categories and Sources","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#infrastructure-change-events","title":"Infrastructure Change Events","text":"<ul> <li>Node Events: Node ready/not ready, resource pressure, kubelet issues</li> <li>Pod Events: Pod scheduling, image pulling, container creation/termination</li> <li>Storage Events: Volume mounting/unmounting, PVC binding issues</li> <li>Network Events: Service endpoint changes, ingress controller updates</li> <li>Resource Events: Deployment scaling, ConfigMap/Secret updates</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#security-events","title":"Security Events","text":"<ul> <li>RBAC Violations: Unauthorized access attempts, permission denials</li> <li>Pod Security Policy Violations: SecurityContext violations, privileged access attempts</li> <li>Image Security Events: Image scanning failures, vulnerable image deployments</li> <li>Network Policy Violations: Blocked network connections, policy enforcement</li> <li>Certificate Events: TLS certificate expiration, rotation failures</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#admission-control-events","title":"Admission Control Events","text":"<ul> <li>ValidatingAdmissionWebhook: Policy violations, validation failures</li> <li>MutatingAdmissionWebhook: Resource mutations, injection failures</li> <li>Resource Quota Violations: Quota exceeded, resource limit breaches</li> <li>LimitRange Violations: Container resource limit violations</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"RH OVE Cluster\"\n        K8S_API[\"Kubernetes API Server\"]\n        FALCO[\"Falco Security Runtime\"]\n        OPA_GATEKEEPER[\"OPA Gatekeeper\"]\n        KUBE_EVENTS[\"Kubernetes Events\"]\n        AUDIT_LOGS[\"Audit Logs\"]\n    end\n\n    subgraph \"Event Processing Layer\"\n        KUBEWATCH[\"Kubewatch\"]\n        BOTKUBE[\"Botkube\"]\n        EVENT_EXPORTER[\"Event Exporter\"]\n        FLUENT_BIT[\"Fluent Bit\"]\n        CUSTOM_CONTROLLER[\"Custom Event Controller\"]\n    end\n\n    subgraph \"Event Bus\"\n        NATS[\"NATS JetStream\"]\n        KAFKA[\"Apache Kafka\"]\n        RED_HAT_AMQ[\"Red Hat AMQ Streams\"]\n    end\n\n    subgraph \"Integration Adapters\"\n        SERVICENOW_ADAPTER[\"ServiceNow Adapter\"]\n        GLPI_ADAPTER[\"GLPI Adapter\"]\n        SIEM_ADAPTER[\"SIEM Adapter\"]\n        WEBHOOK_ADAPTER[\"Webhook Adapter\"]\n    end\n\n    subgraph \"Target Systems\"\n        SERVICENOW[\"ServiceNow CMDB\"]\n        GLPI[\"GLPI ITSM\"]\n        SPLUNK[\"Splunk SIEM\"]\n        ELASTIC_SIEM[\"Elastic Security\"]\n        QRadar[\"IBM QRadar\"]\n    end\n\n    K8S_API --&gt; KUBE_EVENTS\n    FALCO --&gt; EVENT_EXPORTER\n    OPA_GATEKEEPER --&gt; KUBEWATCH\n    AUDIT_LOGS --&gt; FLUENT_BIT\n\n    KUBE_EVENTS --&gt; KUBEWATCH\n    KUBE_EVENTS --&gt; BOTKUBE\n    KUBE_EVENTS --&gt; EVENT_EXPORTER\n    KUBE_EVENTS --&gt; CUSTOM_CONTROLLER\n\n    KUBEWATCH --&gt; NATS\n    BOTKUBE --&gt; KAFKA\n    EVENT_EXPORTER --&gt; RED_HAT_AMQ\n    FLUENT_BIT --&gt; NATS\n    CUSTOM_CONTROLLER --&gt; KAFKA\n\n    NATS --&gt; SERVICENOW_ADAPTER\n    KAFKA --&gt; GLPI_ADAPTER\n    RED_HAT_AMQ --&gt; SIEM_ADAPTER\n    NATS --&gt; WEBHOOK_ADAPTER\n\n    SERVICENOW_ADAPTER --&gt; SERVICENOW\n    GLPI_ADAPTER --&gt; GLPI\n    SIEM_ADAPTER --&gt; SPLUNK\n    SIEM_ADAPTER --&gt; ELASTIC_SIEM\n    SIEM_ADAPTER --&gt; QRadar\n    WEBHOOK_ADAPTER --&gt; SERVICENOW\n    WEBHOOK_ADAPTER --&gt; GLPI\n\n    style K8S_API fill:#f9f,stroke:#333\n    style NATS fill:#ffa,stroke:#333\n    style KAFKA fill:#aaf,stroke:#333\n    style RED_HAT_AMQ fill:#afa,stroke:#333</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#step-1-deploy-event-bus-solution","title":"Step 1: Deploy Event Bus Solution","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#deploy-nats-streaming","title":"Deploy NATS Streaming","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nats-streaming\n  namespace: event-bus\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nats-streaming\n  template:\n    metadata:\n      labels:\n        app: nats-streaming\n    spec:\n      containers:\n      - name: nats\n        image: nats-streaming:latest\n        ports:\n        - containerPort: 4222\n        - containerPort: 8222\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 0.5\n          requests:\n            memory: 256Mi\n            cpu: 0.25\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#step-2-configure-event-processing-solutions","title":"Step 2: Configure Event Processing Solutions","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#option-1-deploy-kubewatch-for-event-monitoring","title":"Option 1: Deploy Kubewatch for Event Monitoring","text":"<p>Kubewatch is a Kubernetes watcher that publishes notifications to various channels.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubewatch\n  namespace: event-processing\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubewatch\n  template:\n    metadata:\n      labels:\n        app: kubewatch\n    spec:\n      serviceAccountName: kubewatch\n      containers:\n      - name: kubewatch\n        image: bitnami/kubewatch:latest\n        env:\n        - name: KW_CONFIG\n          value: /config/kubewatch-config.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 100m\n          requests:\n            memory: 128Mi\n            cpu: 50m\n      volumes:\n      - name: config\n        configMap:\n          name: kubewatch-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kubewatch-config\n  namespace: event-processing\ndata:\n  kubewatch-config.yaml: |\n    namespace: \"\"\n    handler:\n      webhook:\n        url: \"http://event-router.event-processing.svc.cluster.local:8080/webhook\"\n    resource:\n      deployment: true\n      replicationcontroller: false\n      replicaset: false\n      daemonset: true\n      services: true\n      pod: true\n      job: true\n      node: true\n      clusterrole: false\n      serviceaccount: false\n      persistentvolume: true\n      namespace: true\n      secret: false\n      configmap: false\n      ingress: true\n      event: true\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kubewatch\n  namespace: event-processing\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kubewatch\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"nodes\", \"namespaces\", \"events\", \"services\", \"persistentvolumes\"]\n  verbs: [\"list\", \"watch\", \"get\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"daemonsets\", \"replicasets\"]\n  verbs: [\"list\", \"watch\", \"get\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"list\", \"watch\", \"get\"]\n- apiGroups: [\"batch\"]\n  resources: [\"jobs\"]\n  verbs: [\"list\", \"watch\", \"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubewatch\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kubewatch\nsubjects:\n- kind: ServiceAccount\n  name: kubewatch\n  namespace: event-processing\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#option-2-deploy-botkube-for-advanced-event-processing","title":"Option 2: Deploy Botkube for Advanced Event Processing","text":"<p>Botkube provides intelligent event filtering and routing capabilities.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: botkube-config\n  namespace: event-processing\ndata:\n  config.yaml: |\n    settings:\n      clustername: \"rh-ove-cluster\"\n      kubectl:\n        enabled: false\n      configwatcher: true\n      upgradenotifier: true\n\n    communications:\n      webhook:\n        enabled: true\n        url: \"http://event-router.event-processing.svc.cluster.local:8080/botkube\"\n\n    resources:\n    - name: \"v1/pods\"\n      namespaces:\n        include:\n        - \"default\"\n        - \"kube-system\"\n        - \"openshift-*\"\n      events:\n      - create\n      - delete\n      - error\n      updateSetting:\n        includeDiff: true\n        fields:\n        - \"spec.containers[*].image\"\n        - \"status.phase\"\n\n    - name: \"v1/services\"\n      namespaces:\n        include:\n        - \"default\"\n      events:\n      - create\n      - delete\n      - error\n\n    - name: \"v1/nodes\"\n      events:\n      - create\n      - delete\n      - error\n\n    - name: \"v1/events\"\n      events:\n      - error\n      - warning\n      filters:\n        objectAnnotations:\n          pattern: \".*\"\n        reason:\n        - \"FailedScheduling\"\n        - \"Unhealthy\"\n        - \"FailedMount\"\n        - \"NetworkNotReady\"\n        - \"NodeNotReady\"\n        - \"Rebooted\"\n        - \"SystemOOM\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: botkube\n  namespace: event-processing\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: botkube\n  template:\n    metadata:\n      labels:\n        app: botkube\n    spec:\n      serviceAccountName: botkube\n      containers:\n      - name: botkube\n        image: infracloudio/botkube:latest\n        env:\n        - name: CONFIG_PATH\n          value: \"/tmp/config/\"\n        - name: LOG_LEVEL\n          value: \"info\"\n        volumeMounts:\n        - name: config\n          mountPath: \"/tmp/config\"\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 100m\n          requests:\n            memory: 128Mi\n            cpu: 50m\n      volumes:\n      - name: config\n        configMap:\n          name: botkube-config\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: botkube\n  namespace: event-processing\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: botkube\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: botkube\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: botkube\nsubjects:\n- kind: ServiceAccount\n  name: botkube\n  namespace: event-processing\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#option-3-deploy-kubernetes-event-exporter","title":"Option 3: Deploy Kubernetes Event Exporter","text":"<p>For more granular control over event processing and routing.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-exporter\n  namespace: event-processing\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: event-exporter\n  template:\n    metadata:\n      labels:\n        app: event-exporter\n    spec:\n      serviceAccountName: event-exporter\n      containers:\n      - name: event-exporter\n        image: giantswarm/event-exporter:latest\n        args:\n        - --config-file=/config/config.yaml\n        - --logtostderr\n        - --v=2\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 100m\n          requests:\n            memory: 128Mi\n            cpu: 50m\n      volumes:\n      - name: config\n        configMap:\n          name: event-exporter-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: event-exporter-config\n  namespace: event-processing\ndata:\n  config.yaml: |\n    logLevel: 2\n    logFormat: json\n    receivers:\n    - name: \"webhook-receiver\"\n      webhook:\n        endpoint: \"http://event-router.event-processing.svc.cluster.local:8080/events\"\n        headers:\n          X-Source: \"kubernetes-events\"\n          Content-Type: \"application/json\"\n    - name: \"kafka-receiver\"\n      kafka:\n        brokers:\n        - \"kafka.event-bus.svc.cluster.local:9092\"\n        topic: \"kubernetes-events\"\n\n    route:\n      routes:\n      # Infrastructure change events\n      - match:\n        - receiver: \"webhook-receiver\"\n          name: \"infrastructure-changes\"\n        - receiver: \"kafka-receiver\"\n          name: \"infrastructure-changes\"\n        groupBy: [\"namespace\", \"reason\"]\n        groupWait: 10s\n        groupInterval: 30s\n        repeatInterval: 1h\n        routes:\n        - match:\n          - field: \"reason\"\n            op: \"in\"\n            values: [\"Scheduled\", \"Pulled\", \"Created\", \"Started\"]\n          name: \"pod-lifecycle\"\n        - match:\n          - field: \"reason\"\n            op: \"in\"\n            values: [\"NodeReady\", \"NodeNotReady\", \"Rebooted\"]\n          name: \"node-events\"\n        - match:\n          - field: \"reason\"\n            op: \"in\"\n            values: [\"SuccessfulMount\", \"FailedMount\"]\n          name: \"storage-events\"\n\n      # Security events\n      - match:\n        - receiver: \"webhook-receiver\"\n          name: \"security-events\"\n        groupBy: [\"namespace\", \"reason\", \"involvedObject.kind\"]\n        routes:\n        - match:\n          - field: \"reason\"\n            op: \"in\"\n            values: [\"Forbidden\", \"Unauthorized\", \"PolicyViolation\"]\n          name: \"security-violations\"\n        - match:\n          - field: \"message\"\n            op: \"re\"\n            values: [\".*security.*\", \".*violation.*\", \".*denied.*\"]\n          name: \"security-related\"\n\n      # Admission control events\n      - match:\n        - receiver: \"kafka-receiver\"\n          name: \"admission-control\"\n        routes:\n        - match:\n          - field: \"reason\"\n            op: \"in\"\n            values: [\"FailedCreate\", \"AdmissionWebhookDenied\"]\n          name: \"admission-denied\"\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: event-exporter\n  namespace: event-processing\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: event-exporter\nrules:\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: event-exporter\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: event-exporter\nsubjects:\n- kind: ServiceAccount\n  name: event-exporter\n  namespace: event-processing\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#option-4-custom-event-controller-with-event-router","title":"Option 4: Custom Event Controller with Event Router","text":"<p>For maximum flexibility, deploy a custom event router that can transform and route events.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-router\n  namespace: event-processing\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: event-router\n  template:\n    metadata:\n      labels:\n        app: event-router\n    spec:\n      containers:\n      - name: event-router\n        image: quay.io/example/event-router:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NATS_URL\n          value: \"nats://nats.event-bus.svc.cluster.local:4222\"\n        - name: KAFKA_BROKERS\n          value: \"kafka.event-bus.svc.cluster.local:9092\"\n        - name: LOG_LEVEL\n          value: \"info\"\n        resources:\n          limits:\n            memory: 512Mi\n            cpu: 200m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: event-router\n  namespace: event-processing\nspec:\n  selector:\n    app: event-router\n  ports:\n  - name: http\n    port: 8080\n    targetPort: 8080\n  type: ClusterIP\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#step-3-deploy-event-bus-solutions","title":"Step 3: Deploy Event Bus Solutions","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#deploy-red-hat-amq-streams-kafka","title":"Deploy Red Hat AMQ Streams (Kafka)","text":"<pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: event-cluster\n  namespace: event-bus\nspec:\n  kafka:\n    version: 3.4.0\n    replicas: 3\n    listeners:\n    - name: plain\n      port: 9092\n      type: internal\n      tls: false\n    - name: tls\n      port: 9093\n      type: internal\n      tls: true\n    config:\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      default.replication.factor: 3\n      min.insync.replicas: 2\n      inter.broker.protocol.version: \"3.4\"\n      log.retention.hours: 168  # 7 days\n      log.segment.bytes: 1073741824  # 1GB\n    storage:\n      type: persistent-claim\n      size: 100Gi\n      class: ocs-storagecluster-ceph-rbd\n    resources:\n      requests:\n        memory: 2Gi\n        cpu: 500m\n      limits:\n        memory: 4Gi\n        cpu: 1\n  zookeeper:\n    replicas: 3\n    storage:\n      type: persistent-claim\n      size: 10Gi\n      class: ocs-storagecluster-ceph-rbd\n    resources:\n      requests:\n        memory: 1Gi\n        cpu: 500m\n      limits:\n        memory: 2Gi\n        cpu: 1\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: kubernetes-events\n  namespace: event-bus\n  labels:\n    strimzi.io/cluster: event-cluster\nspec:\n  partitions: 12\n  replicas: 3\n  config:\n    retention.ms: 604800000  # 7 days\n    segment.ms: 3600000      # 1 hour\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: security-events\n  namespace: event-bus\n  labels:\n    strimzi.io/cluster: event-cluster\nspec:\n  partitions: 6\n  replicas: 3\n  config:\n    retention.ms: 2592000000  # 30 days\n    segment.ms: 3600000       # 1 hour\n---\napiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: infrastructure-changes\n  namespace: event-bus\n  labels:\n    strimzi.io/cluster: event-cluster\nspec:\n  partitions: 6\n  replicas: 3\n  config:\n    retention.ms: 1209600000  # 14 days\n    segment.ms: 3600000       # 1 hour\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#deploy-nats-jetstream","title":"Deploy NATS JetStream","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  namespace: event-bus\nspec:\n  serviceName: nats\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nats\n  template:\n    metadata:\n      labels:\n        app: nats\n    spec:\n      containers:\n      - name: nats\n        image: nats:alpine\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 7422\n          name: leafnodes\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        - containerPort: 7777\n          name: metrics\n        args:\n        - --config\n        - /etc/nats-config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats-config\n        - name: data\n          mountPath: /data\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 200m\n          limits:\n            memory: 1Gi\n            cpu: 500m\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n      storageClassName: ocs-storagecluster-ceph-rbd\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nats-config\n  namespace: event-bus\ndata:\n  nats.conf: |\n    port: 4222\n    http_port: 8222\n\n    cluster {\n      name: nats-cluster\n      port: 6222\n      routes = [\n        nats://nats-0.nats.event-bus.svc.cluster.local:6222\n        nats://nats-1.nats.event-bus.svc.cluster.local:6222\n        nats://nats-2.nats.event-bus.svc.cluster.local:6222\n      ]\n    }\n\n    jetstream {\n      store_dir: \"/data\"\n      max_memory_store: 256MB\n      max_file_store: 2GB\n    }\n\n    accounts {\n      $SYS { users = [ { user: \"admin\", pass: \"password\" } ] }\n      events {\n        jetstream: enabled\n        users = [\n          { user: \"event-publisher\", pass: \"publisher-secret\" }\n          { user: \"event-consumer\", pass: \"consumer-secret\" }\n        ]\n      }\n    }\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nats\n  namespace: event-bus\nspec:\n  selector:\n    app: nats\n  clusterIP: None\n  ports:\n  - name: client\n    port: 4222\n  - name: cluster\n    port: 6222\n  - name: monitor\n    port: 8222\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#step-4-configure-integration-adapters","title":"Step 4: Configure Integration Adapters","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#servicenow-integration-adapter","title":"ServiceNow Integration Adapter","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: servicenow-adapter\n  namespace: integration\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: servicenow-adapter\n  template:\n    metadata:\n      labels:\n        app: servicenow-adapter\n    spec:\n      containers:\n      - name: adapter\n        image: quay.io/example/servicenow-adapter:latest\n        env:\n        - name: SERVICENOW_INSTANCE\n          value: \"https://dev12345.service-now.com\"\n        - name: SERVICENOW_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: servicenow-credentials\n              key: username\n        - name: SERVICENOW_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: servicenow-credentials\n              key: password\n        - name: KAFKA_BROKERS\n          value: \"event-cluster-kafka-bootstrap.event-bus.svc.cluster.local:9092\"\n        - name: KAFKA_TOPICS\n          value: \"kubernetes-events,infrastructure-changes,security-events\"\n        - name: SERVICENOW_TABLE_MAPPING\n          value: |\n            {\n              \"kubernetes-events\": \"incident\",\n              \"infrastructure-changes\": \"change_request\",\n              \"security-events\": \"sn_si_incident\"\n            }\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 100m\n          limits:\n            memory: 512Mi\n            cpu: 200m\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: servicenow-credentials\n  namespace: integration\ntype: Opaque\ndata:\n  username: &lt;base64-encoded-username&gt;\n  password: &lt;base64-encoded-password&gt;\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: servicenow-field-mapping\n  namespace: integration\ndata:\n  mapping.json: |\n    {\n      \"incident\": {\n        \"short_description\": \"{{ .reason }}: {{ .message | truncate 100 }}\",\n        \"description\": \"Kubernetes Event Details:\\n\\nNamespace: {{ .namespace }}\\nObject: {{ .involvedObject.kind }}/{{ .involvedObject.name }}\\nReason: {{ .reason }}\\nMessage: {{ .message }}\\nFirst Seen: {{ .firstTimestamp }}\\nLast Seen: {{ .lastTimestamp }}\\nCount: {{ .count }}\",\n        \"category\": \"Software\",\n        \"subcategory\": \"Kubernetes\",\n        \"urgency\": \"{{ if eq .type \\\"Warning\\\" }}2{{ else }}3{{ end }}\",\n        \"impact\": \"{{ if contains .reason \\\"Failed\\\" }}2{{ else }}3{{ end }}\",\n        \"assignment_group\": \"Platform Engineering\",\n        \"caller_id\": \"kubernetes-system\",\n        \"u_kubernetes_cluster\": \"{{ .clusterName }}\",\n        \"u_kubernetes_namespace\": \"{{ .namespace }}\",\n        \"u_kubernetes_object\": \"{{ .involvedObject.kind }}/{{ .involvedObject.name }}\"\n      },\n      \"change_request\": {\n        \"short_description\": \"Infrastructure Change: {{ .reason }}\",\n        \"description\": \"{{ .message }}\",\n        \"category\": \"Standard\",\n        \"type\": \"Normal\",\n        \"risk\": \"Low\",\n        \"impact\": \"3\",\n        \"priority\": \"4\",\n        \"assignment_group\": \"Platform Engineering\"\n      }\n    }\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#publish-events-to-siem","title":"Publish Events to SIEM","text":"<ul> <li>Configure integration using SIEM platform's ingress mechanisms.</li> <li>Example for Splunk: <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: publish-events-splunk\n  namespace: integration\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: splunk-publisher\n            image: splunk-integration:latest\n            env:\n            - name: SPLUNK_HEC_URL\n              value: \"https://splunk-hec.local/services/collector\"\n            - name: SPLUNK_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: splunk-credentials\n                  key: hec_token\n            command: [\"publish\", \"--source=nats\", \"--target=splunk\"]\n</code></pre></li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#step-4-persistent-storage-for-event-retention","title":"Step 4: Persistent Storage for Event Retention","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#storage-configuration","title":"Storage Configuration","text":"<ul> <li>Use OpenShift Data Foundation for persistent storage.</li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: events-storage\n  namespace: event-bus\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n</code></pre>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#troubleshooting","title":"Troubleshooting","text":""},{"location":"use-cases/publishing-events-to-cmdb-siem/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li>Connectivity Issues: Verify network policies and DNS resolution to external CMDB and SIEM platforms.</li> <li>Event Capture Failures: Ensure event-controller configurations are correct and have appropriate RBAC permissions.</li> <li>Performance Bottlenecks: Scale event-bus components and monitor resource utilization.</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#best-practices","title":"Best Practices","text":"<ul> <li>Secure Integration: Use secure channels (TLS) for communication with external platforms.</li> <li>Scalable Event Processing: Implement horizontal scaling for event-bus solutions.</li> <li>Audit and Compliance: Maintain detailed logs and audit trails for event processing.</li> </ul>"},{"location":"use-cases/publishing-events-to-cmdb-siem/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":"<ul> <li>Automation and Policy Enforcement: Use event-driven automation for rapid remediation and policy enforcement.</li> <li>Proactive Monitoring: Enhance monitoring and observability for real-time insights into infrastructure and application changes.</li> </ul> <p>This use case guides implementing a scalable and flexible solution to publish vital Kubernetes events to enterprise IT management and security platforms, enhancing operational visibility and control.  </p>"},{"location":"use-cases/setup-multi-env-application/","title":"Use Case: Setup a New Application with 3 Environments (Dev, Lat, Prod)","text":""},{"location":"use-cases/setup-multi-env-application/#business-context","title":"Business Context","text":"<p>Setting up distinct environments for development, testing (LAT - User Acceptance Testing level), and production is crucial for ensuring software quality and stability. This use case demonstrates deploying applications across multiple clusters and namespaces, with development running on a separate cluster for enhanced isolation, while LAT and production environments share a cluster but use different namespaces.</p> <p>Key Focus: Label-Based Application Management This use case emphasizes the strategic use of Kubernetes labels for effective application lifecycle management, environment identification, resource organization, and automated operations across multi-cluster deployments.</p>"},{"location":"use-cases/setup-multi-env-application/#label-management-strategy","title":"Label Management Strategy","text":""},{"location":"use-cases/setup-multi-env-application/#core-labeling-standards","title":"Core Labeling Standards","text":"<p>This implementation follows a comprehensive labeling strategy for consistent resource management:</p>"},{"location":"use-cases/setup-multi-env-application/#standard-label-schema","title":"Standard Label Schema","text":"<pre><code># Application identification labels\napp.kubernetes.io/name: &lt;application-name&gt;           # Application name\napp.kubernetes.io/version: &lt;version&gt;                 # Application version\napp.kubernetes.io/component: &lt;component&gt;             # Component type (frontend, backend, database)\napp.kubernetes.io/part-of: &lt;system&gt;                  # Higher-level application/system\napp.kubernetes.io/managed-by: &lt;tool&gt;                # Management tool (argocd, helm, etc.)\n\n# Environment and deployment labels\nenvironment: &lt;env&gt;                                   # Environment (dev, lat, prod) \ntier: &lt;tier&gt;                                        # Tier (production, non-production)\ncluster: &lt;cluster-name&gt;                             # Target cluster identifier\nregion: &lt;region&gt;                                    # Geographic region\n\n# Operational labels\nowner: &lt;team&gt;                                       # Owning team\ncost-center: &lt;code&gt;                                 # Cost allocation\nmonitoring: &lt;enabled/disabled&gt;                     # Monitoring status\nbackup: &lt;policy&gt;                                    # Backup policy\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#label-selection-and-filtering-examples","title":"Label Selection and Filtering Examples","text":"<pre><code># Select all resources for a specific application\nkubectl get all -l app.kubernetes.io/name=sample-app\n\n# Select resources by environment\nkubectl get pods -l environment=prod\n\n# Select resources by cluster and environment\nkubectl get deployments -l cluster=prod-cluster,environment=lat\n\n# Select all production tier resources\nkubectl get services -l tier=production\n\n# Select resources managed by ArgoCD\nkubectl get all -l app.kubernetes.io/managed-by=argocd\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/setup-multi-env-application/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>Development Cluster: Dedicated OpenShift 4.12+ cluster for development workloads</li> <li>Production Cluster: OpenShift 4.12+ cluster hosting LAT and Production environments</li> <li>Red Hat Advanced Cluster Management (RHACM) for multi-cluster management</li> <li>Network connectivity between clusters for GitOps and management operations</li> <li>Shared container registry accessible by both clusters</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#resource-requirements","title":"Resource Requirements","text":""},{"location":"use-cases/setup-multi-env-application/#development-cluster","title":"Development Cluster","text":"<ul> <li>CPU: 4-8 cores for development workloads</li> <li>Memory: 16-32GB RAM for development environments</li> <li>Storage: Fast local storage for rapid development cycles</li> <li>Network: Internal network access, limited external exposure</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#production-cluster","title":"Production Cluster","text":"<ul> <li>CPU: 16-32 cores for LAT and Production workloads</li> <li>Memory: 64-128GB RAM for production-grade environments</li> <li>Storage: Enterprise-grade persistent storage with backup capabilities</li> <li>Network: Full production network access with security controls</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Development Cluster\"\n        DEV_NS[\"Development Namespace\"]\n        DEV_APP[\"Application: v1.0.0-dev\"]\n        DEV_CONFIG[\"Dev ConfigMaps/Secrets\"]\n        DEV_STORAGE[\"Dev Storage\"]\n    end\n\n    subgraph \"Production Cluster\"\n        subgraph \"LAT Environment\"\n            LAT_NS[\"LAT Namespace\"]\n            LAT_APP[\"Application: v1.0.0-rc\"]\n            LAT_CONFIG[\"LAT ConfigMaps/Secrets\"]\n            LAT_STORAGE[\"LAT Storage\"]\n        end\n\n        subgraph \"Production Environment\"\n            PROD_NS[\"Production Namespace\"]\n            PROD_APP[\"Application: v1.0.0\"]\n            PROD_CONFIG[\"Prod ConfigMaps/Secrets\"]\n            PROD_STORAGE[\"Prod Storage\"]\n        end\n    end\n\n    subgraph \"Shared Infrastructure\"\n        REGISTRY[\"Container Registry\"]\n        RHACM[\"Red Hat ACM\"]\n        GITOPS[\"GitOps (ArgoCD)\"]\n    end\n\n    DEV_NS --&gt; DEV_APP\n    DEV_APP --&gt; DEV_CONFIG\n    DEV_APP --&gt; DEV_STORAGE\n\n    LAT_NS --&gt; LAT_APP\n    LAT_APP --&gt; LAT_CONFIG\n    LAT_APP --&gt; LAT_STORAGE\n\n    PROD_NS --&gt; PROD_APP\n    PROD_APP --&gt; PROD_CONFIG\n    PROD_APP --&gt; PROD_STORAGE\n\n    REGISTRY --&gt; DEV_APP\n    REGISTRY --&gt; LAT_APP\n    REGISTRY --&gt; PROD_APP\n\n    RHACM --&gt; DEV_NS\n    RHACM --&gt; LAT_NS\n    RHACM --&gt; PROD_NS\n\n    GITOPS --&gt; DEV_NS\n    GITOPS --&gt; LAT_NS\n    GITOPS --&gt; PROD_NS\n\n    style DEV_NS fill:#ddf,stroke:#333\n    style LAT_NS fill:#ffd,stroke:#333\n    style PROD_NS fill:#fdd,stroke:#333\n    style REGISTRY fill:#f9f,stroke:#333\n    style RHACM fill:#9ff,stroke:#333</code></pre>"},{"location":"use-cases/setup-multi-env-application/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/setup-multi-env-application/#step-1-setup-multi-cluster-management-with-rhacm","title":"Step 1: Setup Multi-Cluster Management with RHACM","text":""},{"location":"use-cases/setup-multi-env-application/#deploy-rhacm-hub-cluster","title":"Deploy RHACM Hub Cluster","text":"<pre><code>apiVersion: operator.open-cluster-management.io/v1\nkind: MultiClusterHub\nmetadata:\n  name: multiclusterhub\n  namespace: open-cluster-management\nspec:\n  availabilityConfig: High\n  enableClusterBackup: true\n  imagePullSecret: multiclusterhub-operator-pull-secret\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#register-development-cluster-as-managed-cluster","title":"Register Development Cluster as Managed Cluster","text":"<pre><code>apiVersion: cluster.open-cluster-management.io/v1\nkind: ManagedCluster\nmetadata:\n  name: dev-cluster\n  labels:\n    # Cluster identification labels\n    cloud: auto-detect\n    vendor: OpenShift\n    environment: development\n    cluster-role: development\n    # Management labels\n    managed-by: rhacm\n    region: us-east-1\n    tier: non-production\n    # Operational labels\n    monitoring: enabled\n    cost-center: dev-ops\n    owner: platform-team\nspec:\n  hubAcceptsClient: true\n---\napiVersion: agent.open-cluster-management.io/v1\nkind: KlusterletAddonConfig\nmetadata:\n  name: dev-cluster\n  namespace: dev-cluster\nspec:\n  clusterName: dev-cluster\n  clusterNamespace: dev-cluster\n  clusterLabels:\n    cloud: auto-detect\n    vendor: OpenShift\n    environment: development\n    cluster-role: development\n    tier: non-production\n    region: us-east-1\n  applicationManager:\n    enabled: true\n  policyController:\n    enabled: true\n  searchCollector:\n    enabled: true\n  certPolicyController:\n    enabled: true\n  iamPolicyController:\n    enabled: true\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-2-create-namespaces-across-clusters","title":"Step 2: Create Namespaces Across Clusters","text":""},{"location":"use-cases/setup-multi-env-application/#development-cluster-namespace-configuration","title":"Development Cluster - Namespace Configuration","text":"<pre><code># Apply to Development Cluster\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: development\n  labels:\n    # Standard Kubernetes labels\n    name: development\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/part-of: sample-system\n    app.kubernetes.io/managed-by: argocd\n\n    # Environment and deployment labels\n    environment: dev\n    tier: non-production\n    cluster: dev-cluster\n    region: us-east-1\n\n    # Operational labels\n    owner: dev-team\n    cost-center: engineering\n    monitoring: enabled\n    backup: daily\n\n    # Feature flags for this environment\n    feature.experimental: \"true\"\n    feature.debug: \"true\"\n  annotations:\n    openshift.io/description: \"Development environment for sample application\"\n    openshift.io/display-name: \"Sample App - Development\"\n    cluster.open-cluster-management.io/managedCluster: \"dev-cluster\"\n    # Label management annotations\n    label-policy.io/required: \"environment,tier,owner\"\n    label-policy.io/validation: \"strict\"\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#production-cluster-lat-and-production-namespaces","title":"Production Cluster - LAT and Production Namespaces","text":"<pre><code># Apply to Production Cluster\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: lat\n  labels:\n    # Standard Kubernetes labels\n    name: lat\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/part-of: sample-system\n    app.kubernetes.io/managed-by: argocd\n\n    # Environment and deployment labels\n    environment: lat\n    tier: non-production\n    cluster: prod-cluster\n    region: us-east-1\n\n    # Operational labels\n    owner: qa-team\n    cost-center: quality-assurance\n    monitoring: enabled\n    backup: weekly\n\n    # Feature flags for LAT environment\n    feature.experimental: \"false\"\n    feature.debug: \"false\"\n    feature.performance-testing: \"true\"\n  annotations:\n    openshift.io/description: \"LAT/UAT environment for sample application\"\n    openshift.io/display-name: \"Sample App - LAT\"\n    cluster.open-cluster-management.io/managedCluster: \"prod-cluster\"\n    label-policy.io/required: \"environment,tier,owner\"\n    label-policy.io/validation: \"strict\"\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    # Standard Kubernetes labels\n    name: production\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/part-of: sample-system\n    app.kubernetes.io/managed-by: argocd\n\n    # Environment and deployment labels\n    environment: prod\n    tier: production\n    cluster: prod-cluster\n    region: us-east-1\n\n    # Operational labels\n    owner: platform-team\n    cost-center: production-ops\n    monitoring: enhanced\n    backup: continuous\n\n    # Production-specific feature flags\n    feature.experimental: \"false\"\n    feature.debug: \"false\"\n    feature.high-availability: \"true\"\n    feature.auto-scaling: \"true\"\n  annotations:\n    openshift.io/description: \"Production environment for sample application\"\n    openshift.io/display-name: \"Sample App - Production\"\n    cluster.open-cluster-management.io/managedCluster: \"prod-cluster\"\n    label-policy.io/required: \"environment,tier,owner,backup\"\n    label-policy.io/validation: \"strict\"\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-3-configure-cross-cluster-gitops-with-argocd","title":"Step 3: Configure Cross-Cluster GitOps with ArgoCD","text":""},{"location":"use-cases/setup-multi-env-application/#argocd-applicationset-for-multi-cluster-deployment-with-label-based-selection","title":"ArgoCD ApplicationSet for Multi-Cluster Deployment with Label-Based Selection","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: sample-app-multi-env\n  namespace: argocd\n  labels:\n    # ApplicationSet identification\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: applicationset\n    app.kubernetes.io/managed-by: argocd\n    app.kubernetes.io/part-of: sample-system\n    # Management labels\n    owner: platform-team\n    deployment-strategy: multi-cluster\nspec:\n  generators:\n  # Development cluster selector\n  - clusters:\n      selector:\n        matchLabels:\n          environment: development\n          tier: non-production\n      values:\n        environment: development\n        namespace: development\n        imageTag: \"1.0.0-dev\"\n        replicas: \"2\"\n        resources: \"small\"\n        cluster_type: \"development\"\n  # LAT environment selector (on production cluster)\n  - clusters:\n      selector:\n        matchLabels:\n          cluster-role: production\n          tier: production\n      values:\n        environment: lat\n        namespace: lat\n        imageTag: \"1.0.0-rc\"\n        replicas: \"2\"\n        resources: \"medium\"\n        cluster_type: \"production\"\n  # Production environment selector\n  - clusters:\n      selector:\n        matchLabels:\n          cluster-role: production\n          tier: production\n      values:\n        environment: production\n        namespace: production\n        imageTag: \"1.0.0\"\n        replicas: \"5\"\n        resources: \"large\"\n        cluster_type: \"production\"\n  template:\n    metadata:\n      name: 'sample-app-{{values.environment}}'\n      labels:\n        # Application labels for ArgoCD application\n        app.kubernetes.io/name: sample-app\n        app.kubernetes.io/component: application\n        app.kubernetes.io/managed-by: argocd\n        environment: '{{values.environment}}'\n        cluster-type: '{{values.cluster_type}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://git.example.com/sample-app-config\n        targetRevision: HEAD\n        path: overlays/{{values.environment}}\n        helm:\n          parameters:\n          - name: image.tag\n            value: '{{values.imageTag}}'\n          - name: replicaCount\n            value: '{{values.replicas}}'\n          - name: resources.profile\n            value: '{{values.resources}}'\n          - name: environment\n            value: '{{values.environment}}'\n          - name: cluster_type\n            value: '{{values.cluster_type}}'\n      destination:\n        server: '{{server}}'\n        namespace: '{{values.namespace}}'\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n        syncOptions:\n        - CreateNamespace=true\n        - RespectIgnoreDifferences=true\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-4-configure-rbac-and-service-accounts","title":"Step 4: Configure RBAC and Service Accounts","text":""},{"location":"use-cases/setup-multi-env-application/#development-cluster-service-account-with-enhanced-labels","title":"Development Cluster Service Account with Enhanced Labels","text":"<pre><code># Apply to Development Cluster\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sample-app-sa\n  namespace: development\n  labels:\n    # Standard Kubernetes labels\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: serviceaccount\n    app.kubernetes.io/managed-by: kubectl\n    app.kubernetes.io/part-of: sample-system\n\n    # Environment and deployment labels\n    app: sample-app\n    environment: dev\n    cluster: dev-cluster\n    tier: non-production\n\n    # RBAC and security labels\n    security.io/rbac: enabled\n    security.io/scope: namespace\n\n    # Operational labels\n    owner: dev-team\n    managed-by: platform-team\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: sample-app-role\n  namespace: development\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\", \"secrets\", \"pods\", \"services\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: sample-app-rolebinding\n  namespace: development\nsubjects:\n- kind: ServiceAccount\n  name: sample-app-sa\n  namespace: development\nroleRef:\n  kind: Role\n  name: sample-app-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#production-cluster-service-accounts-with-enhanced-labels","title":"Production Cluster Service Accounts with Enhanced Labels","text":"<pre><code># Apply to Production Cluster - LAT Environment\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sample-app-sa\n  namespace: lat\n  labels:\n    # Standard Kubernetes labels\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: serviceaccount\n    app.kubernetes.io/managed-by: kubectl\n    app.kubernetes.io/part-of: sample-system\n\n    # Environment and deployment labels\n    app: sample-app\n    environment: lat\n    cluster: prod-cluster\n    tier: non-production\n\n    # RBAC and security labels\n    security.io/rbac: enabled\n    security.io/scope: namespace\n    security.io/audit: enabled\n\n    # Operational labels\n    owner: qa-team\n    managed-by: platform-team\n---\n# Apply to Production Cluster - Production Environment\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sample-app-sa\n  namespace: production\n  labels:\n    # Standard Kubernetes labels\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: serviceaccount\n    app.kubernetes.io/managed-by: kubectl\n    app.kubernetes.io/part-of: sample-system\n\n    # Environment and deployment labels\n    app: sample-app\n    environment: prod\n    cluster: prod-cluster\n    tier: production\n\n    # RBAC and security labels\n    security.io/rbac: enabled\n    security.io/scope: namespace\n    security.io/audit: enhanced\n    security.io/compliance: required\n\n    # Operational labels\n    owner: platform-team\n    managed-by: platform-team\n    criticality: high\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-5-configure-environment-specific-resources","title":"Step 5: Configure Environment Specific Resources","text":""},{"location":"use-cases/setup-multi-env-application/#development-environment-configuration-apply-to-development-cluster","title":"Development Environment Configuration (Apply to Development Cluster)","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: development\n  labels:\n    app: sample-app\n    environment: dev\n    cluster: dev-cluster\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: config\ndata:\n  APP_ENV: \"development\"\n  LOG_LEVEL: \"debug\"\n  DATABASE_HOST: \"dev-postgres.development.svc.cluster.local\"\n  DATABASE_PORT: \"5432\"\n  DATABASE_NAME: \"sampleapp_dev\"\n  REDIS_HOST: \"dev-redis.development.svc.cluster.local\"\n  REDIS_PORT: \"6379\"\n  API_BASE_URL: \"https://api-dev.example.com\"\n  FEATURE_FLAGS: \"new-ui:true,beta-features:true\"\n  MAX_CONNECTIONS: \"10\"\n  CACHE_TTL: \"300\"\n  CLUSTER_NAME: \"dev-cluster\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\n  namespace: development\n  labels:\n    app: sample-app\n    environment: dev\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: secrets\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"dev_password_123\"\n  REDIS_PASSWORD: \"dev_redis_password\"\n  JWT_SECRET: \"dev_jwt_secret_key\"\n  API_KEY: \"dev_api_key_12345\"\n  ENCRYPTION_KEY: \"dev_encryption_key\"\n---\n# LAT Environment Configuration (Apply to Production Cluster)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: lat\n  labels:\n    app: sample-app\n    environment: lat\n    cluster: prod-cluster\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: config\ndata:\n  APP_ENV: \"lat\"\n  LOG_LEVEL: \"info\"\n  DATABASE_HOST: \"lat-postgres.lat.svc.cluster.local\"\n  DATABASE_PORT: \"5432\"\n  DATABASE_NAME: \"sampleapp_lat\"\n  REDIS_HOST: \"lat-redis.lat.svc.cluster.local\"\n  REDIS_PORT: \"6379\"\n  API_BASE_URL: \"https://api-lat.example.com\"\n  FEATURE_FLAGS: \"new-ui:true,beta-features:false\"\n  MAX_CONNECTIONS: \"20\"\n  CACHE_TTL: \"600\"\n  CLUSTER_NAME: \"prod-cluster\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\n  namespace: lat\n  labels:\n    app: sample-app\n    environment: lat\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: secrets\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"lat_password_456\"\n  REDIS_PASSWORD: \"lat_redis_password\"\n  JWT_SECRET: \"lat_jwt_secret_key\"\n  API_KEY: \"lat_api_key_67890\"\n  ENCRYPTION_KEY: \"lat_encryption_key\"\n---\n# Production Environment Configuration (Apply to Production Cluster)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: production\n  labels:\n    app: sample-app\n    environment: prod\n    cluster: prod-cluster\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: config\ndata:\n  APP_ENV: \"production\"\n  LOG_LEVEL: \"warn\"\n  DATABASE_HOST: \"prod-postgres.production.svc.cluster.local\"\n  DATABASE_PORT: \"5432\"\n  DATABASE_NAME: \"sampleapp_prod\"\n  REDIS_HOST: \"prod-redis.production.svc.cluster.local\"\n  REDIS_PORT: \"6379\"\n  API_BASE_URL: \"https://api.example.com\"\n  FEATURE_FLAGS: \"new-ui:false,beta-features:false\"\n  MAX_CONNECTIONS: \"50\"\n  CACHE_TTL: \"3600\"\n  CLUSTER_NAME: \"prod-cluster\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\n  namespace: production\n  labels:\n    app: sample-app\n    environment: prod\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/component: secrets\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"prod_secure_password_789\"\n  REDIS_PASSWORD: \"prod_redis_secure_password\"\n  JWT_SECRET: \"prod_jwt_secret_key_secure\"\n  API_KEY: \"prod_api_key_secure_abcdef\"\n  ENCRYPTION_KEY: \"prod_encryption_key_secure\"\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-6-deploy-application-across-clusters-and-environments","title":"Step 6: Deploy Application Across Clusters and Environments","text":""},{"location":"use-cases/setup-multi-env-application/#development-environment-deployment-apply-to-development-cluster","title":"Development Environment Deployment (Apply to Development Cluster)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app\n  namespace: development\n  labels:\n    app: sample-app\n    environment: dev\n    cluster: dev-cluster\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: application\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  selector:\n    matchLabels:\n      app: sample-app\n      environment: dev\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        environment: dev\n        app.kubernetes.io/name: sample-app\n        app.kubernetes.io/version: \"1.0.0\"\n    spec:\n      serviceAccountName: sample-app-sa\n      containers:\n      - name: sample-app\n        image: quay.io/example/sample-app:1.0.0-dev\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"200m\"\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 3\n        volumeMounts:\n        - name: app-data\n          mountPath: /data\n        - name: temp-storage\n          mountPath: /tmp\n      volumes:\n      - name: app-data\n        persistentVolumeClaim:\n          claimName: sample-app-data\n      - name: temp-storage\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sample-app-service\n  namespace: development\n  labels:\n    app: sample-app\n    environment: dev\nspec:\n  selector:\n    app: sample-app\n    environment: dev\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  type: ClusterIP\n---\n# LAT Environment Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app\n  namespace: lat\n  labels:\n    app: sample-app\n    environment: lat\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: application\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  selector:\n    matchLabels:\n      app: sample-app\n      environment: lat\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        environment: lat\n        app.kubernetes.io/name: sample-app\n        app.kubernetes.io/version: \"1.0.0\"\n    spec:\n      serviceAccountName: sample-app-sa\n      containers:\n      - name: sample-app\n        image: quay.io/example/sample-app:1.0.0-rc\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 3\n        volumeMounts:\n        - name: app-data\n          mountPath: /data\n        - name: temp-storage\n          mountPath: /tmp\n      volumes:\n      - name: app-data\n        persistentVolumeClaim:\n          claimName: sample-app-data\n      - name: temp-storage\n        emptyDir: {}\n---\n# Production Environment Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app\n  namespace: production\n  labels:\n    app: sample-app\n    environment: prod\n    app.kubernetes.io/name: sample-app\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/component: application\nspec:\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 2\n  selector:\n    matchLabels:\n      app: sample-app\n      environment: prod\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        environment: prod\n        app.kubernetes.io/name: sample-app\n        app.kubernetes.io/version: \"1.0.0\"\n    spec:\n      serviceAccountName: sample-app-sa\n      containers:\n      - name: sample-app\n        image: quay.io/example/sample-app:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"1\"\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 3\n        volumeMounts:\n        - name: app-data\n          mountPath: /data\n        - name: temp-storage\n          mountPath: /tmp\n      volumes:\n      - name: app-data\n        persistentVolumeClaim:\n          claimName: sample-app-data\n      - name: temp-storage\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - sample-app\n              topologyKey: kubernetes.io/hostname\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-5-configure-persistent-storage","title":"Step 5: Configure Persistent Storage","text":""},{"location":"use-cases/setup-multi-env-application/#persistentvolumeclaims-for-each-environment","title":"PersistentVolumeClaims for Each Environment","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sample-app-data\n  namespace: development\n  labels:\n    app: sample-app\n    environment: dev\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sample-app-data\n  namespace: lat\n  labels:\n    app: sample-app\n    environment: lat\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sample-app-data\n  namespace: production\n  labels:\n    app: sample-app\n    environment: prod\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: ocs-storagecluster-ceph-rbd\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#step-6-configure-network-and-resource-policies","title":"Step 6: Configure Network and Resource Policies","text":""},{"location":"use-cases/setup-multi-env-application/#network-policies","title":"Network Policies","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-specific\n  namespace: development\nspec:\n  podSelector:\n    matchLabels:\n      app: sample-app\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 10.0.0.0/8\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/8\n---\n# Repeat for LAT and Prod with respective rules\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#resource-quotas","title":"Resource Quotas","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-resources\n  namespace: development\nspec:\n  hard:\n    requests.cpu: \"2\"\n    requests.memory: 4Gi\n    limits.cpu: \"4\"\n    limits.memory: 8Gi\n---\n# Repeat for LAT and Prod with tailored allocations\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#validation-and-testing","title":"Validation and Testing","text":""},{"location":"use-cases/setup-multi-env-application/#validate-deployments","title":"Validate Deployments","text":"<pre><code>kubectl get deployments -n development\nkubectl get pods -n development\nkubectl get services -n development\n</code></pre> <ul> <li>Verify that all resources are correctly deployed and running.</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#test-application-functionality","title":"Test Application Functionality","text":"<ul> <li>Ensure each environment functions as expected from development to LAT and into production.</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#label-based-management-operations","title":"Label-Based Management Operations","text":""},{"location":"use-cases/setup-multi-env-application/#advanced-label-queries-and-operations","title":"Advanced Label Queries and Operations","text":"<pre><code># Application lifecycle management using labels\n\n# Find all resources belonging to sample-app across all environments\nkubectl get all --all-namespaces -l app.kubernetes.io/name=sample-app\n\n# Get all production tier resources\nkubectl get all --all-namespaces -l tier=production\n\n# List all resources managed by ArgoCD\nkubectl get all --all-namespaces -l app.kubernetes.io/managed-by=argocd\n\n# Find resources by owner team\nkubectl get all --all-namespaces -l owner=platform-team\n\n# Get all development environment resources\nkubectl get all --all-namespaces -l environment=dev\n\n# Find all resources with backup enabled\nkubectl get all --all-namespaces -l backup!=none\n\n# Complex queries combining multiple labels\nkubectl get deployments --all-namespaces -l 'app.kubernetes.io/name=sample-app,tier=production'\n\n# Scale all deployments in development environment\nkubectl scale deployments -l environment=dev --replicas=1 --all-namespaces\n\n# Delete all test resources (be careful!)\nkubectl delete all -l environment=test --all-namespaces\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#label-based-resource-policies","title":"Label-Based Resource Policies","text":"<pre><code># OPA Gatekeeper policy to enforce required labels\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        type: object\n        properties:\n          labels:\n            type: array\n            items:\n              type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n\n        violation[{\"msg\": msg}] {\n          required := input.parameters.labels\n          provided := input.review.object.metadata.labels\n          missing := required[_]\n          not provided[missing]\n          msg := sprintf(\"Missing required label: %v\", [missing])\n        }\n---\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: must-have-environment\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\"]\n      - apiGroups: [\"\"]\n        kinds: [\"Service\"]\n  parameters:\n    labels: [\"environment\", \"owner\", \"app.kubernetes.io/name\"]\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#best-practices-for-label-management","title":"Best Practices for Label Management","text":""},{"location":"use-cases/setup-multi-env-application/#label-strategy-best-practices","title":"Label Strategy Best Practices","text":"<ul> <li>Consistent Naming: Use standardized label keys across all environments</li> <li>Hierarchical Organization: Implement app.kubernetes.io standard labels</li> <li>Environment Identification: Always include environment and tier labels</li> <li>Ownership Tracking: Include owner and cost-center labels for accountability</li> <li>Automation Friendly: Design labels for automated selection and management</li> <li>Policy Enforcement: Use admission controllers to enforce required labels</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#configuration-management","title":"Configuration Management","text":"<ul> <li>Consistent Configuration Management: Use ConfigMaps and Secrets effectively across environments.</li> <li>Isolation: Ensure namespaces are isolated to prevent cross-environment contamination.</li> <li>Resource Management: Tailor quotas and limits to reflect environment purposes.</li> <li>Automation: Implement CI/CD pipelines for streamlined deployments.</li> <li>Monitoring: Use Prometheus and Grafana for monitoring and alerts.</li> </ul>"},{"location":"use-cases/setup-multi-env-application/#label-based-monitoring-and-alerting","title":"Label-Based Monitoring and Alerting","text":"<pre><code># Prometheus monitoring rules using labels\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: sample-app-alerts\n  labels:\n    app.kubernetes.io/name: sample-app\n    monitoring: prometheus\nspec:\n  groups:\n  - name: sample-app\n    rules:\n    - alert: ApplicationDown\n      expr: up{app_kubernetes_io_name=\"sample-app\"} == 0\n      for: 5m\n      labels:\n        severity: critical\n        environment: \"{{ $labels.environment }}\"\n        owner: \"{{ $labels.owner }}\"\n      annotations:\n        summary: \"Sample app is down in {{ $labels.environment }}\"\n        description: \"Application {{ $labels.app_kubernetes_io_name }} in {{ $labels.environment }} environment has been down for more than 5 minutes.\"\n</code></pre>"},{"location":"use-cases/setup-multi-env-application/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":"<ul> <li>GitOps Integration: Use ArgoCD for deploying and managing environment configurations.</li> <li>Policy Automation: Enforce standards and policies through OPA Gatekeeper.</li> <li>Multi-Cluster Management: Utilize RHACM for managing environments across multiple clusters.</li> </ul> <p>This guide provides a clear pathway for setting up a robust, multi-environment application deployment workflow within RH OVE using Kubernetes namespaces, ensuring seamless transitions from development through to production.</p>"},{"location":"use-cases/use-cases-table/","title":"Summary Table","text":""},{"location":"use-cases/use-cases-table/#use-cases-dependencies-graph","title":"Use Cases Dependencies Graph","text":"<p>This diagram shows the logical dependencies and recommended implementation flow between various use cases within the RH OVE ecosystem. The graph illustrates prerequisite relationships - arrows point from foundational capabilities to dependent advanced features.</p>"},{"location":"use-cases/use-cases-table/#color-legend","title":"Color Legend","text":"<ul> <li>\ud83d\udd35 VM Lifecycle (Light Blue): Core virtualization capabilities</li> <li>\ud83d\udfe3 Observability (Purple): Monitoring and insights</li> <li>\ud83d\udfe2 Application Deployment (Green): Application management and deployment</li> <li>\ud83d\udfe0 Security (Orange): Security and protection services  </li> <li>\ud83d\udd34 Enterprise Integration (Pink): Enterprise systems integration</li> <li>\ud83d\udfe1 PaaS Services (Light Green): Platform-as-a-Service offerings</li> </ul> <pre><code>graph TD\n    %% VM Lifecycle Foundation\n    VMImport[VM Import and Migration]\n    VMTemplate[VM Template Management]\n    VMScale[VM Scaling and Performance]\n    VMBackup[VM Backup and Recovery]\n\n    %% Observability Foundation\n    Observability[End-to-End Observability]\n\n    %% Application and Platform Services\n    HybridApps[Hybrid Applications]\n    DatabasePaaS[Database Services as PaaS]\n    MultiEnv[Multi-Environment Application Setup]\n\n    %% Security\n    WAF[WAF and Firewalling]\n\n    %% Enterprise Integration\n    EventPublish[Event Publishing to CMDB/SIEM]\n    DisasterRecovery[Disaster Recovery]\n    LegacyMod[Legacy Application Modernization]\n\n    %% VM Lifecycle Dependencies\n    VMImport --&gt; VMTemplate\n    VMTemplate --&gt; VMScale\n    VMTemplate --&gt; VMBackup\n    VMScale --&gt; DisasterRecovery\n    VMBackup --&gt; DisasterRecovery\n\n    %% Platform Dependencies\n    VMTemplate --&gt; MultiEnv\n    VMScale --&gt; HybridApps\n    VMBackup --&gt; HybridApps\n\n    %% Observability Dependencies\n    VMScale --&gt; Observability\n    HybridApps --&gt; Observability\n    Observability --&gt; WAF\n    Observability --&gt; EventPublish\n\n    %% Advanced Services Dependencies\n    HybridApps --&gt; DatabasePaaS\n    MultiEnv --&gt; DatabasePaaS\n    Observability --&gt; DatabasePaaS\n\n    %% Enterprise Integration Dependencies\n    DisasterRecovery --&gt; EventPublish\n    DatabasePaaS --&gt; EventPublish\n\n    %% Legacy Modernization Dependencies\n    HybridApps --&gt; LegacyMod\n    DatabasePaaS --&gt; LegacyMod\n    EventPublish --&gt; LegacyMod\n    DisasterRecovery --&gt; LegacyMod\n\n    %% Styling by Category and Complexity\n    classDef vmLifecycle fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef observability fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n    classDef application fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px,color:#000\n    classDef security fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000\n    classDef enterprise fill:#fce4ec,stroke:#880e4f,stroke-width:2px,color:#000\n    classDef paas fill:#f1f8e9,stroke:#33691e,stroke-width:2px,color:#000\n\n    class VMImport,VMTemplate,VMScale,VMBackup vmLifecycle\n    class Observability observability\n    class HybridApps,MultiEnv application\n    class WAF security\n    class DisasterRecovery,EventPublish,LegacyMod enterprise\n    class DatabasePaaS paas</code></pre>"},{"location":"use-cases/use-cases-table/#use-cases-summary-table","title":"Use Cases Summary Table","text":""},{"location":"use-cases/use-cases-table/#overview","title":"Overview","text":"<p>This table provides a comprehensive overview of all use cases documented for the Red Hat OpenShift Virtualization Engine (RH OVE) ecosystem.</p>"},{"location":"use-cases/use-cases-table/#use-cases-matrix","title":"Use Cases Matrix","text":"Use Case Category Complexity Key Technologies Business Value Implementation Time Prerequisites VM Import and Migration VM Lifecycle Medium KubeVirt, CDI, MTV Legacy system modernization 2-4 weeks OpenShift 4.12+, Source VM access VM Template Management VM Lifecycle Low KubeVirt, CDI Standardized deployments 1-2 weeks OpenShift 4.12+, Template storage VM Scaling and Performance VM Lifecycle High HPA, VPA, KubeVirt Resource optimization 3-6 weeks Metrics server, Monitoring stack VM Backup and Recovery VM Lifecycle Medium Rubrik CDM, Polaris Data protection 2-4 weeks Rubrik infrastructure Hybrid Applications Application Deployment High Cilium, Service Mesh Modernization flexibility 4-8 weeks Multi-network setup Database Services as PaaS PaaS Integration High DB Operators, Helm Self-service databases 6-10 weeks Persistent storage, Operators Legacy Application Modernization Enterprise Integration Very High MTA, Service Mesh, Tekton Digital transformation 12-24 weeks Application analysis Disaster Recovery Enterprise Integration High RHACM, Storage replication Business continuity 8-12 weeks Multi-site infrastructure End-to-End Observability Observability Medium Prometheus, Jaeger, Dynatrace Operational insights 3-6 weeks Monitoring infrastructure WAF and Firewalling Security Medium Cilium L4-L7, F5 BigIP Application security 2-4 weeks F5 BigIP appliance Event Publishing to CMDB/SIEM Integration High Event Bus, Adapters Enterprise integration 4-8 weeks CMDB/SIEM connectivity Multi-Environment Application Setup Application Deployment Medium RHACM, ArgoCD, Namespaces Development workflow 2-4 weeks Multi-cluster setup"},{"location":"use-cases/use-cases-table/#complexity-levels","title":"Complexity Levels","text":"Level Description Skills Required Timeline Low Basic configuration with standard components Platform administrator 1-2 weeks Medium Integration of multiple components with custom configuration Senior platform engineer 2-6 weeks High Complex multi-component solutions requiring custom development Solution architect + team 4-12 weeks Very High Enterprise-wide transformation requiring extensive planning Enterprise architect + multiple teams 12+ weeks"},{"location":"use-cases/use-cases-table/#category-breakdown","title":"Category Breakdown","text":""},{"location":"use-cases/use-cases-table/#vm-lifecycle-management","title":"VM Lifecycle Management","text":"<ul> <li>Purpose: Managing virtual machine operations and lifecycle</li> <li>Use Cases: 4 use cases covering import, templates, scaling, and backup</li> <li>Key Benefits: Infrastructure consolidation, operational efficiency</li> </ul>"},{"location":"use-cases/use-cases-table/#application-deployment","title":"Application Deployment","text":"<ul> <li>Purpose: Deploying and managing hybrid application architectures</li> <li>Use Cases: 1 comprehensive use case for hybrid applications</li> <li>Key Benefits: Application modernization, deployment flexibility</li> </ul>"},{"location":"use-cases/use-cases-table/#paas-integration","title":"PaaS Integration","text":"<ul> <li>Purpose: Providing platform services for development teams</li> <li>Use Cases: 1 comprehensive database services platform</li> <li>Key Benefits: Developer productivity, service standardization</li> </ul>"},{"location":"use-cases/use-cases-table/#enterprise-integration","title":"Enterprise Integration","text":"<ul> <li>Purpose: Integrating with existing enterprise systems and processes</li> <li>Use Cases: 2 use cases covering modernization and disaster recovery</li> <li>Key Benefits: Risk mitigation, business continuity</li> </ul>"},{"location":"use-cases/use-cases-table/#observability","title":"Observability","text":"<ul> <li>Purpose: Monitoring and understanding system behavior</li> <li>Use Cases: 1 comprehensive observability solution</li> <li>Key Benefits: Operational visibility, proactive issue resolution</li> </ul>"},{"location":"use-cases/use-cases-table/#security","title":"Security","text":"<ul> <li>Purpose: Protecting applications and infrastructure</li> <li>Use Cases: 1 WAF and firewalling solution</li> <li>Key Benefits: Security compliance, threat protection</li> </ul>"},{"location":"use-cases/use-cases-table/#integration","title":"Integration","text":"<ul> <li>Purpose: Connecting with external enterprise systems</li> <li>Use Cases: 1 event publishing integration</li> <li>Key Benefits: Enterprise integration, compliance reporting</li> </ul>"},{"location":"use-cases/use-cases-table/#implementation-priority-matrix","title":"Implementation Priority Matrix","text":""},{"location":"use-cases/use-cases-table/#phase-1-foundation-weeks-1-8","title":"Phase 1: Foundation (Weeks 1-8)","text":"<ol> <li>VM Template Management</li> <li>VM Import and Migration</li> <li>End-to-End Observability</li> </ol>"},{"location":"use-cases/use-cases-table/#phase-2-core-services-weeks-9-20","title":"Phase 2: Core Services (Weeks 9-20)","text":"<ol> <li>VM Scaling and Performance</li> <li>VM Backup and Recovery</li> <li>WAF and Firewalling</li> </ol>"},{"location":"use-cases/use-cases-table/#phase-3-advanced-integration-weeks-21-36","title":"Phase 3: Advanced Integration (Weeks 21-36)","text":"<ol> <li>Hybrid Applications</li> <li>Database Services as PaaS</li> <li>Event Publishing to CMDB/SIEM</li> </ol>"},{"location":"use-cases/use-cases-table/#phase-4-enterprise-transformation-weeks-37","title":"Phase 4: Enterprise Transformation (Weeks 37+)","text":"<ol> <li>Legacy Application Modernization</li> <li>Disaster Recovery</li> </ol>"},{"location":"use-cases/use-cases-table/#prerequisites-summary","title":"Prerequisites Summary","text":""},{"location":"use-cases/use-cases-table/#common-prerequisites","title":"Common Prerequisites","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>Sufficient compute, memory, and storage resources</li> <li>Network connectivity between components</li> <li>Identity provider integration</li> </ul>"},{"location":"use-cases/use-cases-table/#specialized-prerequisites","title":"Specialized Prerequisites","text":"<ul> <li>Rubrik Infrastructure: For VM backup and recovery</li> <li>F5 BigIP: For advanced WAF capabilities</li> <li>External Systems: CMDB, SIEM, legacy systems for integration use cases</li> <li>Multi-site Setup: For disaster recovery scenarios</li> </ul>"},{"location":"use-cases/use-cases-table/#success-metrics","title":"Success Metrics","text":"Use Case Category Key Performance Indicators VM Lifecycle VM provisioning time, resource utilization, backup success rate Application Deployment Deployment frequency, rollback rate, application performance PaaS Integration Service provisioning time, developer satisfaction, service availability Enterprise Integration Integration success rate, compliance score, incident response time Observability Mean time to detection (MTTD), alert accuracy, dashboard usage Security Security incident reduction, compliance pass rate, threat detection rate"},{"location":"use-cases/use-cases-table/#getting-started","title":"Getting Started","text":"<ol> <li>Assessment: Review your current infrastructure and identify priority use cases</li> <li>Planning: Create implementation roadmap based on complexity and business value</li> <li>Prerequisites: Ensure all required infrastructure and tools are available</li> <li>Pilot: Start with low-complexity use cases to build expertise</li> <li>Scale: Gradually implement more complex use cases as team capabilities grow</li> </ol> <p>For detailed implementation guidance, refer to the individual use case documentation linked in the table above.</p>"},{"location":"use-cases/vm-backup-recovery/","title":"Use Case: VM Backup and Recovery","text":""},{"location":"use-cases/vm-backup-recovery/#business-context","title":"Business Context","text":"<p>Ensuring data integrity and availability requires effective VM backup and recovery strategies. This use case explores how to implement comprehensive backup solutions and recovery processes to protect critical VM workloads, ensuring minimal downtime and data loss.</p>"},{"location":"use-cases/vm-backup-recovery/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/vm-backup-recovery/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>Rubrik Cloud Data Management for comprehensive backup and recovery</li> <li>Network connectivity for Rubrik management and data transfer</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Storage: Adequate space for backup snapshots and archival</li> <li>Network: Sufficient bandwidth for backup data transfer</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"OpenShift Cluster\"\n        VM1[\"VM Instance 1\"]\n        VM2[\"VM Instance 2\"]\n        VM3[\"VM Instance 3\"]\n    end\n\n    subgraph \"Rubrik Ecosystem\"\n        RUBRIK_CDM[\"Rubrik CDM\"]\n        RUBRIK_POLARIS[\"Rubrik Polaris\"]\n        SLA_DOMAINS[\"SLA Domains\"]\n    end\n\n    subgraph \"Storage &amp; Replication\"\n        LOCAL_STORAGE[\"Local Rubrik Storage\"]\n        REMOTE_RUBRIK[\"Remote Rubrik Cluster\"]\n        CLOUD_ARCHIVE[\"Cloud Archive (AWS/Azure)\"]\n    end\n\n    VM1 --&gt; RUBRIK_CDM\n    VM2 --&gt; RUBRIK_CDM\n    VM3 --&gt; RUBRIK_CDM\n\n    RUBRIK_CDM --&gt; SLA_DOMAINS\n    RUBRIK_CDM --&gt; LOCAL_STORAGE\n    LOCAL_STORAGE --&gt; REMOTE_RUBRIK\n    LOCAL_STORAGE --&gt; CLOUD_ARCHIVE\n\n    RUBRIK_POLARIS --&gt; RUBRIK_CDM\n    RUBRIK_POLARIS --&gt; REMOTE_RUBRIK\n\n    style RUBRIK_CDM fill:#f9f,stroke:#333\n    style RUBRIK_POLARIS fill:#ff9,stroke:#333\n    style SLA_DOMAINS fill:#9ff,stroke:#333</code></pre>"},{"location":"use-cases/vm-backup-recovery/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/vm-backup-recovery/#step-1-setup-backup-infrastructure","title":"Step 1: Setup Backup Infrastructure","text":""},{"location":"use-cases/vm-backup-recovery/#deploy-rubrik-kubernetes-connector","title":"Deploy Rubrik Kubernetes Connector","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rubrik-connector\n  namespace: rubrik-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rubrik-connector\n  template:\n    metadata:\n      labels:\n        app: rubrik-connector\n    spec:\n      containers:\n      - name: connector\n        image: rubrik/kubernetes-connector:latest\n        env:\n        - name: RUBRIK_CDM_URL\n          valueFrom:\n            secretKeyRef:\n              name: rubrik-credentials\n              key: cdm-url\n        - name: RUBRIK_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: rubrik-credentials\n              key: username\n        - name: RUBRIK_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rubrik-credentials\n              key: password\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: rubrik-kubeconfig\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: rubrik-credentials\n  namespace: rubrik-system\ntype: Opaque\ndata:\n  cdm-url: &lt;base64-encoded-rubrik-cdm-url&gt;\n  username: &lt;base64-encoded-username&gt;\n  password: &lt;base64-encoded-password&gt;\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#configure-rubrik-sla-domains","title":"Configure Rubrik SLA Domains","text":"<pre><code># Rubrik SLA Domain Configuration\napiVersion: rubrik.com/v1\nkind: SLADomain\nmetadata:\n  name: vm-backup-sla\n  namespace: vm-workloads\nspec:\n  name: \"VM Production Backup\"\n  frequencies:\n  - timeUnit: \"Daily\"\n    frequency: 1\n    retention: 30  # days\n  - timeUnit: \"Weekly\"\n    frequency: 1\n    retention: 12  # weeks\n  - timeUnit: \"Monthly\"\n    frequency: 1\n    retention: 12  # months\n  archivalSpecs:\n  - locationId: \"cloud-archive-location\"\n    archivalThreshold: 7  # days\n  replicationSpecs:\n  - locationId: \"remote-rubrik-cluster\"\n    retentionLimit: 90  # days\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#step-2-configure-vm-backup-policies","title":"Step 2: Configure VM Backup Policies","text":""},{"location":"use-cases/vm-backup-recovery/#assign-vms-to-sla-domains","title":"Assign VMs to SLA Domains","text":"<pre><code>apiVersion: rubrik.com/v1\nkind: VMBackupPolicy\nmetadata:\n  name: vm-backup-policy\n  namespace: vm-workloads\nspec:\n  selector:\n    matchLabels:\n      backup: \"enabled\"\n  slaRef:\n    name: vm-backup-sla\n  backupOptions:\n    consistencyType: \"application-consistent\"\n    excludedDisks: []\n    includeIndexing: true\n---\n# Apply backup policy to specific VMs\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: critical-app-vm\n  namespace: vm-workloads\n  labels:\n    backup: \"enabled\"\n    tier: \"production\"\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        backup: \"enabled\"\n    spec:\n      domain:\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n        memory:\n          guest: 8Gi\n        cpu:\n          cores: 4\n      volumes:\n      - dataVolume:\n          name: critical-app-vm-dv\n        name: rootdisk\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#on-demand-backup-configuration","title":"On-Demand Backup Configuration","text":"<pre><code># Trigger immediate backup using Rubrik CLI\nrubrik backup vm --vm-name \"critical-app-vm\" --namespace \"vm-workloads\" --sla \"VM Production Backup\"\n\n# Schedule backup using Rubrik API\ncurl -X POST \"https://rubrik-cdm.example.com/api/internal/vmware/vm/{vm-id}/snapshot\" \\\n  -H \"Authorization: Bearer ${RUBRIK_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"slaId\": \"vm-backup-sla-id\",\n    \"archivalPolicy\": {\n      \"locationId\": \"cloud-archive-location\",\n      \"retentionDays\": 2555\n    }\n  }'\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#step-3-configure-backup-storage-and-replication","title":"Step 3: Configure Backup Storage and Replication","text":""},{"location":"use-cases/vm-backup-recovery/#cloud-archive-configuration","title":"Cloud Archive Configuration","text":"<pre><code>apiVersion: rubrik.com/v1\nkind: CloudArchive\nmetadata:\n  name: aws-s3-archive\n  namespace: rubrik-system\nspec:\n  provider: \"AWS\"\n  bucketName: \"rubrik-vm-backups\"\n  region: \"us-west-2\"\n  encryptionType: \"AWS_KMS\"\n  credentials:\n    secretRef:\n      name: aws-credentials\n      namespace: rubrik-system\n  storageClass: \"GLACIER\"\n  retentionLock: true\n  retentionLockDurationDays: 2555  # 7 years\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-credentials\n  namespace: rubrik-system\ntype: Opaque\ndata:\n  access-key-id: &lt;base64-encoded-access-key&gt;\n  secret-access-key: &lt;base64-encoded-secret-key&gt;\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#replication-target-configuration","title":"Replication Target Configuration","text":"<pre><code>apiVersion: rubrik.com/v1\nkind: ReplicationTarget\nmetadata:\n  name: dr-site-rubrik\n  namespace: rubrik-system\nspec:\n  targetClusterAddress: \"rubrik-dr.example.com\"\n  credentials:\n    secretRef:\n      name: dr-rubrik-credentials\n  replicationBandwidth: \"100 Mbps\"\n  encryptionInTransit: true\n  retentionOnTarget: 90  # days\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#step-4-implement-recovery-procedures","title":"Step 4: Implement Recovery Procedures","text":""},{"location":"use-cases/vm-backup-recovery/#instant-recovery-configuration","title":"Instant Recovery Configuration","text":"<pre><code>apiVersion: rubrik.com/v1\nkind: VMRecovery\nmetadata:\n  name: vm-instant-recovery\n  namespace: vm-workloads\nspec:\n  sourceVM:\n    name: \"critical-app-vm\"\n    namespace: \"vm-workloads\"\n  recoveryPoint:\n    snapshotId: \"snapshot-id-from-rubrik\"\n    # or use timestamp\n    # timestamp: \"2024-01-15T10:30:00Z\"\n  recoveryOptions:\n    type: \"instant\"  # instant, full, or export\n    targetNamespace: \"vm-recovery\"\n    targetVMName: \"critical-app-vm-recovered\"\n    powerOn: true\n    preserveMAC: false\n  networkMapping:\n  - sourceNetwork: \"production-network\"\n    targetNetwork: \"recovery-network\"\n---\n# Alternative: Full VM Recovery\napiVersion: rubrik.com/v1\nkind: VMRecovery\nmetadata:\n  name: vm-full-recovery\n  namespace: vm-workloads\nspec:\n  sourceVM:\n    name: \"critical-app-vm\"\n    namespace: \"vm-workloads\"\n  recoveryPoint:\n    snapshotId: \"latest\"\n  recoveryOptions:\n    type: \"full\"\n    targetNamespace: \"vm-workloads\"\n    targetVMName: \"critical-app-vm\"\n    powerOn: false\n    overwriteExisting: true\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#granular-file-recovery","title":"Granular File Recovery","text":"<pre><code># Mount VM snapshot for file-level recovery\nrubrik vm mount --vm-name \"critical-app-vm\" \\\n  --snapshot-id \"snapshot-id\" \\\n  --mount-path \"/mnt/recovery\" \\\n  --read-only\n\n# Extract specific files\ncp /mnt/recovery/important-file.txt /recovery/destination/\n\n# Unmount when recovery is complete\nrubrik vm unmount --mount-id \"mount-id\"\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#cross-platform-recovery","title":"Cross-Platform Recovery","text":"<pre><code># Export VM to different platform (e.g., VMware to KubeVirt)\napiVersion: rubrik.com/v1\nkind: VMExport\nmetadata:\n  name: vm-cross-platform-recovery\n  namespace: vm-workloads\nspec:\n  sourceVM:\n    name: \"legacy-vmware-vm\"\n    platform: \"vmware\"\n  exportOptions:\n    targetPlatform: \"kubevirt\"\n    targetNamespace: \"migrated-workloads\"\n    format: \"ova\"\n    targetStorage:\n      storageClass: \"fast-ssd\"\n      size: \"100Gi\"\n  conversionOptions:\n    optimizeForKubernetes: true\n    removeVMwareTools: true\n    installKubernetesAgents: true\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/vm-backup-recovery/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/vm-backup-recovery/#rubrik-connector-issues","title":"Rubrik Connector Issues","text":"<ul> <li>Issue: Rubrik connector cannot communicate with CDM</li> <li>Solution: </li> <li>Verify network connectivity between OpenShift cluster and Rubrik CDM</li> <li>Check Rubrik credentials in the secret</li> <li>Ensure proper RBAC permissions for the connector service account</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#backup-failures","title":"Backup Failures","text":"<ul> <li>Issue: VM backups failing or incomplete</li> <li>Solution:</li> <li>Check VM quiesce settings and ensure VM tools are installed</li> <li>Verify SLA domain configuration and policies</li> <li>Monitor Rubrik cluster storage capacity</li> <li>Review backup logs in Rubrik Polaris dashboard</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#recovery-performance-issues","title":"Recovery Performance Issues","text":"<ul> <li>Issue: Slow recovery or instant recovery not working</li> <li>Solution:</li> <li>Verify network bandwidth between Rubrik and OpenShift cluster</li> <li>Check storage performance on target environment</li> <li>Use Rubrik's storage optimization features</li> <li>Consider using local recovery points for faster access</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#replication-and-archive-issues","title":"Replication and Archive Issues","text":"<ul> <li>Issue: Replication to remote site or cloud archive failing</li> <li>Solution:</li> <li>Verify bandwidth allocation and network policies</li> <li>Check cloud credentials and permissions</li> <li>Review replication target configuration</li> <li>Monitor Rubrik cluster bandwidth utilization</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#best-practices","title":"Best Practices","text":"<ul> <li>Regular Backups: Schedule regular backups and snapshots for critical VMs.</li> <li>Test Recovery: Regularly test recovery procedures using Rubrik capabilities to ensure data integrity.</li> <li>Security and Compliance: Leverage Rubrik's built-in encryption and compliance features.</li> </ul>"},{"location":"use-cases/vm-backup-recovery/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/vm-backup-recovery/#monitoring-and-alerting-integration","title":"Monitoring and Alerting Integration","text":"<pre><code># Rubrik Metrics ServiceMonitor\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: rubrik-metrics\n  namespace: rubrik-system\nspec:\n  selector:\n    matchLabels:\n      app: rubrik-connector\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n---\n# Rubrik Backup Alerts\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: rubrik-backup-alerts\n  namespace: rubrik-system\nspec:\n  groups:\n  - name: rubrik.backup\n    rules:\n    - alert: RubrikBackupFailure\n      expr: rubrik_backup_failures_total &gt; 0\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Rubrik backup failure detected\"\n        description: \"VM backup has failed for {{ $labels.vm_name }}\"\n\n    - alert: RubrikClusterCapacity\n      expr: rubrik_cluster_used_capacity_percentage &gt; 85\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Rubrik cluster capacity high\"\n        description: \"Rubrik cluster is {{ $value }}% full\"\n\n    - alert: RubrikReplicationDelay\n      expr: rubrik_replication_lag_minutes &gt; 60\n      for: 15m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Rubrik replication lag detected\"\n        description: \"Replication is {{ $value }} minutes behind\"\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#gitops-integration","title":"GitOps Integration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: rubrik-backup-policies\n  namespace: argocd\nspec:\n  project: platform-services\n  source:\n    repoURL: https://git.example.com/rubrik-backup-config\n    targetRevision: HEAD\n    path: policies/production\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: vm-workloads\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"use-cases/vm-backup-recovery/#multi-cluster-backup-management","title":"Multi-Cluster Backup Management","text":"<ul> <li>Centralized Management: Use Rubrik Polaris to manage backup policies across multiple OpenShift clusters</li> <li>Cross-Cluster Recovery: Implement recovery procedures that can restore VMs to different clusters</li> <li>Global SLA Management: Define consistent backup policies across the entire RH OVE ecosystem</li> </ul> <p>This guide provides the necessary steps and best practices for implementing VM backup and recovery processes in the RH OVE environment, ensuring data resilience and availability for enterprise workloads.</p>"},{"location":"use-cases/vm-importation/","title":"Use Case: VM Importation and Migration","text":""},{"location":"use-cases/vm-importation/#business-context","title":"Business Context","text":"<p>Organizations often need to migrate existing virtual machines from various virtualization platforms (VMware vSphere, Red Hat Virtualization, Hyper-V, or KVM) to OpenShift Virtualization. This use case demonstrates how to import VMs while maintaining data integrity, minimizing downtime, and ensuring seamless operation in the new environment.</p>"},{"location":"use-cases/vm-importation/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/vm-importation/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ cluster with KubeVirt enabled</li> <li>Container Data Importer (CDI) operator installed</li> <li>Sufficient storage capacity for VM disk images</li> <li>Network connectivity to source virtualization platform</li> <li>Migration Toolkit for Virtualization (MTV) operator (optional)</li> </ul>"},{"location":"use-cases/vm-importation/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: 2+ cores per VM being imported</li> <li>Memory: 4GB+ RAM per VM being imported</li> <li>Storage: 2x the source VM disk size (for staging and final storage)</li> <li>Network: High-bandwidth connection for large VM transfers</li> </ul>"},{"location":"use-cases/vm-importation/#implementation-architecture","title":"Implementation Architecture","text":"<pre><code>graph TB\n    subgraph \"Source Environment\"\n        VS[VMware vSphere]\n        RHV[Red Hat Virtualization]\n        HV[Hyper-V]\n        KVM[KVM/QEMU]\n    end\n\n    subgraph \"OpenShift Cluster\"\n        MTV[Migration Toolkit for Virtualization]\n        CDI[Container Data Importer]\n        PVC[PersistentVolumeClaim]\n        DV[DataVolume]\n        VM[VirtualMachine]\n    end\n\n    subgraph \"Storage\"\n        SC[StorageClass]\n        PV[PersistentVolume]\n        CSI[CSI Driver]\n    end\n\n    VS --&gt; MTV\n    RHV --&gt; MTV\n    HV --&gt; MTV\n    KVM --&gt; CDI\n\n    MTV --&gt; DV\n    CDI --&gt; DV\n    DV --&gt; PVC\n    PVC --&gt; PV\n    PV --&gt; CSI\n\n    DV --&gt; VM\n\n    style MTV fill:#99ccff\n    style CDI fill:#99ffcc\n    style VM fill:#ff99cc</code></pre>"},{"location":"use-cases/vm-importation/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/vm-importation/#step-1-prepare-the-source-environment","title":"Step 1: Prepare the Source Environment","text":""},{"location":"use-cases/vm-importation/#for-vmware-vsphere","title":"For VMware vSphere","text":"<pre><code># Create a service account for MTV\n# In vSphere Client:\n# 1. Create a dedicated user account\n# 2. Assign minimal required permissions:\n#    - Datastore &gt; Browse datastore\n#    - Virtual machine &gt; Snapshot management\n#    - Virtual machine &gt; Provisioning\n</code></pre>"},{"location":"use-cases/vm-importation/#for-kvmqemu-direct-import","title":"For KVM/QEMU Direct Import","text":"<pre><code># Prepare VM disk images\nqemu-img convert -f qcow2 -O qcow2 /path/to/source.qcow2 /path/to/optimized.qcow2\n\n# Compress for faster transfer\ngzip /path/to/optimized.qcow2\n</code></pre>"},{"location":"use-cases/vm-importation/#step-2-install-migration-tools","title":"Step 2: Install Migration Tools","text":""},{"location":"use-cases/vm-importation/#install-migration-toolkit-for-virtualization","title":"Install Migration Toolkit for Virtualization","text":"<pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: mtv-operator\n  namespace: openshift-mtv\nspec:\n  channel: release-v2.4\n  installPlanApproval: Automatic\n  name: mtv-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-mtv\n</code></pre>"},{"location":"use-cases/vm-importation/#install-container-data-importer","title":"Install Container Data Importer","text":"<pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: containerized-data-importer\n  namespace: openshift-cnv\nspec:\n  channel: stable\n  installPlanApproval: Automatic\n  name: containerized-data-importer\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre>"},{"location":"use-cases/vm-importation/#step-3-configure-source-provider-mtv","title":"Step 3: Configure Source Provider (MTV)","text":"<pre><code>apiVersion: forklift.konveyor.io/v1beta1\nkind: Provider\nmetadata:\n  name: vmware-source\n  namespace: openshift-mtv\nspec:\n  type: vsphere\n  url: https://vcenter.example.com/sdk\n  secret:\n    name: vmware-credentials\n    namespace: openshift-mtv\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vmware-credentials\n  namespace: openshift-mtv\ntype: Opaque\nstringData:\n  user: migration-user@vsphere.local\n  password: \"your-secure-password\"\n  thumbprint: \"AA:BB:CC:DD:EE:FF:00:11:22:33:44:55:66:77:88:99:AA:BB:CC:DD\"\n</code></pre>"},{"location":"use-cases/vm-importation/#step-4-create-migration-plan","title":"Step 4: Create Migration Plan","text":"<pre><code>apiVersion: forklift.konveyor.io/v1beta1\nkind: Plan\nmetadata:\n  name: webapp-migration-plan\n  namespace: openshift-mtv\nspec:\n  provider:\n    source:\n      name: vmware-source\n      namespace: openshift-mtv\n    destination:\n      name: host\n      namespace: openshift-mtv\n  targetNamespace: app-web-prod\n  vms:\n  - id: vm-12345\n    name: web-server-01\n  - id: vm-12346\n    name: app-server-01\n  map:\n    network:\n    - source:\n        name: \"VM Network\"\n      destination:\n        name: default\n        type: pod\n    - source:\n        name: \"Storage Network\"\n      destination:\n        name: storage-net\n        type: multus\n    storage:\n    - source:\n        name: \"datastore1\"\n      destination:\n        storageClass: ocs-storagecluster-ceph-rbd\n    - source:\n        name: \"datastore2\"\n      destination:\n        storageClass: ocs-storagecluster-cephfs\n</code></pre>"},{"location":"use-cases/vm-importation/#step-5-direct-import-using-cdi-alternative-method","title":"Step 5: Direct Import Using CDI (Alternative Method)","text":""},{"location":"use-cases/vm-importation/#import-from-httphttps-source","title":"Import from HTTP/HTTPS Source","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: imported-vm-disk\n  namespace: app-web-prod\nspec:\n  source:\n    http:\n      url: \"https://storage.example.com/vm-images/web-server.qcow2\"\n      secretRef: \"image-pull-secret\"\n  storage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 50Gi\n    storageClassName: ocs-storagecluster-ceph-rbd\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: image-pull-secret\n  namespace: app-web-prod\ntype: kubernetes.io/basic-auth\nstringData:\n  username: \"storage-user\"\n  password: \"storage-password\"\n</code></pre>"},{"location":"use-cases/vm-importation/#import-from-registry","title":"Import from Registry","text":"<pre><code>apiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: registry-imported-vm\n  namespace: app-web-prod\nspec:\n  source:\n    registry:\n      url: \"docker://quay.io/example/vm-images:web-server-v1.0\"\n      pullMethod: pod\n      secretRef: \"registry-credentials\"\n  storage:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 30Gi\n    storageClassName: ocs-storagecluster-ceph-rbd\n</code></pre>"},{"location":"use-cases/vm-importation/#step-6-create-virtual-machine-definition","title":"Step 6: Create Virtual Machine Definition","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: imported-web-server\n  namespace: app-web-prod\n  labels:\n    app: web-server\n    tier: frontend\n    environment: production\n  annotations:\n    description: \"Imported web server from VMware vSphere\"\nspec:\n  running: false\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: imported-web-server\n    spec:\n      domain:\n        cpu:\n          cores: 4\n          sockets: 1\n          threads: 1\n        memory:\n          guest: 8Gi\n        devices:\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: storage-network\n            bridge: {}\n        machine:\n          type: pc-q35-rhel8.6.0\n        resources:\n          requests:\n            memory: 8Gi\n            cpu: 4\n      networks:\n      - name: default\n        pod: {}\n      - name: storage-network\n        multus:\n          networkName: storage-net\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: imported-vm-disk\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            user: admin\n            password: changeme\n            chpasswd: { expire: False }\n            ssh_pwauth: True\n            package_upgrade: true\n            packages:\n              - qemu-guest-agent\n            runcmd:\n              - systemctl enable qemu-guest-agent\n              - systemctl start qemu-guest-agent\n</code></pre>"},{"location":"use-cases/vm-importation/#step-7-execute-migration","title":"Step 7: Execute Migration","text":""},{"location":"use-cases/vm-importation/#start-migration-plan","title":"Start Migration Plan","text":"<pre><code># Apply the migration plan\noc apply -f migration-plan.yaml\n\n# Monitor migration progress\noc get migration -n openshift-mtv -w\n\n# Check migration status\noc describe migration webapp-migration -n openshift-mtv\n</code></pre>"},{"location":"use-cases/vm-importation/#monitor-import-progress-cdi","title":"Monitor Import Progress (CDI)","text":"<pre><code># Check DataVolume status\noc get dv imported-vm-disk -n app-web-prod\n\n# Monitor import progress\noc describe dv imported-vm-disk -n app-web-prod\n\n# Check PVC status\noc get pvc imported-vm-disk -n app-web-prod\n</code></pre>"},{"location":"use-cases/vm-importation/#step-8-post-migration-validation","title":"Step 8: Post-Migration Validation","text":""},{"location":"use-cases/vm-importation/#start-and-validate-vm","title":"Start and Validate VM","text":"<pre><code># Start the imported VM\noc patch vm imported-web-server -n app-web-prod --type merge -p '{\"spec\":{\"running\":true}}'\n\n# Check VM status\noc get vm imported-web-server -n app-web-prod\n\n# Check VMI status\noc get vmi imported-web-server -n app-web-prod\n\n# Access VM console\nvirtctl console imported-web-server -n app-web-prod\n</code></pre>"},{"location":"use-cases/vm-importation/#network-connectivity-test","title":"Network Connectivity Test","text":"<pre><code># Test pod network connectivity\noc exec -it deployment/test-client -- ping $(oc get vmi imported-web-server -n app-web-prod -o jsonpath='{.status.interfaces[0].ipAddress}')\n\n# Test multus network connectivity\noc exec -it deployment/storage-client -- ping $(oc get vmi imported-web-server -n app-web-prod -o jsonpath='{.status.interfaces[1].ipAddress}')\n</code></pre>"},{"location":"use-cases/vm-importation/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/vm-importation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/vm-importation/#1-import-fails-with-insufficient-storage","title":"1. Import Fails with \"Insufficient Storage\"","text":"<pre><code># Check available storage\noc get pv | grep Available\n\n# Check storage class\noc describe storageclass ocs-storagecluster-ceph-rbd\n\n# Solution: Request smaller disk or add more storage capacity\n</code></pre>"},{"location":"use-cases/vm-importation/#2-network-import-timeout","title":"2. Network Import Timeout","text":"<pre><code># Increase timeout in DataVolume\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: imported-vm-disk\n  annotations:\n    cdi.kubevirt.io/storage.pod.restarts: \"10\"\n    cdi.kubevirt.io/storage.pod.retries: \"4\"\nspec:\n  # ... rest of spec\n</code></pre>"},{"location":"use-cases/vm-importation/#3-vm-boot-issues-after-import","title":"3. VM Boot Issues After Import","text":"<pre><code># Check VM events\noc describe vm imported-web-server -n app-web-prod\n\n# Common fixes:\n# 1. Update machine type\n# 2. Install virtio drivers\n# 3. Adjust memory/CPU allocation\n</code></pre>"},{"location":"use-cases/vm-importation/#4-performance-issues","title":"4. Performance Issues","text":"<pre><code># Optimize disk performance\nspec:\n  domain:\n    devices:\n      disks:\n      - name: rootdisk\n        disk:\n          bus: virtio\n        cache: writethrough  # Add cache policy\n        io: native          # Add I/O policy\n</code></pre>"},{"location":"use-cases/vm-importation/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/vm-importation/#pre-migration","title":"Pre-Migration","text":"<ol> <li>Inventory Assessment: Document all VMs, their dependencies, and resource requirements</li> <li>Network Planning: Map source networks to destination networks</li> <li>Storage Planning: Choose appropriate storage classes for different workload types</li> <li>Security Review: Ensure imported VMs meet security standards</li> </ol>"},{"location":"use-cases/vm-importation/#during-migration","title":"During Migration","text":"<ol> <li>Batch Processing: Migrate VMs in small batches to manage resources</li> <li>Monitoring: Continuously monitor migration progress and resource utilization</li> <li>Validation: Test each migrated VM before proceeding to the next batch</li> <li>Documentation: Keep detailed logs of the migration process</li> </ol>"},{"location":"use-cases/vm-importation/#post-migration","title":"Post-Migration","text":"<ol> <li>Performance Tuning: Optimize VM configurations for the new environment</li> <li>Security Hardening: Apply security policies and configurations</li> <li>Backup Configuration: Set up backup strategies for migrated VMs</li> <li>Monitoring Setup: Configure monitoring and alerting for new VMs</li> </ol>"},{"location":"use-cases/vm-importation/#production-considerations","title":"Production Considerations","text":"<ol> <li>Resource Quotas: Implement appropriate resource quotas per namespace</li> <li>Network Policies: Apply network segmentation policies</li> <li>Storage Policies: Use appropriate storage classes for different tiers</li> <li>Disaster Recovery: Plan for cross-cluster failover scenarios</li> </ol>"},{"location":"use-cases/vm-importation/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/vm-importation/#gitops-integration","title":"GitOps Integration","text":"<pre><code># ArgoCD Application for VM management\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: imported-vms\n  namespace: argocd\nspec:\n  project: production\n  source:\n    repoURL: https://git.company.com/rh-ove/vm-configs\n    targetRevision: HEAD\n    path: imported-vms\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: app-web-prod\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n</code></pre>"},{"location":"use-cases/vm-importation/#monitoring-integration","title":"Monitoring Integration","text":"<pre><code># ServiceMonitor for VM metrics\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: vm-metrics\n  namespace: app-web-prod\nspec:\n  selector:\n    matchLabels:\n      app: kubevirt-vm\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n</code></pre> <p>This comprehensive guide provides everything needed to successfully import and migrate VMs into the RH OVE multi-cluster ecosystem while maintaining enterprise-grade security, monitoring, and operational standards.</p>"},{"location":"use-cases/vm-scaling-performance/","title":"Use Case: VM Scaling and Performance Optimization","text":""},{"location":"use-cases/vm-scaling-performance/#business-context","title":"Business Context","text":"<p>Dynamic scaling and performance optimization are essential for maintaining optimal VM performance while managing resource costs. This use case demonstrates how to implement auto-scaling, resource optimization, and performance monitoring for virtual machines in the RH OVE ecosystem.</p>"},{"location":"use-cases/vm-scaling-performance/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/vm-scaling-performance/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>Horizontal Pod Autoscaler (HPA) support</li> <li>Metrics server and custom metrics API</li> <li>Persistent storage with performance monitoring capabilities</li> <li>CPU and memory monitoring tools (Prometheus/Grafana)</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: Variable based on workload demands</li> <li>Memory: Dynamic allocation based on usage patterns</li> <li>Storage: High-performance storage with IOPS monitoring</li> <li>Network: Low-latency network for performance-sensitive applications</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Monitoring Stack\"\n        PROMETHEUS[\"Prometheus\"]\n        GRAFANA[\"Grafana\"]\n        ALERTMANAGER[\"AlertManager\"]\n    end\n\n    subgraph \"Scaling Infrastructure\"\n        HPA[\"Horizontal Pod Autoscaler\"]\n        VPA[\"Vertical Pod Autoscaler\"]\n        METRICS[\"Metrics Server\"]\n    end\n\n    subgraph \"VM Workloads\"\n        VM1[\"VM Instance 1\"]\n        VM2[\"VM Instance 2\"]\n        VM3[\"VM Instance 3\"]\n    end\n\n    PROMETHEUS --&gt; ALERTMANAGER\n    GRAFANA --&gt; PROMETHEUS\n    METRICS --&gt; HPA\n    METRICS --&gt; VPA\n    HPA --&gt; VM1\n    HPA --&gt; VM2\n    VPA --&gt; VM3\n\n    VM1 --&gt; PROMETHEUS\n    VM2 --&gt; PROMETHEUS\n    VM3 --&gt; PROMETHEUS\n\n    style PROMETHEUS fill:#f9f,stroke:#333\n    style HPA fill:#ff9,stroke:#333</code></pre>"},{"location":"use-cases/vm-scaling-performance/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/vm-scaling-performance/#step-1-enable-vm-performance-monitoring","title":"Step 1: Enable VM Performance Monitoring","text":""},{"location":"use-cases/vm-scaling-performance/#deploy-vm-with-performance-monitoring","title":"Deploy VM with Performance Monitoring","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: performance-vm\n  namespace: vm-workloads\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        app: performance-vm\n        monitoring: \"enabled\"\n    spec:\n      domain:\n        cpu:\n          cores: 2\n          model: host-passthrough\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n          interfaces:\n          - name: default\n            bridge: {}\n        memory:\n          guest: 4Gi\n        resources:\n          requests:\n            memory: 4Gi\n            cpu: 2\n          limits:\n            memory: 8Gi\n            cpu: 4\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - dataVolume:\n          name: performance-vm-dv\n        name: rootdisk\n---\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: performance-vm-dv\n  namespace: vm-workloads\nspec:\n  source:\n    http:\n      url: \"https://vm-images.example.com/performance-vm.img\"\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 50Gi\n    storageClassName: fast-ssd\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#step-2-configure-horizontal-pod-autoscaler-for-vms","title":"Step 2: Configure Horizontal Pod Autoscaler for VMs","text":""},{"location":"use-cases/vm-scaling-performance/#hpa-configuration","title":"HPA Configuration","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vm-hpa\n  namespace: vm-workloads\nspec:\n  scaleTargetRef:\n    apiVersion: kubevirt.io/v1\n    kind: VirtualMachine\n    name: performance-vm\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 60\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 25\n        periodSeconds: 60\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#step-3-implement-vertical-pod-autoscaler","title":"Step 3: Implement Vertical Pod Autoscaler","text":""},{"location":"use-cases/vm-scaling-performance/#vpa-configuration","title":"VPA Configuration","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: vm-vpa\n  namespace: vm-workloads\nspec:\n  targetRef:\n    apiVersion: kubevirt.io/v1\n    kind: VirtualMachine\n    name: performance-vm\n  updatePolicy:\n    updateMode: \"Auto\"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: compute\n      minAllowed:\n        cpu: 100m\n        memory: 1Gi\n      maxAllowed:\n        cpu: 8\n        memory: 16Gi\n      controlledResources: [\"cpu\", \"memory\"]\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#step-4-performance-monitoring-and-alerting","title":"Step 4: Performance Monitoring and Alerting","text":""},{"location":"use-cases/vm-scaling-performance/#servicemonitor-for-vm-metrics","title":"ServiceMonitor for VM Metrics","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: vm-performance-monitor\n  namespace: vm-workloads\n  labels:\n    app: vm-monitor\nspec:\n  selector:\n    matchLabels:\n      monitoring: \"enabled\"\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n    honorLabels: true\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#performance-alerting-rules","title":"Performance Alerting Rules","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: vm-performance-alerts\n  namespace: vm-workloads\nspec:\n  groups:\n  - name: vm.performance\n    rules:\n    - alert: VMHighCPUUsage\n      expr: kubevirt_vm_cpu_usage &gt; 0.8\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} has high CPU usage\"\n        description: \"VM {{ $labels.name }} CPU usage is above 80% for more than 5 minutes.\"\n\n    - alert: VMHighMemoryUsage\n      expr: kubevirt_vm_memory_usage &gt; 0.9\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"VM {{ $labels.name }} has high memory usage\"\n        description: \"VM {{ $labels.name }} memory usage is above 90% for more than 5 minutes.\"\n\n    - alert: VMDiskIOHigh\n      expr: kubevirt_vm_disk_iops &gt; 1000\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"VM {{ $labels.name }} has high disk I/O\"\n        description: \"VM {{ $labels.name }} disk IOPS is above 1000 for more than 10 minutes.\"\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#step-5-performance-optimization-strategies","title":"Step 5: Performance Optimization Strategies","text":""},{"location":"use-cases/vm-scaling-performance/#cpu-pinning-for-performance-critical-vms","title":"CPU Pinning for Performance-Critical VMs","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: high-performance-vm\n  namespace: vm-workloads\nspec:\n  running: true\n  template:\n    metadata:\n      annotations:\n        cpu-load-balancing.crio.io: \"disable\"\n        cpu-quota.crio.io: \"disable\"\n        irq-load-balancing.crio.io: \"disable\"\n    spec:\n      domain:\n        cpu:\n          cores: 4\n          sockets: 1\n          threads: 1\n          dedicatedCpuPlacement: true\n          isolateEmulatorThread: true\n          model: host-passthrough\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n        memory:\n          guest: 8Gi\n          hugepages:\n            pageSize: 1Gi\n        resources:\n          requests:\n            memory: 8Gi\n            cpu: 4\n          limits:\n            memory: 8Gi\n            cpu: 4\n      nodeSelector:\n        node-role.kubernetes.io/worker: \"\"\n        performance-node: \"true\"\n      volumes:\n      - dataVolume:\n          name: high-performance-vm-dv\n        name: rootdisk\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#step-6-storage-performance-optimization","title":"Step 6: Storage Performance Optimization","text":""},{"location":"use-cases/vm-scaling-performance/#high-performance-storage-configuration","title":"High-Performance Storage Configuration","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: high-performance-ssd\nprovisioner: kubernetes.io/csi-driver\nparameters:\n  type: gp3\n  iops: \"3000\"\n  throughput: \"125\"\n  fsType: ext4\nreclaimPolicy: Delete\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/vm-scaling-performance/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/vm-scaling-performance/#scaling-not-triggering","title":"Scaling Not Triggering","text":"<ul> <li>Issue: HPA/VPA not scaling VMs as expected</li> <li>Solution: </li> <li>Check metrics server functionality: <code>kubectl top nodes</code></li> <li>Verify resource requests are set properly</li> <li>Check HPA status: <code>kubectl describe hpa vm-hpa</code></li> </ul>"},{"location":"use-cases/vm-scaling-performance/#performance-degradation","title":"Performance Degradation","text":"<ul> <li>Issue: VM performance is not meeting expectations</li> <li>Solution:</li> <li>Review CPU pinning configuration</li> <li>Check for resource contention on nodes</li> <li>Verify storage performance metrics</li> <li>Analyze network latency and throughput</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#memory-issues","title":"Memory Issues","text":"<ul> <li>Issue: Out of memory errors or high memory pressure</li> <li>Solution:</li> <li>Increase memory limits in VM specification</li> <li>Enable hugepages for better memory performance</li> <li>Check for memory leaks in applications</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/vm-scaling-performance/#resource-management","title":"Resource Management","text":"<ul> <li>Right-sizing: Start with conservative resource allocations and scale based on monitoring data</li> <li>Resource Limits: Always set both requests and limits to prevent resource starvation</li> <li>Node Selection: Use node selectors and taints to ensure VMs are scheduled on appropriate nodes</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>CPU Optimization: Use CPU pinning for performance-critical workloads</li> <li>Memory Optimization: Configure hugepages for memory-intensive applications</li> <li>Storage Optimization: Use high-performance storage classes for I/O intensive workloads</li> <li>Network Optimization: Configure SR-IOV for network-intensive applications</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<ul> <li>Proactive Monitoring: Set up comprehensive monitoring for all performance metrics</li> <li>Alert Thresholds: Configure appropriate alert thresholds to prevent performance issues</li> <li>Capacity Planning: Use historical data for capacity planning and resource allocation</li> </ul>"},{"location":"use-cases/vm-scaling-performance/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/vm-scaling-performance/#gitops-integration","title":"GitOps Integration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: vm-performance\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://git.example.com/vm-performance-config\n    targetRevision: HEAD\n    path: performance\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: vm-workloads\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"use-cases/vm-scaling-performance/#multi-cluster-performance-management","title":"Multi-Cluster Performance Management","text":"<ul> <li>Centralized Monitoring: Aggregate performance metrics across multiple clusters</li> <li>Cross-Cluster Scaling: Implement scaling policies that consider cluster resource availability</li> <li>Performance Benchmarking: Establish performance baselines across different cluster configurations</li> </ul> <p>This comprehensive guide provides the tools and strategies needed to implement effective VM scaling and performance optimization within the RH OVE ecosystem, ensuring optimal resource utilization and application performance.</p>"},{"location":"use-cases/vm-template-management/","title":"Use Case: VM Template Management","text":""},{"location":"use-cases/vm-template-management/#business-context","title":"Business Context","text":"<p>Efficient VM template management is crucial for maintaining consistency, reducing deployment times, and ensuring that VMs adhere to organizational standards. By leveraging templates, organizations can streamline the creation of VMs with predefined configurations that meet specific operational, security, and compliance requirements.</p>"},{"location":"use-cases/vm-template-management/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/vm-template-management/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ with KubeVirt enabled</li> <li>Storage solutions for template images (e.g., OpenShift Data Foundation)</li> <li>Network connectivity between clusters for template distribution</li> </ul>"},{"location":"use-cases/vm-template-management/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Storage: Adequate space for storing multiple VM templates</li> <li>Network: Fast connectivity for template distribution and deployment</li> </ul>"},{"location":"use-cases/vm-template-management/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    TEMPLATE_STORE[\"Template Storage\"] -- Distributes --&gt; CLUSTER1[\"OpenShift Cluster 1\"]\n    TEMPLATE_STORE -- Distributes --&gt; CLUSTER2[\"OpenShift Cluster 2\"]\n    CLUSTER1 -- Deploys --&gt; VM1[\"VM Instances\"]\n    CLUSTER2 -- Deploys --&gt; VM2[\"VM Instances\"]\n\n    style TEMPLATE_STORE fill:#f9f,stroke:#333</code></pre>"},{"location":"use-cases/vm-template-management/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/vm-template-management/#step-1-create-vm-templates","title":"Step 1: Create VM Templates","text":""},{"location":"use-cases/vm-template-management/#define-template-specification","title":"Define Template Specification","text":"<pre><code>apiVersion: kubevirt.io/v1alpha3\nkind: VirtualMachineTemplate\nmetadata:\n  name: webserver-template\nspec:\n  domain:\n    cpu:\n      cores: 2\n    devices:\n      disks:\n      - name: rootdisk\n        disk:\n          bus: virtio\n    memory:\n      guest: 4Gi\n  volumes:\n  - name: rootdisk\n    containerDisk:\n      image: kubevirt/cirros-registry-disk-demo\n</code></pre>"},{"location":"use-cases/vm-template-management/#step-2-store-templates","title":"Step 2: Store Templates","text":""},{"location":"use-cases/vm-template-management/#upload-template-to-storage","title":"Upload Template to Storage","text":"<ul> <li>Use OpenShift Data Foundation or another persistent storage solution to store VM templates.</li> </ul>"},{"location":"use-cases/vm-template-management/#step-3-distribute-templates","title":"Step 3: Distribute Templates","text":""},{"location":"use-cases/vm-template-management/#sync-templates-across-clusters","title":"Sync Templates Across Clusters","text":"<ul> <li>Ensure templates are replicated to all relevant clusters using synchronized storage.</li> </ul>"},{"location":"use-cases/vm-template-management/#step-4-deploy-vms-using-templates","title":"Step 4: Deploy VMs Using Templates","text":""},{"location":"use-cases/vm-template-management/#create-vms-from-templates","title":"Create VMs from Templates","text":"<pre><code>apiVersion: kubevirt.io/v1alpha3\nkind: VirtualMachine\nmetadata:\n  name: new-webserver\nspec:\n  running: false\n  template:\n    metadata:\n      labels:\n        vm.kubevirt.io/template: webserver-template\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: rootdisk\n        memory:\n          guest: 4Gi\n</code></pre>"},{"location":"use-cases/vm-template-management/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/vm-template-management/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"use-cases/vm-template-management/#template-sync-issues","title":"Template Sync Issues","text":"<ul> <li>Verify storage connectivity and check log files for errors.</li> </ul>"},{"location":"use-cases/vm-template-management/#deployment-failures","title":"Deployment Failures","text":"<ul> <li>Ensure templates are properly defined and contain all necessary configurations.</li> </ul>"},{"location":"use-cases/vm-template-management/#best-practices","title":"Best Practices","text":"<ul> <li>Template Versioning: Keep versions of templates to track changes and rollback if necessary.</li> <li>Regular Audits: Periodically review and update templates to ensure they meet current security and compliance standards.</li> <li>Backup Templates: Ensure templates are backed up to prevent loss in case of storage failures.</li> </ul>"},{"location":"use-cases/vm-template-management/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":""},{"location":"use-cases/vm-template-management/#monitoring-integration","title":"Monitoring Integration","text":"<ul> <li>Set up monitoring for template storage systems to detect and resolve issues promptly.</li> </ul> <p>This guide provides the necessary steps and best practices for efficient VM template management within an RH OVE environment, facilitating faster and more consistent VM deployments.</p>"},{"location":"use-cases/waf-firewalling/","title":"Use Case: Web Application Firewall (WAF) Firewalling and Integration with F5 BigIP","text":""},{"location":"use-cases/waf-firewalling/#business-context","title":"Business Context","text":"<p>As enterprises adopt cloud-native architectures, securing web applications becomes crucial for protecting sensitive data and maintaining business continuity. This use case demonstrates how to implement a robust Web Application Firewall (WAF) using native L4-L7 capabilities and integrate it with F5 BigIP for enhanced security management within the RH OVE ecosystem.</p>"},{"location":"use-cases/waf-firewalling/#technical-requirements","title":"Technical Requirements","text":""},{"location":"use-cases/waf-firewalling/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>OpenShift 4.12+ cluster with Multus CNI enabled</li> <li>Cilium CNI for L4-L7 Policy Enforcement</li> <li>F5 BigIP for advanced traffic management and security policies</li> <li>Persistent storage solutions for logs and reports</li> </ul>"},{"location":"use-cases/waf-firewalling/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>CPU: Sufficient compute resources to support firewall processing</li> <li>Memory: Adequate memory allocation for traffic inspection and logs</li> <li>Storage: High-performance storage for log retention and reporting</li> <li>Network: Scalable network infrastructure for seamless traffic flow</li> </ul>"},{"location":"use-cases/waf-firewalling/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    subgraph \"Application Layer\"\n        APP1[\"Web Application 1\"]\n        APP2[\"Web Application 2\"]\n    end\n\n    subgraph \"Security Layer\"\n        CILIUM_WAF[\"Cilium L4-L7 WAF\"]\n        F5_BIGIP[\"F5 BigIP\"]\n    end\n\n    subgraph \"Infrastructure Layer\"\n        NETWORK[\"Network Infrastructure\"]\n        STORAGE[\"Persistent Storage\"]\n    end\n\n    APP1 --&gt;|request| CILIUM_WAF\n    APP2 --&gt;|request| CILIUM_WAF\n    CILIUM_WAF --&gt;|forward| F5_BIGIP\n    F5_BIGIP --&gt;|filter| NETWORK\n\n    F5_BIGIP --&gt;|logs| STORAGE\n    CILIUM_WAF --&gt;|metrics| STORAGE\n\n    style CILIUM_WAF fill:#f9f,stroke:#333\n    style F5_BIGIP fill:#9ff,stroke:#333</code></pre>"},{"location":"use-cases/waf-firewalling/#implementation-steps","title":"Implementation Steps","text":""},{"location":"use-cases/waf-firewalling/#step-1-deploy-cilium-l4-l7-firewall","title":"Step 1: Deploy Cilium L4-L7 Firewall","text":""},{"location":"use-cases/waf-firewalling/#cilium-configuration","title":"Cilium Configuration","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: web-app-waf-policy\n  namespace: infrastructure\nspec:\n  endpointSelector:\n    matchLabels:\n      app.kubernetes.io/name: web\n  ingress:\n  - rules:\n      http:\n      - method: \"POST\"\n        path: \"/api\"\n      - method: \"GET\"\n        path: \"/\"\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        infrastructure: f5-bigip\n</code></pre>"},{"location":"use-cases/waf-firewalling/#step-2-integrate-with-f5-bigip","title":"Step 2: Integrate with F5 BigIP","text":""},{"location":"use-cases/waf-firewalling/#f5-bigip-virtual-server-configuration","title":"F5 BigIP Virtual Server Configuration","text":"<ul> <li>Configure F5 virtual server to handle traffic directed from Cilium WAF.</li> <li>Implement F5 policies for SSL termination, traffic redirection, and detailed logging.</li> </ul> <pre><code># Example F5 BigIP CLI configuration\ncreate ltm virtual vs-web-app {\n  destination 192.168.1.100:80\n  ip-protocol tcp\n  profiles add { http { context clientside } }\n  pool my-web-app-pool\n  rules { waf-inspection }  \n}\n\n# Associate WAF policies\ncreate ltm policy waf-inspection {\n  rules add {\n    10 { conditions { tcp } actions { forward pool-member my-web-app-pool\n    } }\n  }\n}\n</code></pre>"},{"location":"use-cases/waf-firewalling/#step-3-advanced-traffic-monitoring-and-logging","title":"Step 3: Advanced Traffic Monitoring and Logging","text":""},{"location":"use-cases/waf-firewalling/#persistent-storage-configuration","title":"Persistent Storage Configuration","text":"<ul> <li>Configure persistent volumes for log storage using Cilium and F5 BigIP integrations.</li> </ul> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: storage-logs\n  namespace: infrastructure\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Gi\n  storageClassName: high-performance\n</code></pre>"},{"location":"use-cases/waf-firewalling/#step-4-deploy-monitoring-and-analytics-tools","title":"Step 4: Deploy Monitoring and Analytics Tools","text":""},{"location":"use-cases/waf-firewalling/#monitoring-with-grafana-and-prometheus","title":"Monitoring with Grafana and Prometheus","text":"<ul> <li>Use Grafana dashboards to visualize traffic patterns and security metrics.</li> <li>Implement Prometheus alerting for suspicious activity detection.</li> </ul> <pre><code># Grafana Dashboard Configuration\napiVersion: integreatly.org/v1alpha1\nkind: GrafanaDashboard\nmetadata:\n  name: waf-dashboard\n  namespace: monitoring\nspec:\n  json: |\n    {\n      \"title\": \"Web Application Firewall Overview\",\n      \"panels\": [\n        {\n          \"type\": \"graph\",\n          \"title\": \"HTTP Requests\",\n          \"targets\": [\n            { \"expr\": \"sum(rate(http_requests_total[5m]))\", \"interval\": \",5m\" }\n          ]\n        }\n      ]\n    }\n</code></pre>"},{"location":"use-cases/waf-firewalling/#troubleshooting-and-maintenance","title":"Troubleshooting and Maintenance","text":""},{"location":"use-cases/waf-firewalling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li>Policy Misconfiguration: Verify Cilium and F5 policy configurations for errors.</li> <li>Performance Degradation: Ensure adequate resources for Cilium and F5 processing.</li> <li>Logging Failures: Check storage availability and permissions.</li> </ul>"},{"location":"use-cases/waf-firewalling/#best-practices","title":"Best Practices","text":"<ul> <li>Regular Audits: Conduct regular audits of firewall rules and configurations.</li> <li>Security Hardening: Apply security patches to F5 BigIP and Cilium regularly.</li> <li>Performance Monitoring: Continuously monitor firewall performance and resource usage.</li> </ul>"},{"location":"use-cases/waf-firewalling/#integration-with-rh-ove-ecosystem","title":"Integration with RH OVE Ecosystem","text":"<ul> <li>Seamless Traffic Flow: Ensure smooth integration of traffic between application components and security layers.</li> <li>Consistent Policy Management: Use GitOps practices to manage and version firewall rules.</li> <li>Network Observability: Leverage Cilium Hubble for enhanced network observability within the OpenShift clusters.</li> </ul> <p>This comprehensive guide provides the steps and best practices required to deploy and manage an effective Web Application Firewall solution in combination with F5 BigIP, ensuring robust protection for your web applications while seamlessly integrating within the RH OVE multi-cluster ecosystem.</p>"},{"location":"workspace/manual/","title":"RH OVE Infrastructure Project","text":""},{"location":"workspace/manual/#scope","title":"Scope","text":"<p>Focus on the complete lifecycle of infrastructure setup and operations, including study, HLD, LLD, implementation, testing, and Day-2 operations.</p>"},{"location":"workspace/manual/#documentation-areas","title":"Documentation Areas","text":"<ul> <li>Architecture: <code>global-overview.md</code>, <code>overview.md</code>, <code>design-principles.md</code>, <code>network.md</code>, <code>storage.md</code>, <code>iam.md</code></li> <li>ADRs (Architecture Decision Records): Multi-cluster patterns, GitOps, cluster topology, admission control, network CNI, backup, monitoring, IAM</li> <li>Deployment: <code>prerequisites.md</code>, <code>installation.md</code>, <code>configuration.md</code></li> <li>Management: <code>admission-control.md</code>, <code>gitops.md</code>, <code>monitoring.md</code>, <code>backup.md</code></li> <li>Operations: <code>day2-ops.md</code>, <code>troubleshooting.md</code>, <code>performance.md</code></li> </ul>"},{"location":"workspace/manual/#work-phases","title":"Work Phases","text":"<ul> <li>Study: Gather detailed understanding of the RH OVE architecture and components.</li> <li>Design: Create and validate HLD and LLD documents.</li> <li>Implementation: Deploy infrastructure components following established practices.</li> <li>Testing: Verify infrastructure robustness, security, and performance.</li> <li>Day-2 Operations: Establish ongoing monitoring, troubleshooting, and tuning.</li> </ul>"},{"location":"workspace/manual/#required-personas","title":"Required Personas","text":"<ul> <li>Architect</li> <li>DevOps Engineer</li> <li>System Administrator</li> </ul>"},{"location":"workspace/manual/#use-cases-implementation-project","title":"Use-Cases Implementation Project","text":""},{"location":"workspace/manual/#scope_1","title":"Scope","text":"<p>Study, design, implement, test, and operate comprehensive use-cases demonstrating RH OVE capabilities for applications and services.</p>"},{"location":"workspace/manual/#documentation-areas_1","title":"Documentation Areas","text":"<ul> <li>Use Cases: Overview, multi-env setup, hybrid applications, database services, legacy modernization, disaster recovery, observability, security, integration</li> <li>References: <code>best-practices.md</code>, <code>glossary.md</code></li> </ul>"},{"location":"workspace/manual/#work-phases_1","title":"Work Phases","text":"<ul> <li>Study: Analyze use case requirements and dependencies.</li> <li>Design: Develop HLD and LLD for each use case.</li> <li>Implementation: Build and deploy use case configurations, manifests, and scripts.</li> <li>Testing: Conduct functional and integration testing.</li> <li>Day-2 Operations: Develop runbooks for operational support.</li> </ul>"},{"location":"workspace/manual/#required-personas_1","title":"Required Personas","text":"<ul> <li>Solution Architect</li> <li>Application Developer</li> <li>Testing Specialist</li> </ul>"},{"location":"workspace/manual/#migration-workload-from-vmware-project","title":"Migration Workload from VMware Project","text":""},{"location":"workspace/manual/#scope_2","title":"Scope","text":"<p>Plan and execute the migration of workloads from VMware environments to RH OVE.</p>"},{"location":"workspace/manual/#documentation-areas_2","title":"Documentation Areas","text":"<ul> <li>VM Lifecycle: <code>vm-importation.md</code>, <code>vm-template-management.md</code>, <code>vm-scaling-performance.md</code>, <code>vm-backup-recovery.md</code></li> <li>Related ADRs: Migration strategies or compatibility considerations</li> </ul>"},{"location":"workspace/manual/#work-phases_2","title":"Work Phases","text":"<ul> <li>Study: Inventory and analyze existing VMware workloads and requirements.</li> <li>Design: Create migration strategies, including HLD and LLD documents.</li> <li>Implementation: Execute migration workflows.</li> <li>Testing: Validate functionality and performance post-migration.</li> <li>Day-2 Operations: Develop monitoring, backup, and disaster recovery procedures.</li> </ul>"},{"location":"workspace/manual/#required-personas_2","title":"Required Personas","text":"<ul> <li>Migration Specialist</li> <li>VMware Administrator</li> <li>RH OVE Engineer</li> </ul>"}]}